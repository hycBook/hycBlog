---
title: 数据库相关操作
date: '2022/9/10 20:46:25'
top_img: 'https://pic.hycbook.com/i//hexo/post_imgs/蕾姆9.webp'
cover: 'https://pic.hycbook.com/i//hexo/post_cover/蕾姆9.webp'
categories:
  - python
tags:
  - python
  - 数据库
  - Elasticsearch
  - Oracle
  - Postgresql
  - sql
  - orm
  - SqlAlchemy
abbrlink: 59006
---

---




# Elasticsearch数据库

## 环境配置

>安装环境
>
>pip install elasticsearch==7.6.0
>
>

## EsDao包装类

```python
# -- coding: utf-8 --

"""
@version: v1.0
@author: huangyc
@file: EsDao.py
@Description: Es统一操作类
@time: 2020/4/27 10:22
"""

from elasticsearch.helpers import bulk
from elasticsearch import Elasticsearch
import pandas as pd


class EsDao(object):
    """
    ElasticSearch的数据操作类
    """
    # 查询批次大小
    DEFAULT_BATCH_SIZE = 1000

    # 写入批次大小
    BULK_BATCH_SIZE = 10000

    def __init__(self, hosts, timeout=3600*24):
        self.hosts = hosts
        self.timeout = timeout
        self.es = Elasticsearch(hosts, timeout=self.timeout)

    def save_data_list(self, index_name, data_list):
        """
        保存数据列表到es的指定索引中
        :param index_name: 索引名称
        :param data_list: 数据列表，列表元素代表一行数据，元素类型为dict
        :return:
        """
        bulk_data_lst = [
            data_list[i:i + self.BULK_BATCH_SIZE]
            for i in range(0, len(data_list), self.BULK_BATCH_SIZE)
        ]

        if len(data_list) > 0 and '_id' in data_list[0]:
            for bulk_data in bulk_data_lst:
                actions = [{
                    "_index": index_name,
                    "_type": index_name,
                    "_id": data.pop("_id"),
                    "_source": data
                }
                    for data in bulk_data
                ]
                bulk(self.es, actions, index=index_name, raise_on_error=True)
        else:
            for bulk_data in bulk_data_lst:
                actions = [{
                    "_index": index_name,
                    "_type": index_name,
                    "_source": data
                }
                    for data in bulk_data
                ]
                bulk(self.es, actions, index=index_name, raise_on_error=True)

    def is_index_exists(self, index_name):
        """
        判断指定索引是否存在
        :param index_name: 索引名称
        :return:
        """
        return self.es.indices.exists(index=index_name)

    def delete_by_query(self, index_name, query_body):
        """
        按查询结果删除数据
        :param index_name:
        :param query_body:
        :return:
        """
        return self.es.delete_by_query(index_name, query_body)

    def clear_index_data(self, index_name):
        """
        清空指定索引的数据
        :param index_name:
        :return:
        """
        return self.delete_by_query(
            index_name=index_name,
            query_body={
                "query": {
                    "match_all": {}
                }
            }
        )

    def save_df_data(self, index_name, df):
        """
        保存pandas的DataFrame到es的指定索引中
        :param index_name: 索引名称
        :param df: 要保存的dataframe
        :return:
        """
        col_lst = df.columns.tolist()
        dic_lst = [dict([(c, v) for c, v in zip(col_lst, r)]) for r in df.values.tolist()]
        self.save_data_list(index_name=index_name, data_list=dic_lst)

    def create_index(self, index_name, mapping_properties):
        """
        创建索引
        :param index_name: 索引名称
        :param mapping_properties: 索引mapping中的属性列表
        :return:
        """
        if not self.es.indices.exists(index=index_name):
            mapping = {
                "mappings": {
                    index_name: {
                        "properties": mapping_properties
                    }
                }
            }
            res = self.es.indices.create(index=index_name, body=mapping)
            if res is not None and 'acknowledged' in res:
                return res.get('acknowledged')
        return False

    def _search_with_scroll(self, index_name, query_body):
        if "size" not in query_body:
            query_body["size"] = self.DEFAULT_BATCH_SIZE
        response = self.es.search(
            index=index_name,
            body=query_body,
            search_type="dfs_query_then_fetch",
            scroll="120m",
            timeout="60m"
        )
        scroll_id = response["_scroll_id"]
        while True:
            sources = [doc["_source"] for doc in response["hits"]["hits"]]
            if len(sources) == 0:
                break
            yield sources
            response = self.es.scroll(scroll_id=scroll_id, scroll="60m")

    def query_for_df(self, index_name, query_body):
        """
        执行查询并获取pandas.DataFrame格式的返回值
        :param index_name: 索引名称
        :param query_body: 查询条件
        :return:
        """
        sources = []
        for sub_source in self._search_with_scroll(index_name=index_name, query_body=query_body):
            sources.extend(sub_source)
        return pd.DataFrame(sources)

    def query_for_df_with_batch(self, index_name, query_body, batch_size=DEFAULT_BATCH_SIZE):
        """
        按批次大小查询并返回pandas.DataFrame的generator格式的返回值
        :param index_name: 索引名称
        :param query_body: 查询条件
        :param batch_size: 批次大小
        :return:
        """
        if "size" not in query_body:
            query_body["size"] = batch_size
        for sub_source in self._search_with_scroll(index_name=index_name, query_body=query_body):
            yield pd.DataFrame(sub_source)

    def get_first_row_with_df(self, index_name):
        """
        获取指定索引的首行数据，格式为pandas.DataFrame
        可用于获取索引的元信息
        :param index_name: 索引名称
        :return:
        """
        query_body = {
            "size": 1,
            "query": {
                "match_all": {}
            }
        }
        for sub_source in self._search_with_scroll(index_name=index_name, query_body=query_body):
            return pd.DataFrame(sub_source)

```



## 使用案例

```python
class TaskMeta:
    '''
    数据元类
    '''
    def __init__(self, text, doc_id, sentence_id, reg_lst, flag, has_reg, text_source="primitive"):
        self.text = text
        self.doc_id = doc_id
        self.sentence_id = sentence_id
        self.reg_lst = reg_lst
        self.flag = flag
        self.has_reg = has_reg
        self.text_source = text_source

    def __repr__(self):
        return f'{self.text} {self.doc_id} {self.sentence_id} {self.reg_lst} {self.flag} {self.has_reg} {self.text_source}'

    def to_dict(self):
        return {"text": self.text,
                "doc_id": self.doc_id,
                "sentence_id": self.sentence_id,
                "reg_lst": self.reg_lst,
                "flag": self.flag,
                "has_reg": self.has_reg,
                "text_source": self.text_source}
```

```python
def create_index(target_es_dao, index_name, mapping):
    '''
    创建es索引
    :return: 是否创建成功
    '''
    if not target_es_dao.is_index_exists(index_name):
        target_es_dao.create_index(index_name, mapping)
    else:
        target_es_dao.clear_index_data(index_name)
        print(f"索引{index_name}已存在, 已清除数据")

def writer_fun(target_es_dao, target_index, sample_lst):
    '''
    写数据到es库
    '''
    df_sample_lst = []
    [df_sample_lst.append(sample.to_dict()) for sample in sample_lst]
    df_sample_lst = pd.DataFrame(df_sample_lst)
    target_es_dao.save_df_data(target_index, df_sample_lst)
    print(f'写入数据{len(sample_lst)}条')

def es_cal_test():
    # 获取连接
    source_es_dao = EsDao(f"http://{aug_config.SOURCE_IP}:{aug_config.SOURCE_PORT}/")
    query_condition = {
        "query_string": {
            "default_field": "has_reg",
            "query": "true"
        }
    }
    query_body = {
        "query": query_condition
    }
    # 查询数据
    datas = source_es_dao.query_for_df(index_name=aug_config.SOURCE_INDEX, query_body=query_body)
    records = datas.to_dict(orient='record')
    sample_lst = []
    for record in records:
        sample_lst.append(
            TaskMeta(
                text=record["text"],
                doc_id=record["doc_id"],
                sentence_id=record["sentence_id"],
                reg_lst=record["reg_lst"],
                flag=record["flag"],
                has_reg=record["has_reg"]
            )
        )

    # 创建索引
    create_index(target_es_dao, aug_config.TARGET_INDEX, aug_config.MAPPING)
    # 写入数据
    writer_fun(target_es_dao, aug_config.TARGET_INDEX, sample_lst=sample_lst)

if __name__ == '__main__':
    es_cal_test()
```



# Oracle数据库

> [Python操作Oracle数据库：cx_Oracle](https://www.cnblogs.com/chenhuabin/p/12689163.html)

## 环境配置

>第一步：安装库
>
>pip install cx-Oracle
>
>第二步：文件拷贝
>
>需要将`oci.dll、oraocci11.dll、oraociei11.dll`复制到sitepackages路径下

## sql基础

### 建表

```sql
--blob字段插入实例
create table blob_table_tmp(
  id number primary key,
  blob_cl blob not null,
	clob_cl clob not null
);
insert into blob_table_tmp values(1,rawtohex('11111000011111'),'增加一条记录时，碰到插入blob类型数据出错');
insert into blob_table_tmp values(3,rawtohex('4561888'),'增加一条记录时，碰到插入blob类型数据出错');
insert into blob_table_tmp values(4,rawtohex('增加一条记录时333'),'增加一条记录时，碰到插入blob类型数据出错');
```

### 查询

>获取连接

```python
FINANCE_DB_HOST = "192.168.x.x"
FINANCE_DB_PORT = 1521
FINANCE_DB_USER = "hyc"
FINANCE_DB_PASSWORD = "123456"
FINANCE_DB_DB = "ORCL"

class OracleConn():
    config_path = ''
    @staticmethod
    def get_conn(conn_name, encoding="UTF-8"):
        conn_str = str(eval("%s_DB_USER" % (OracleConn.config_path, conn_name))) + "/" + str(eval("%s.%s_DB_PASSWORD" % (OracleConn.config_path, conn_name)))
        conn_str += "@" + str(eval("%s_DB_HOST" % (OracleConn.config_path, conn_name)))
        conn_str += ":" + str(eval("%s_DB_PORT" % (OracleConn.config_path, conn_name)))
        conn_str += "/" + str(eval("%s_DB_DB" % (OracleConn.config_path, conn_name)))
        return ora.connect(conn_str, encoding=encoding, nencoding=encoding)
```

>读写数据库

```python
def oracle_test():
    # 获取数据库连接
    conn = OracleConn.get_conn("FINANCE")
    cur = conn.cursor()

    # 查询数据
    sql = "select id,blob_cl,clob_cl from FINANCE.blob_table_tmp"
    datas = []
    r = cur.execute(sql)
    # 假设name是clob字段类型
    [datas.append((gg[0], gg[1].read().decode('utf-8'), gg[2].read())) for gg in r]

    # 写入数据
    insert_sql = "INSERT INTO new_table(id,new_name) VALUES (:ID,:NEW_NAME)"
    res = []
    [res.append((data[0], data[1])) for data in datas]
    cur.executemany(insert_sql, res)
    cur.execute('commit')

    cur.close()
    conn.close()
    print("写入结束")


if __name__ == '__main__':
    oracle_test()
```



## 相关操作

>关于数据库的连接，查询和写入

```python
import cx_Oracle

class Setting:
    DB_USER = 'narutohyc'
    DB_PASSWORD = 'hyc'
    DB_IP = '192.168.0.1'
    DB_PORT = ''
    DB_SERVICE = 'dataBaseName'
setting = Setting()

def oracle_test():
    # 获取数据库连接
    conn = cx_Oracle.connect('%s/%s@%s/%s' % (setting.DB_USER, setting.DB_PASSWORD, setting.DB_IP, setting.DB_SERVICE), encoding='utf-8')
    cur = conn.cursor()

    # 查询数据
    sql = "select ID, name from hyc_database"
    datas = []
    r = cur.execute(sql)
    # 假设name是clob字段类型
    [datas.append((gg[0], gg[1].read())) for gg in r]

    # 写入数据
    insert_sql = "INSERT INTO new_table(id,new_name) VALUES (:ID,:NEW_NAME)"
    res = []
    [res.append((data[0], data[1])) for data in datas]
    cur.executemany(insert_sql, res)
    cur.execute('commit')

    cur.close()
    conn.close()
    print("写入结束")

if __name__ == '__main__':
    oracle_test()
```

# Postgresql数据库

> [我终于学会了使用python操作postgresql](https://www.cnblogs.com/zszxz/p/12222201.html)
>
> [保姆级 CentOS 7离线安装PostgreSQL 14教程](https://blog.csdn.net/binglang2012/article/details/124210280)
>
> [易百_PostgreSQL教程](https://www.yiibai.com/postgresql/postgresql-indexes.html)

## 离线安装数据库

> 先从[centos7-pg_14.2下载](https://yum.postgresql.org/14/redhat/rhel-7-x86_64/repoview/postgresqldbserver14.group.html)下载rpm包([微云下载centos7.6_PostgreSQL14.2](https://share.weiyun.com/6DmxKUrx))，或者直接[官方下载安装教程](https://www.postgresql.org/download/linux/redhat/)安装，如果离线安装就下载rpm包

```sh
# 离线安装执行以下命令安装
rpm -ivh postgresql14-libs-14.2-1PGDG.rhel7.x86_64.rpm
rpm -ivh postgresql14-14.2-1PGDG.rhel7.x86_64.rpm
rpm -ivh postgresql14-server-14.2-1PGDG.rhel7.x86_64.rpm
rpm -ivh postgresql14-contrib-14.2-1PGDG.rhel7.x86_64.rpm
```

出现OSError: Python library not found: libpython3.6mu.so.1.0, libpython3.6m.so.1.0, libpython3.6.so.1.0, libpython3.6m.so的解决办法

```sh
yum install python3-devel
```

创建数据库data和log文件夹

```sh
# 创建数据库data和log文件夹
mkdir -p /home/postgres/pgsql_data
mkdir -p /home/postgres/pgsql_log
# 创建日志文件
touch /home/postgres/pgsql_log/pgsql.log
```

授权给安装数据时自动创建的postgres用户

```sh
chown -R postgres:postgres /home/postgres/pgsql_data
chown -R postgres:postgres /home/postgres/pgsql_log
```

切换到安装数据时自动创建的postgres用户

```sh
su - postgres
```

初始化数据库到新建数据目录

```sh
/usr/pgsql-14/bin/initdb -D /home/postgres/pgsql_data
```

启动服务器(初始化数据库日志文件)

```
/usr/pgsql-14/bin/pg_ctl -D  /home/postgres/pgsql_data/ -l /home/postgres/pgsql_log/pgsql.log start
# 查看状态
/usr/pgsql-14/bin/pg_ctl -D /home/postgres/pgsql_data/ -l /home/postgres/pgsql_log/pgsql.log status
```

切换到管理员开启端口并重启防火墙

```sh
su root
firewall-cmd --zone=public --add-port=5432/tcp --permanent
firewall-cmd --reload
```

修改配置文件实现远程访问`vi /home/postgres/pgsql_data/postgresql.conf`

```sh
# 修改监听地址
listen_addresses = '*'
# 修改最大连接数（按需）
max_connections = 1000
# 修改密码认证
password_encryption = md5
```

修改可访问的用户IP段

```sh
vi /home/pgsql_data/pg_hba.conf（a进入编辑模式，esc退出编辑模式，:wq并按回车保存）
IPV4下修改为或新增
host    all             all             0.0.0.0/0               trust
```

postgres用户重启数据库服务

```sh
su - postgres
/usr/pgsql-14/bin/pg_ctl -D  /home/postgres/pgsql_data/ -l /home/postgres/pgsql_log/pgsql.log restart
```

数据库安装结束，管理员postgres，默认密码123456

![img](https://pic.hycbook.com/i/hexo/bk_resources/python/10.数据库相关操作/image-20220512140313497.webp)
<center>使用navicat连接pg库后新建数据库</center>





## 环境配置

>pip install psycopg2

## sql语法

### 数据库连接

```sql
-- 获取数据库实例连接数
select count(*) from pg_stat_activity;
-- 获取数据库最大连接数
show max_connections;
-- 查询当前连接数详细信息
select * from pg_stat_activity;
-- 查询数据库中各个用户名对应的数据库连接数
select usename, count(*) from pg_stat_activity group by usename;
```

### 数据库信息

```sql
-- 查询数据库大小
select pg_size_pretty (pg_database_size('pg_fac_stk'));

-- 查询各表磁盘占用
SELECT
    table_schema || '.' || table_name AS table_full_name,
    pg_size_pretty(pg_total_relation_size('"' || table_schema || '"."' || table_name || '"')) AS size
FROM information_schema.tables where table_name like 'finance_%'
ORDER BY
    pg_total_relation_size('"' || table_schema || '"."' || table_name || '"') DESC;
    
-- 获取各个表中的数据记录数
select relname as TABLE_NAME, reltuples as rowCounts from pg_class where relkind = 'r' order by rowCounts desc;

-- 查看数据库表对应的数据文件
select pg_relation_filepath('product');

-- 查看数据库实例的版本
select version();

-- 分析评估SQL执行情况
EXPLAIN ANALYZE SELECT * FROM t_cfg_opinfo;

-- 获取数据库当前的回滚事务数以及死锁数
select datname,xact_rollback,deadlocks from pg_stat_database;

```

### 数据库备份

```sql
-- 备份postgres库并tar打包
pg_dump -h 127.0.0.1 -p 5432 -U postgres -f postgres.sql.tar -Ft;

-- 备份postgres库，转储数据为带列名的INSERT命令
pg_dumpall -d postgres -U postgres -f postgres.sql --column-inserts;
```



### 表空间

> 新建表空间

```sh
# 新建表空间目录 t_fac_ts
mkdir /home/huangyc/t_fac_ts
# 修改表空间的用户权限
chown postgres /home/huangyc/t_fac_ts
```

> pg库新建表空间

```sh
create tablespace t_fac_ts owner postgres location '/home/huangyc/t_fac_ts';
```

> 表空间有关的一些语法

```sql
# 删除表空间 (需要先drop表空间所有的表, 或者将该空间下所有的表移除才能drop表空间)
DROP TABLESPACE t_fac_ts;
# 修改具体的表到指定表空间下
ALTER TABLE t_fac_tushare_stock_basic SET TABLESPACE t_fac_ts;
# 修改指定库到指定表空间下
ALTER DATABASE name SET TABLESPACE new_tablespace;

```

### 锁表处理

> pg锁表解锁

1. 查看被锁的表

   ```sql
   select a.locktype,a.database,a.pid,a.mode,a.relation,b.relname
   from pg_locks a
   join pg_class b on a.relation = b.oid where relname='t_opt_strhdk_blsj';
   ```

2. 杀死被锁的pid

   ```sql
   select pg_terminate_backend(pid);
   ```

### 表结构修改

```sql
-- 修改表名
alter table "user" rename to "ts_user";
-- 添加新字段
alter table table_name add column col_name varchar(50);
-- 丢弃某列
alter table table_name drop column col_name;
-- 添加主键
alter table table_name add primary key("col_name");
-- 修改字段名
alter table table_name rename column old_col_name to new_col_name;
```

### 数据更新和查询

> 设置某字段的值

```sql
-- 设置某字段的值
update table_name set col_name=new_value;
```

> 删除表中重复数据

```sql
-- 查询[旧表]数据的重复情况
select col1,col2,count(*) from old_table group by col1,col2;

-- 所有字段都一样的情况
create table bak_table as select distinct * from table_name;
-- 查询[新表]数据的重复情况
select col1,col2,count(*) from bak_table group by col1,col2;
truncate table old_table;
insert into old_table (col1,col2) select col1,col2 from bak_table;
```

### 数据和结构复制

```sql
-- [复制表和数据] 复制表结构和数据 自动建表，不会复制主键什么的
create table new_table as select * from old_table;
-- [复制数据] 复制数据到 新表 表需要提前建，并且表字段要一致，不会复制主键什么的
insert into new_table (col_0, col_1) select col_0, col_1 from old_table;
```

### 索引

```sql
-- 获取数据库表中的索引
select * from pg_indexes where tablename = 't_cfg_opinfo'; 
-- 创建索引
create index index_name on table_name (col_0, col_1);
-- 查询索引
select * from pg_indexes where tablename='table_name';
-- 删除索引
drop index index_name;
```

> 什么情况下要避免使用索引？

虽然索引的目的在于提高数据库的性能，但这里有几个情况需要避免使用索引

**使用索引时，需要考虑下列准则**：

- 索引不应该使用在较小的表上
- 索引不应该使用在有频繁的大批量的更新或插入操作的表上
- 索引不应该使用在含有大量的 NULL 值的列上
- 索引不应该使用在频繁操作的列上

### 其他语法

> 筛选某列，逗号拼接

```sql
select string_agg(bs_org_id,',') as bs_org_ids 
  from bs_org 
  where par_org_id ='100'
```



> 日期转换

```sql
select to_char(col_name,'yyyyMMDD')-interval '2 day' from table_name
-- -interval '2 day' 表示往前2天
```

> 转时间戳

```sql
select '2011-01-06 09:57:59'::timestamp;
TO_TIMESTAMP('2011-01-06 09:57:59', 'YYYY-MM-DD HH24:MI:S')
```

> postgresql 获取分组第一条数据 窗口函数

1. 给数据分组并排名，使用 `row_number() over (partition by 分组的字段名 order by 排序规则) as 排名`
2. 从上述第一步中取出，排名为第一的数据，即为第一条数据 `select * from 上述第一步 where 排名=1`
3. 获取前N名的数据，将一中第二步的条件换成`where 排名 < N+1`

>  distributed key

```sql
alter table table_name set distributed by (id);
alter table table_name add primary key (id);
```

# ORM框架

## ORM框架比较

> [一文了解 Python 的三种数据源架构模式](https://zhuanlan.zhihu.com/p/64428869)
>
> [SQLAlchemy 和其他的 ORM 框架](https://www.oschina.net/translate/sqlalchemy-vs-orms)

> SQLObject

* 优点：

  采用了易懂的ActiveRecord 模式

  一个相对较小的代码库

* 缺点：

  方法和类的命名遵循了Java 的小驼峰风格

  不支持数据库session隔离工作单元

> Storm

* 优点：

  清爽轻量的API，短学习曲线和长期可维护性

  不需要特殊的类构造函数，也没有必要的基类

* 缺点：

  迫使程序员手工写表格创建的DDL语句，而不是从模型类自动派生

  Storm的贡献者必须把他们的贡献的版权给Canonical公司

> Django's ORM

* 优点：

  易用，学习曲线短

  和Django紧密集合，用Django时使用约定俗成的方法去操作数据库

* 缺点：

  不好处理复杂的查询，强制开发者回到原生SQL

  紧密和Django集成，使得在Django环境外很难使用

> peewee

* 优点：

  Django式的API，使其易用

  轻量实现，很容易和任意web框架集成

* 缺点：

  不支持自动化 schema 迁移

  多对多查询写起来不直观

> SQLAlchemy

* 优点：

  企业级 API，使得代码有健壮性和适应性

  灵活的设计，使得能轻松写复杂查询

* 缺点：

  工作单元概念不常见

  重量级 API，导致长学习曲线

相比其他的ORM， SQLAlchemy 意味着，无论你何时写SQLAlchemy代码， 都专注于工作单元的前沿概念 。DB Session 的概念可能最初很难理解和正确使用，但是后来你会欣赏这额外的复杂性，这让意外的时序提交相关的数据库bug减少到0。在SQLAlchemy中处理多数据库是棘手的， 因为每个DB session 都限定了一个数据库连接。但是，这种类型的限制实际上是好事， 因为这样强制你绞尽脑汁去想在多个数据库之间的交互， 从而使得数据库交互代码很容易调试。

## SQLAlchemy

> [SQLAlchemy 1.4 Documentation](https://www.osgeo.cn/sqlalchemy/index.html#)
>
> [sqlalchemy操作数据库](https://www.cnblogs.com/xiaonq/p/8420826.html)
>
> [sqlalchemy外键和relationship查询](https://www.cnblogs.com/goldsunshine/p/9269880.html)
>
> [SQLALlchemy数据查询小集合](https://www.cnblogs.com/goldsunshine/p/10124859.htmlhttps://www.cnblogs.com/goldsunshine/p/10124859.html)
>
> [SQLAlchemy 的连接池机制](https://sanyuesha.com/2019/01/02/sqlalchemy-pool-mechanism/)
>
> [SQLAlchemy 中的 Session、sessionmaker、scoped_session](https://www.cnblogs.com/ChangAn223/p/11277468.html)
>
> [Contextual/Thread-local Sessions](https://docs.sqlalchemy.org/en/13/orm/contextual.html#unitofwork-contextual)
>
> [SQLAlchemy(常用的SQLAlchemy列选项)](https://www.cnblogs.com/xintiao-/p/10376708.html)
>
> [查询官网例子Object Relational Tutorial (1.x API)](https://docs.sqlalchemy.org/en/14/orm/tutorial.html?highlight=from_statement#using-literal-sql)
>
> [sqlalchemy外键和relationship查询](https://www.cnblogs.com/goldsunshine/p/9269880.html#SsSph3tf)

### session和scoped_session

session用于创建程序和数据库之间的会话，所有对象的载入和保存都需通过session对象 。
通过sessionmaker调用创建一个工厂，并关联Engine以确保每个session都可以使用该Engine连接资源
scoped_session 实现了一个线程的隔离，保证不同的线程拿到不同的session, 同一个线程拿到的session 是同一个值

```python
s1 = Session()
s2 = Session()
s1.add(person)
s1.commit()
# 必须先close，s2才能继续操作person
s1.close()
s2.add(person)
```

session 和scoped_session本质上都是用来操作数据库的，只是session 只适合在单线程下面使用
官方文档提到了scoped_session的正确使用方法。request结束后要调用scoped_session.remove()

> [Engine Configuration](http://sunnyingit.github.io/book/section_python/SQLalchemy-engine.html)

使用 `create_engine`创建我们需要的`DB starting point`

```python
from sqlalchemy import create_engine

scheme = 'mysql+pymysql://root:123456@localhost:3306/dev_shopping?charset=utf8'
engine = create_engine(scheme, pool_size=10 , max_overflow=-1, pool_recycle=1200)
```

`create_engine` 函数常用参数：

* pool_size=10 # 连接池的大小，0表示连接数无限制
* pool_recycle=-1 # 连接池回收连接的时间，如果设置为-1，表示没有no timeout, 注意，mysql会自动断开超过8小时的连接，所以sqlalchemy沿用被mysql断开的连接会抛出MySQL has gone away
* max_overflow=-1 # 连接池中允许‘溢出’的连接个数，如果设置为-1，表示连接池中可以创建任意数量的连接
* pool_timeout=30 # 在连接池获取一个空闲连接等待的时间
* echo=False # 如果设置True, Engine将会记录所有的日志，日志默认会输出到sys.stdout

创建`Engine`之后，接下来的问题，就是如何使用`Engine`

在单进程中，建议在在初始化的模块的时候创建`Engine`, 使`Engine`成为全局变量， 而不是为每个调用`Engine`的对象或者函数中创建, `Engine`不同于`connect`, `connect`函数会创建数据库连接的资源，`Engine`是管理`connect`创建的连接资源

在多进程中，为每个子进程都创建各自的`Engine`, 因为进程之间是不能共享`Engine`

### 几种操作方式

> [Working with Engines and Connections](https://docs.sqlalchemy.org/en/14/core/connections.html#basic-usage)
>
> [SqlAlchemy的Engine，Connection和Session 区别？适合什么时候用？](https://blog.csdn.net/qq_36622490/article/details/109850409)

> Engine方式

Engine是SQLAlchemy中连接数据库最底层级别的对象，它维护了一个连接池，可以在应用程序需要和数据库对话时使用。在Engine.execute(close_with_result=True) close_with_result=True 表示连接自动关闭；

```python
result = engine.execute('SELECT * FROM tablename;') 
conn = engine.connect(close_with_result=True)
result = conn.execute('SELECT * FROM tablename;')
for row in result:
    print(result['columnname']
result.close()
```

> Connection方式

Connection，实际上是执行SQL查询的工作，每当你想更好的控制连接的属性，如何时关闭等都建议使用这个操作；比如在一个事务中，要控制它提交commit的时间，在connection控制中就可以运行多个不同的SQL语句，如果其中一个出现问题，则其他所有的语句都会撤销更改；

```python
connection = engine.connect()
trans = connection.begin()
try:
    connection.execute("INSERT INTO films VALUES ('Comedy', '82 minutes');")
    connection.execute("INSERT INTO datalog VALUES ('added a comedy');")
    trans.commit()
except:
    trans.rollback()
    raise
```

> Session方式

Session，一般都是用于ORM中，因为在ORM中，会自动生成SQL语句以及自动连接数据库（自己配置），使用session.execute（）也是个编辑的方法，可以将会话绑定到任何对象；如果你确定使用ORM，就建议使用session来处理execute(),否则还是使用connection更好方便；

> 总结: 从应用角度来看，可以把这三类分为两种：

1. 直接使用Engine.execute( ) 或Connection.execute( )，更加灵活，可以使用原生SQL语句

2. 使用Session处理交易类型的数据，因为方便使用session.add(), session.rollback(), session.commit(), session.close()等，它是使用ORM时推荐的一种和数据库交互的方式





