---
title: docker基本使用
date: '2022/10/9 13:17:57'
top_img: 'https://pic.hycbook.com/i//hexo/post_imgs/蕾姆5.webp'
cover: 'https://pic.hycbook.com/i//hexo/post_cover/蕾姆5.webp'
categories:
  - common
tags:
  - docker
  - neo4j
  - 图数据库
abbrlink: 50622
---

---



# docker

## docker安装

> [docker安装部署neo4j](https://www.cnblogs.com/caoyusang/p/13610408.html)

docker安装分为安装包和命令行两种

{% tabs 充分条件 %}
<!-- tab 安装包 -->

下载[docker](https://neo4j.com/download-center)安装包，按照指示安装即可

<!-- endtab -->

<!-- tab 命令行 -->

```sh
curl -fsSL https://get.docker.com | bash -s docker --mirror Aliyun
```

<!-- endtab -->

{% endtabs %}

> 配置镜像加速

进入[阿里云](https://cr.console.aliyun.com/)，登陆后点击左侧的镜像加速，生成自己的镜像加速地址

执行阿里云推荐的终端命令，即可更新docker的镜像源为阿里云镜像

```sh
sudo mkdir -p /etc/docker
sudo tee /etc/docker/daemon.json <<-'EOF'
{
  "registry-mirrors": ["https://xk5lrhin.mirror.aliyuncs.com"]
}
EOF
sudo systemctl daemon-reload
sudo systemctl restart docker
```

> 代理配置

1. 为docker服务创建一个内嵌的systemd目录

   ```sh
   mkdir -p /etc/systemd/system/docker.service.d
   ```

2. 创建`/etc/systemd/system/docker.service.d/http-proxy.conf`文件，并添加HTTP_PROXY环境变量，其中[proxy-addr]和[proxy-port]分别改成实际情况的代理地址和端口

   ```sh
   [Service]
   Environment="HTTP_PROXY=http://192.168.4.xx:xx" "HTTPS_PROXY=http://192.168.4.xx:xx"
   ```

3. 如果还有内部的不需要使用代理来访问的Docker registries，那么还需要制定NO_PROXY环境变量

   ```sh
   [Service]
   Environment="HTTP_PROXY=http://192.168.4.xx:xx" "HTTPS_PROXY=http://192.168.4.xx:xx" "NO_PROXY=localhost,127.0.0.1,docker-registry.somecorporation.com"
   ```

4. 更新配置并重启Docker服务

   ```sh
   systemctl daemon-reload
   systemctl restart docker
   ```

$$
docker容器 = 镜像 + 可读层
$$

## 基本使用

> 基本命令

```sh
# 查看版本号
docker version
# 启动docker
systemctl start docker
# 关闭docker
systemctl stop docker
# 重启docker
systemctl restart docker
# docker设置随服务启动而自启动
systemctl enable docker
# 查看docker 运行状态
systemctl status docker
```



> 镜像

```sh
# 查看本地镜像
docker images
# 搜索镜像
docker search neo4j
# 拉取镜像
docker pull 镜像名:tag
# 删除镜像 ------当前镜像没有被任何容器使用才可以删除
docker rmi -f 镜像名/镜像ID
# 删除多个 其镜像ID或镜像用用空格隔开即可
docker rmi -f 镜像名/镜像ID 镜像名/镜像ID
# 保存镜像，将我们的镜像保存为tar压缩文件，这样方便镜像转移和保存
# 然后可以在任何一台安装了docker的服务器上加载这个镜像
docker save 镜像名/镜像ID -o 镜像保存在哪个位置与名字
docker save -o my_ubuntu_v3.tar runoob/ubuntu:v3
# 加载镜像
docker load --input my_ubuntu_v3.tar
```



> 容器

```sh
# 创建一个全新的容器，不会立即运行
docker create  --name my_container nginx:latest
# 创建和启动的组合，创建了一个新容器并立即启动它，-d表示后台
docker run --name my_container -d nginx:latest
# 启动已终止容器
docker container start
# 停止容器
docker stop 容器ID或者容器名
# 重启容器
docker restart 容器ID或者容器名

# 列出所有在运行的容器信息
docker ps
# 查看本地所有容器
docker ps -a
# 删除指定容器
docker rm -f <container_id0> <container_id1>
# 删除未启动成功的容器
docker rm $(docker ps -a|grep Created|awk '{print $1}')
# 删除所有未运行的容器
docker rm $(docker ps -a|grep Created|awk '{print $1}')
```



镜像打包工具  Buildpacks、Dockerfile

[Docker, Dockerfile, 和Docker Compose区别 | Baeldung](https://www.baeldung.com/ops/docker-dockerfile-docker-compose)

Docker Compose允许我们定义容器共享的共同对象。例如，我们可以一次性定义一个卷，然后把它挂在每个容器里，这样它们就可以共享一个共同的文件系统。或者，我们可以定义一个或多个容器用来通信的网络。

Docker Compose只是一个协调多个容器的工具。其他选择包括Kubernetes、Openshift和Apache Mesos



[Docker容器迁移到其他服务器的5种方法详解_docker](https://www.anquanclub.cn/5985.html)



# neo4j

## 安装neo4j

> 拉取neo4j镜像

1. 从[docker官方镜像](https://hub.docker.com/search?type=image)中找合适的镜像

   ```sh
   docker search neo4j
   ```

2. 拉取镜像源

   ```sh
   docker pull neo4j(:版本号) # 缺省 ":版本号" 时默认安装latest版本的
   ```

3. 查看本地镜像，检验是否拉取成功

   ```sh
   docker images
   ```

## 图算法插件

> neo4j提供了一系列常用的[图算法](https://neo4j.com/download-center/#ngds)，该算法库需要单独安装，注意版本对应关系

1. 点击上方官网链接，下载`Neo4j Graph Data Science`下的文件，放到`$NEO4J_HOME/plugins`目录中

2. 将以下内容添加到您的`$NEO4J_HOME/conf/neo4j.conf`文件中，[参考这里](https://www.likecs.com/show-204290898.html)

   ```yaml
   dbms.security.procedures.unrestricted=apoc.*,gds.*
   ```

   此配置条目是必需的，因为 GDS 库访问 Neo4j 的低级组件以最大化性能

3. 重启 Neo4j，在docker下就是重启

> 为了验证您的安装，可以输入以下命令

```cmd
RETURN gds.version()
```

要列出所有已安装的算法，请运行以下`gds.list()`过程

```cmd
CALL gds.list()
```

例子: [neo4j实现PageRank算法](https://blog.csdn.net/weixin_51432117/article/details/109965487)

## 启动neo4j

找一个目录存放docker的各镜像运行目录，比如我这里选的是/home/huangyc/docker

然后再在这个目录新建具体的镜像对应的文件夹/home/huangyc/docker/neo4j，在下面新建四个文件夹(实际上好像不需要手动建，执行命令时自动会新建)

- **data**: 数据存放的文件夹
- **logs**: 运行的日志文件夹
- **conf**: 数据库配置文件夹(在配置文件**neo4j.conf**中配置包括开放远程连接、设置默认激活的数据库)
- **import**: 为了大批量导入csv来构建数据库，需要导入的节点文件**nodes.csv**和关系文件**rel.csv**需要放到这个文件夹下
- **plugins**: 存放jar插件

> 启动命令格式如下

```sh
docker run -d --name container_name \  //-d表示容器后台运行 --name指定容器名字
	-p 7474:7474 -p 7687:7687 \  //映射容器的端口号到宿主机的端口号
	-v /home/huangyc/docker/neo4j/data:/data \  //把容器内的数据目录挂载到宿主机的对应目录下
	-v /home/huangyc/docker/neo4j/logs:/logs \  //挂载日志目录
	-v /home/huangyc/docker/neo4j/conf:/var/lib/neo4j/conf   //挂载配置目录
	-v /home/huangyc/docker/neo4j/import:/var/lib/neo4j/import \  //挂载数据导入目录
	-v /home/huangyc/docker/neo4j/plugins:/var/lib/neo4j/plugins \  //挂载数据导入目录
	--env NEO4J_AUTH=neo4j/password \  //设定数据库的用户名和和密码
	neo4j //指定使用的镜像
```

也可以写成单行命令

```sh
docker run -d --name neo4j_hyc -p 7474:7474 -p 7687:7687 -v /home/huangyc/docker/neo4j/data:/data -v /home/huangyc/docker/neo4j/logs:/logs -v /home/huangyc/docker/neo4j/conf:/var/lib/neo4j/conf -v /home/huangyc/docker/neo4j/import:/var/lib/neo4j/import -v /home/huangyc/docker/neo4j/plugins:/var/lib/neo4j/plugins --env NEO4J_AUTH=neo4j/hyc neo4j
```



## py库

使用Python访问图数据库，主要使用的库时`py2neo`和`neo4j`，

* py2neo通过操作python变量，达到操作neo4j的目的，同时也支持`cypher`语法
* neo4j主要时执行CQL(cypher)语句

{% note success modern %}[cypher 语法](https://neo4j.com/docs/cypher-refcard/current/){% endnote %}

{% tabs 充分条件 %}
<!-- tab py2neo -->

安装

```sh
pip install py2neo
```

使用例子

```python
# coding:utf-8
from py2neo import Graph, Node, Relationship
 
# 连接neo4j数据库，输入地址、用户名、密码
graph = Graph("http://192.168.1.106:7474", name="neo4j")
graph.delete_all()
# 创建结点
test_node_1 = Node('ru_yi_zhuan', name='皇帝') # 修改的部分
test_node_2 = Node('ru_yi_zhuan', name='皇后') # 修改的部分
test_node_3 = Node('ru_yi_zhuan', name='公主') # 修改的部分
 
graph.create(test_node_1)
graph.create(test_node_2)
graph.create(test_node_3)
 
# 创建关系
# 分别建立了test_node_1指向test_node_2和test_node_2指向test_node_1两条关系，关系的类型为"丈夫、妻子"，两条关系都有属性count，且值为1。
node_1_zhangfu_node_1 = Relationship(test_node_1, '丈夫', test_node_2)
node_1_zhangfu_node_1['count'] = 1
node_2_qizi_node_1 = Relationship(test_node_2, '妻子', test_node_1)
node_2_munv_node_1 = Relationship(test_node_2, '母女', test_node_3)
 
node_2_qizi_node_1['count'] = 1
 
graph.create(node_1_zhangfu_node_1)
graph.create(node_2_qizi_node_1)
graph.create(node_2_munv_node_1)
 
print(graph)
print(test_node_1)
print(test_node_2)
print(node_1_zhangfu_node_1)
print(node_2_qizi_node_1)
print(node_2_munv_node_1) 
```

<!-- endtab -->

<!-- tab neo4j -->

安装

```sh
pip install neo4j
```

使用例子

```python
from neo4j import GraphDatabase

driver = GraphDatabase.driver("bolt://localhost:7687", auth=("neo4j", "password"))

def add_friend(tx, name, friend_name):
    tx.run("MERGE (a:Person {name: $name}) "
           "MERGE (a)-[:KNOWS]->(friend:Person {name: $friend_name})",
           name=name, friend_name=friend_name)

def print_friends(tx, name):
    for record in tx.run("MATCH (a:Person)-[:KNOWS]->(friend) WHERE a.name = $name "
                         "RETURN friend.name ORDER BY friend.name", name=name):
        print(record["friend.name"])

with driver.session() as session:
    session.write_transaction(add_friend, "Arthur", "Guinevere")
    session.write_transaction(add_friend, "Arthur", "Lancelot")
    session.write_transaction(add_friend, "Arthur", "Merlin")
    session.read_transaction(print_friends, "Arthur")
```

<!-- endtab -->

{% endtabs %}



# nebula

## 基本介绍

NebulaGraph 由三种服务构成：Graph 服务、Meta 服务和 Storage 服务，是一种存储与计算分离的架构

> Meta 服务

Meta 服务是由 `nebula-metad 进程`提供的，用户可以根据场景配置 nebula-metad 进程数量：

- 测试环境中，用户可以在 NebulaGraph 集群中部署 1 个或 3 个 nebula-metad 进程。如果要部署 3 个，用户可以将它们部署在 1 台机器上，或者分别部署在不同的机器上。
- 生产环境中，建议在 NebulaGraph 集群中部署 3 个 nebula-metad 进程。请将这些进程部署在不同的机器上以保证高可用。

所有 nebula-metad 进程构成了基于 Raft 协议的集群，其中一个进程是 leader，其他进程都是 follower。

leader 是由多数派选举出来，只有 leader 能够对客户端或其他组件提供服务，其他 follower 作为候补，如果 leader 出现故障，会在所有 follower 中选举出新的 leader。

> Graph 服务

Graph 服务是由`nebula-graphd 进程`提供，服务主要负责处理查询请求，包括解析查询语句、校验语句、生成执行计划以及按照执行计划执行四个大步骤，查询请求发送到 Graph 服务后，会由如下模块依次处理：

1. **Parser**：词法语法解析模块。
2. **Validator**：语义校验模块。
3. **Planner**：执行计划与优化器模块。
4. **Executor**：执行引擎模块。

> Storage 服务

NebulaGraph 的存储包含两个部分，一个是 Meta 相关的存储，称为 Meta 服务，在前文已有介绍。

另一个是具体数据相关的存储，称为 Storage 服务。其运行在 `nebula-storaged 进程`中

## 环境搭建

> 以[nebula 3.2.1](https://docs.nebula-graph.com.cn/3.2.1/4.deployment-and-installation/2.compile-and-install-nebula-graph/deploy-nebula-graph-cluster/)为例

下载并安装NebulaGraph

```sh
wget https://oss-cdn.nebula-graph.com.cn/package/3.2.1/nebula-graph-3.2.1.el7.x86_64.rpm
sudo rpm -ivh --prefix=/home/huangyc/nebula nebula-graph-3.2.1.el7.x86_64.rpm
```

`--prefix`为可选项，用于指定安装路径

如不设置，系统会将 NebulaGraph 安装到默认路径`/usr/local/nebula/`

> 集群配置

{% note success modern %}集群配置需要保证集群机器[配置ssh免密](https://hycbook.com/article/d192a1af.html){% endnote %}

修改每个服务器上的 NebulaGraph 配置文件

NebulaGraph 的所有配置文件均位于安装目录的`etc`目录内，包括`nebula-graphd.conf`、`nebula-metad.conf`和`nebula-storaged.conf`，用户可以只修改所需服务的配置文件。各个机器需要修改的配置文件如下。

| 机器名称 | 待修改配置文件                                               |
| -------- | ------------------------------------------------------------ |
| A        | `nebula-graphd.conf`、`nebula-storaged.conf`、`nebula-metad.conf` |
| B        | `nebula-graphd.conf`、`nebula-storaged.conf`、`nebula-metad.conf` |
| C        | `nebula-graphd.conf`、`nebula-storaged.conf`、`nebula-metad.conf` |
| D        | `nebula-graphd.conf`、`nebula-storaged.conf`                 |
| E        | `nebula-graphd.conf`、`nebula-storaged.conf`                 |

* 机器 A 配置

  nebula-graphd.conf

  ```sh
  ########## networking ##########
  # Comma separated Meta Server Addresses
  --meta_server_addrs=192.168.10.111:9559,192.168.10.112:9559,192.168.10.113:9559
  # Local IP used to identify the nebula-graphd process.
  # Change it to an address other than loopback if the service is distributed or
  # will be accessed remotely.
  --local_ip=192.168.10.111
  # Network device to listen on
  --listen_netdev=any
  # Port to listen on
  --port=9669
  ```

  nebula-storaged.conf

  ```sh
  ########## networking ##########
  # Comma separated Meta server addresses
  --meta_server_addrs=192.168.10.111:9559,192.168.10.112:9559,192.168.10.113:9559
  # Local IP used to identify the nebula-storaged process.
  # Change it to an address other than loopback if the service is distributed or
  # will be accessed remotely.
  --local_ip=192.168.10.111
  # Storage daemon listening port
  --port=9779
  ```

  nebula-metad.conf

  ```sh
  ########## networking ##########
  # Comma separated Meta Server addresses
  --meta_server_addrs=192.168.10.111:9559,192.168.10.112:9559,192.168.10.113:9559
  # Local IP used to identify the nebula-metad process.
  # Change it to an address other than loopback if the service is distributed or
  # will be accessed remotely.
  --local_ip=192.168.10.111
  # Meta daemon listening port
  --port=9559
  ```

* 机器 D 配置

  nebula-graphd.conf

  ```sh
  ########## networking ##########
  # Comma separated Meta Server Addresses
  --meta_server_addrs=192.168.10.111:9559,192.168.10.112:9559,192.168.10.113:9559
  # Local IP used to identify the nebula-graphd process.
  # Change it to an address other than loopback if the service is distributed or
  # will be accessed remotely.
  --local_ip=192.168.10.114
  # Network device to listen on
  --listen_netdev=any
  # Port to listen on
  --port=9669
  ```

  nebula-storaged.conf

  ```sh
  ########## networking ##########
  # Comma separated Meta server addresses
  --meta_server_addrs=192.168.10.111:9559,192.168.10.112:9559,192.168.10.113:9559
  # Local IP used to identify the nebula-storaged process.
  # Change it to an address other than loopback if the service is distributed or
  # will be accessed remotely.
  --local_ip=192.168.10.114
  # Storage daemon listening port
  --port=9779
  ```

* ABC三台配置除了`local_ip`，其他都一致，DE两台配置除了`local_ip`，其他都一致

启动服务

## 命令学习

使用脚本`nebula.service`管理服务，包括启动、停止、重启、中止和查看

`nebula.service`的默认路径是`/usr/local/nebula/scripts`，如果修改过安装路径，请使用实际路径

> 语法

```sh
sudo /usr/local/nebula/scripts/nebula.service
[-v] [-c <config_file_path>]
<start | stop | restart | kill | status>
<metad | graphd | storaged | all>
```

比如对于非容器部署的 NebulaGraph，执行如下命令启动服务：

```sh
sudo /usr/local/nebula/scripts/nebula.service start all
[INFO] Starting nebula-metad...
[INFO] Done
[INFO] Starting nebula-graphd...
[INFO] Done
[INFO] Starting nebula-storaged...
[INFO] Done
```

执行如下命令查看 NebulaGraph 服务状态

```sh
sudo /usr/local/nebula/scripts/nebula.service status all
```

如果返回如下结果，表示 NebulaGraph 服务正常运行

```
[INFO] nebula-metad(33fd35e): Running as 29020, Listening on 9559
[INFO] nebula-graphd(33fd35e): Running as 29095, Listening on 9669
[WARN] nebula-storaged after v3.0.0 will not start service until it is added to cluster.
[WARN] See Manage Storage hosts:ADD HOSTS in https://docs.nebula-graph.io/
[INFO] nebula-storaged(33fd35e): Running as 29147, Listening on 9779
```

正常启动 NebulaGraph 后，`nebula-storaged`进程的端口显示红色。这是因为`nebula-storaged`在启动流程中会等待`nebula-metad`添加当前 Storage 服务，当前 Storage 服务收到 Ready 信号后才会正式启动服务。从 3.0.0 版本开始，在配置文件中添加的 Storage 节点无法直接读写，配置文件的作用仅仅是将 Storage 节点注册至 Meta 服务中。必须使用`ADD HOSTS`命令后，才能正常读写 Storage 节点。

> 注册

具体是通过nebula-console命令行或者studio页面执行，用 java client 也可以的, 底层逻辑都是通过客户端发送一条 `add hosts xxxxx` query

```sh
ADD HOSTS 192.168.40.39:9779, 192.168.40.40:9779, 192.168.40.41:9779;
```

比如使用python客户端

```python
from nebula3.gclient.net import ConnectionPool
from nebula3.Config import Config

# define a config
config = Config()
config.max_connection_pool_size = 10
# init connection pool
connection_pool = ConnectionPool()
# if the given servers are ok, return true, else return false
ok = connection_pool.init([('192.168.123.xx', 9669)], config)

# option 1 control the connection release yourself
# get session from the pool
session = connection_pool.get_session('root', 'nebula')

session.execute("ADD HOSTS 192.168.123.24:9779")
```

等待20s后，重新查看服务状态，发现警告没有就说明成功了

```sh
[root@localhost nebula]# sudo scripts/nebula.service status all
[WARN] The maximum files allowed to open might be too few: 1024
[INFO] nebula-metad(bb2e684): Running as 11660, Listening on 9559
[INFO] nebula-graphd(bb2e684): Running as 11727, Listening on 9669
[INFO] nebula-storaged(bb2e684): Running as 11764, Listening on 9779
```

> 清空图

- 数据清除后，**如无备份，无法恢复**。使用该功能务必谨慎

  `CLEAR SPACE`不是原子性操作。如果执行出错，请重新执行，避免残留数据

- 图空间中的数据量越大，`CLEAR SPACE`消耗的时间越长

  如果`CLEAR SPACE`的执行因客户端连接超时而失败，可以增大 [Graph 服务配置](https://docs.nebula-graph.com.cn/3.2.1/5.configurations-and-logs/1.configurations/3.graph-config/)中`storage_client_timeout_ms`参数的值

- 在`CLEAR SPACE`的执行过程中，向该图空间写入数据的行为不会被自动禁止

  这样的写入行为可能导致`CLEAR SPACE`清除数据不完全，残留的数据也可能受到损坏

```sh
CLEAR SPACE [IF EXISTS] <space_name>;
```

保留的数据[¶](https://docs.nebula-graph.com.cn/3.2.1/3.ngql-guide/9.space-statements/6.clear-space/#_4)

图空间中，`CLEAR SPACE`不会删除的数据包括：

- Tag 信息
- Edge type 信息
- 原生索引和全文索引的元数据

`DROP SPACE`语句用于删除指定图空间以及其中的所有信息

```sh
DROP SPACE [IF EXISTS] <graph_space_name>;
```

## 客户端

NebulaGraph 支持多种类型的客户端，包括命令行客户端、可视化界面客户端和流行编程语言客户端

详情参见[客户端列表](https://docs.nebula-graph.com.cn/3.2.1/14.client/1.nebula-client/)

{% tabs 客户端 %}
<!-- tab 原生CLI客户端 -->

> 前提条件[¶](https://docs.nebula-graph.com.cn/3.2.1/4.deployment-and-installation/connect-to-nebula-graph/#_1)
>
> [官方文档](https://docs.nebula-graph.com.cn/3.2.1/nebula-console/)

- NebulaGraph 服务已[启动](https://docs.nebula-graph.com.cn/3.2.1/4.deployment-and-installation/manage-service/)

- 运行 NebulaGraph Console 的机器和运行 NebulaGraph 的服务器网络互通

- NebulaGraph Console 的版本兼容 NebulaGraph 的版本

使用命令`arch`，查看系统结构，去[官网](https://github.com/vesoft-inc/nebula-console/releases)下载安装包，重命名文件为`nebula_console`

1. 为用户授予 nebula-console 文件的执行权限

   ```sh
   chmod 111 nebula-console
   ```

2. 在命令行界面中，切换工作目录至 nebula-console 文件所在目录，执行如下命令连接 NebulaGraph

   ```sh
   ./nebula-console -addr <ip> -port <port> -u <username> -p <password> [-t 120] [-e "nGQL_statement" | -f filename.nGQL]
   ```

   例如，要连接到部署在 192.168.10.8 上的 Graph 服务，运行以下命令

   ```sh
   ./nebula_console -addr 192.168.10.8 -port 9669 -u root -p nebula
   ```

<!-- endtab -->

<!-- tab python客户端 -->

已安装 Python，版本为 3.6 及以上，版本对照表[¶](https://docs.nebula-graph.com.cn/3.2.1/14.client/5.nebula-python-client/#_2)

| NebulaGraph 版本 | NebulaGraph Python 版本 |
| ---------------- | ----------------------- |
| 3.2.1            | 3.1.0                   |
| 2.6.x            | 2.6.0                   |

> pip安装

```sh
$ pip install nebula3-python==<version>
```

<!-- endtab -->

{% endtabs %}



> 初始化一些[数据](https://docs.nebula-graph.com.cn/3.2.1/2.quick-start/4.nebula-graph-crud/#_2)

1. 执行如下语句创建名为`basketballplayer`的图空间

   ```sh
   CREATE SPACE basketballplayer(partition_num=15, replica_factor=1, vid_type=fixed_string(32));
   ```

2. 执行命令`SHOW HOSTS`检查分片的分布情况，确保平衡分布

   ```sh
   nebula> SHOW HOSTS;
   +-------------+-----------+-----------+-----------+--------------+----------------------------------+------------------------+---------+
   | Host        | Port      | HTTP port | Status    | Leader count | Leader distribution              | Partition distribution | Version |
   +-------------+-----------+-----------+-----------+--------------+----------------------------------+------------------------+---------+
   | "storaged0" | 9779      | 19669     | "ONLINE"  | 5            | "basketballplayer:5"             | "basketballplayer:5"   | "3.1.0" |
   | "storaged1" | 9779      | 19669     | "ONLINE"  | 5            | "basketballplayer:5"             | "basketballplayer:5"   | "3.1.0" |
   | "storaged2" | 9779      | 19669     | "ONLINE"  | 5            | "basketballplayer:5"             | "basketballplayer:5"   | "3.1.0" |
   +-------------+-----------+-----------+-----------+--------------+----------------------------------+------------------------+---------+
   ```

3. 选择图空间`basketballplayer`

   ```sh
   nebula[(none)]> USE basketballplayer;
   ```

   用户可以执行命令`SHOW SPACES`查看创建的图空间

   ```sh
   nebula> SHOW SPACES;
   +--------------------+
   | Name               |
   +--------------------+
   | "basketballplayer" |
   +--------------------+
   ```

4. 创建 Tag 和 Edge type

   语法:

   ```sh
   CREATE {TAG | EDGE} [IF NOT EXISTS] {<tag_name> | <edge_type_name>}
       (
         <prop_name> <data_type> [NULL | NOT NULL] [DEFAULT <default_value>] [COMMENT '<comment>']
         [{, <prop_name> <data_type> [NULL | NOT NULL] [DEFAULT <default_value>] [COMMENT '<comment>']} ...] 
       )
       [TTL_DURATION = <ttl_duration>]
       [TTL_COL = <prop_name>]
       [COMMENT = '<comment>'];
   ```

   创建 Tag:`player`和`team`，以及 Edge type:`follow`和`serve`。说明如下表。

   | 名称   | 类型      | 属性                             |
   | ------ | --------- | -------------------------------- |
   | player | Tag       | name (string), age (int)         |
   | team   | Tag       | name (string)                    |
   | follow | Edge type | degree (int)                     |
   | serve  | Edge type | start_year (int), end_year (int) |

   ```sh
   CREATE TAG player(name string, age int);
   CREATE TAG team(name string);
   CREATE EDGE follow(degree int);
   CREATE EDGE serve(start_year int, end_year int);
   ```

5. 插入点和边

   - 插入点

     ```sh
     INSERT VERTEX [IF NOT EXISTS] [tag_props, [tag_props] ...]
     VALUES <vid>: ([prop_value_list])
     
     tag_props:
       tag_name ([prop_name_list])
     
     prop_name_list: [prop_name [, prop_name] ...]
     prop_value_list: [prop_value [, prop_value] ...]  
     ```

     `vid`是 Vertex ID 的缩写，`vid`在一个图空间中是唯一的。参数详情请参见 [INSERT VERTEX](https://docs.nebula-graph.com.cn/3.2.1/3.ngql-guide/12.vertex-statements/1.insert-vertex/)

   - 插入边

     ```sh
     INSERT EDGE [IF NOT EXISTS] <edge_type> ( <prop_name_list> ) VALUES 
     <src_vid> -> <dst_vid>[@<rank>] : ( <prop_value_list> )
     [, <src_vid> -> <dst_vid>[@<rank>] : ( <prop_value_list> ), ...];
     
     <prop_name_list> ::= [ <prop_name> [, <prop_name> ] ...]
     <prop_value_list> ::= [ <prop_value> [, <prop_value> ] ...]
     ```

   参数详情请参见 [INSERT EDGE](https://docs.nebula-graph.com.cn/3.2.1/3.ngql-guide/13.edge-statements/1.insert-edge/)

   - 插入代表球员和球队的点

     ```sh
     nebula> INSERT VERTEX player(name, age) VALUES "player100":("Tim Duncan", 42);
     nebula> INSERT VERTEX player(name, age) VALUES "player101":("Tony Parker", 36);
     nebula> INSERT VERTEX player(name, age) VALUES "player102":("LaMarcus Aldridge", 33);
     nebula> INSERT VERTEX team(name) VALUES "team203":("Trail Blazers"), "team204":("Spurs");
     ```

   - 插入代表球员和球队之间关系的边

     ```sh
     nebula> INSERT EDGE follow(degree) VALUES "player101" -> "player100":(95);
     nebula> INSERT EDGE follow(degree) VALUES "player101" -> "player102":(90);
     nebula> INSERT EDGE follow(degree) VALUES "player102" -> "player100":(75);
     nebula> INSERT EDGE serve(start_year, end_year) VALUES "player101" -> "team204":(1999, 2018),"player102" -> "team203":(2006,  2015);
     ```

6. 查询数据[¶](https://docs.nebula-graph.com.cn/3.2.1/2.quick-start/4.nebula-graph-crud/#_7)

   - [GO](https://docs.nebula-graph.com.cn/3.2.1/3.ngql-guide/7.general-query-statements/3.go/) 语句可以根据指定的条件遍历数据库。`GO`语句从一个或多个点开始，沿着一条或多条边遍历，返回`YIELD`子句中指定的信息。

   - [FETCH](https://docs.nebula-graph.com.cn/3.2.1/3.ngql-guide/7.general-query-statements/4.fetch/) 语句可以获得点或边的属性。

   - [LOOKUP](https://docs.nebula-graph.com.cn/3.2.1/3.ngql-guide/7.general-query-statements/5.lookup/) 语句是基于[索引](https://docs.nebula-graph.com.cn/3.2.1/2.quick-start/4.nebula-graph-crud/#_12)的，和`WHERE`子句一起使用，查找符合特定条件的数据。

   - [MATCH](https://docs.nebula-graph.com.cn/3.2.1/3.ngql-guide/7.general-query-statements/2.match/) 语句是查询图数据最常用的，可以灵活的描述各种图模式，但是它依赖[索引](https://docs.nebula-graph.com.cn/3.2.1/2.quick-start/4.nebula-graph-crud/#_12)去匹配 NebulaGraph 中的数据模型，性能也还需要调优。

7. 



## 图模式

模式(pattern)和图模式匹配，是图查询语言的核心功能

> 单点模式

```sql
(a)
```

示例为一个简单的模式，描述了单个点，并使用变量`a`命名该点

> 多点关联模式

多个点通过边相连是常见的结构，模式用箭头来描述两个点之间的边。例如：

```sql
(a)-[]->(b)
```

示例为一个简单的数据结构：两个点和一条连接两个点的边，两个点分别为`a`和`b`，边是有方向的，从`a`到`b`

这种描述点和边的方式可以扩展到任意数量的点和边，例如：

```sql
(a)-[]->(b)<-[]-(c)
```

这样的一系列点和边称为`路径`(path)

只有在涉及某个点时，才需要命名这个点。如果不涉及这个点，则可以省略名称，例如：

```sql
(a)-[]->()<-[]-(c)
```

> Tag模式

nGQL 中的`Tag`概念与 openCypher 中的`Label`有一些不同。例如，必须创建一个`Tag`之后才能使用它，而且`Tag`还定义了属性的类型

模式除了简单地描述图中的点之外，还可以描述点的 Tag。例如：

```sql
(a:User)-[]->(b)
```

模式也可以描述有多个 Tag 的点，例如：

```sql
(a:User:Admin)-[]->(b)
```

> 属性模式

点和边是图的基本结构。nGQL 在这两种结构上都可以增加属性，方便实现更丰富的模型。

在模式中，属性的表示方式为：用花括号括起一些键值对，用英文逗号分隔。例如一个点有两个属性：

```sql
(a {name: 'Andres', sport: 'Brazilian Ju-Jitsu'})
```

在这个点上可以有一条边是：

```sql
(a)-[{blocked: false}]->(b)
```

> 边模式

描述一条边最简单的方法是使用箭头连接两个点。

可以用以下方式描述边以及它的方向性。如果不关心边的方向，可以省略箭头，例如：

```sql
(a)-[]-(b)
```

和点一样，边也可以命名。一对方括号用于分隔箭头，变量放在两者之间。例如：

```sql
(a)-[r]->(b)
```

和点上的 Tag 一样，边也可以有类型。描述边的类型，例如：

```sql
(a)-[r:REL_TYPE]->(b)
```

和点上的 Tag 不同，一条边只能有一种 Edge type。但是如果我们想描述多个可选 Edge type，可以用管道符号（|）将可选值分开，例如：

```sql
(a)-[r:TYPE1|TYPE2]->(b)
```

和点一样，边的名称可以省略，例如：

```sql
(a)-[:REL_TYPE]->(b)
```

> 变长模式

在图中指定边的长度来描述多条边（以及中间的点）组成的一条长路径，不需要使用多个点和边来描述。例如：

```sql
(a)-[*2]->(b)
```

该模式描述了 3 点 2 边组成的图，它们都在一条路径上（长度为 2），等价于：

```sql
(a)-[]->()-[]->(b)
```

也可以指定长度范围，这样的边模式称为`variable-length edges`，例如：

```sql
(a)-[*3..5]->(b)
```

`*3..5`表示最小长度为 3，最大长度为 5。

该模式描述了 4 点 3 边、5 点 4 边或 6 点 5 边组成的图。

也可以忽略最小长度，只指定最大长度，例如：

```sql
(a)-[*..5]->(b)
```

必须指定最大长度，**不支持**仅指定最小长度（`(a)-[*3..]->(b)`）或都不指定（`(a)-[*]->(b)`）。

> 路径变量

一系列连接的点和边称为`路径`。nGQL 允许使用变量来命名路径，例如：

```sql
p = (a)-[*3..5]->(b)
```

可以在 MATCH 语句中使用路径变量

> 注释

支持四种注释方式：`#`、`//`、`/* */`

nGQL 语句中的反斜线（\）代表换行

> 数据类型

字符串的表示方式为用双引号或单引号包裹，例如`"Hello, Cooper"`或`'Hello, Cooper'`

日期和时间的类型，包括`DATE`、`TIME`、`DATETIME`、`TIMESTAMP`和`DURATION`

````sql
nebula> RETURN DATE({year:-123, month:12, day:3});
+------------------------------------+
| date({year:-(123),month:12,day:3}) |
+------------------------------------+
| -123-12-03                         |
+------------------------------------+
nebula> RETURN DATE("23333");
+---------------+
| date("23333") |
+---------------+
| 23333-01-01   |
+---------------+

# 传入当前时间。
nebula> RETURN timestamp();
+-------------+
| timestamp() |
+-------------+
| 1625469277  |
+-------------+

# 传入指定时间。
nebula> RETURN timestamp("2022-01-05T06:18:43");
+----------------------------------+
| timestamp("2022-01-05T06:18:43") |
+----------------------------------+
| 1641363523                       |
+----------------------------------+
````

`date()`支持的属性名称包括`year`、`month`和`day`。`date()`支持输入`YYYY`、`YYYY-MM`或`YYYY-MM-DD`，未输入的月份或日期默认为`01`

`time()`支持的属性名称包括`hour`、`minute`和`second`

`datetime()`支持的属性名称包括`year`、`month`、`day`、`hour`、`minute`和`second`

列表（List）是复合数据类型，一个列表是一组元素的序列，可以通过元素在序列中的位置访问列表中的元素

nGQL 的下标支持从前往后查询，从 0 开始，0 表示第一个元素，1 表示第二个元素，以此类推；也支持从后往前查询，从-1 开始，-1 表示最后一个元素，-2 表示倒数第二个元素，以此类推。

- [M]：表示下标为 M 的元素。
- [M..N]：表示`M ≤ 下标 ＜ N`的元素。`N`为 0 时，返回为空。
- [M..]：表示`M ≤ 下标`的元素。
- [..N]：表示`下标 ＜ N`的元素。`N`为 0 时，返回为空。





## 可视化

NebulaGraph Studio（简称 Studio）是一款可以通过 Web 访问的开源图数据库可视化工具，搭配 [NebulaGraph](https://docs.nebula-graph.com.cn/3.2.1/) 内核使用，提供构图、数据导入、编写 nGQL 查询等一站式服务。

> 前提条件[¶](https://docs.nebula-graph.com.cn/3.2.1/nebula-studio/deploy-connect/st-ug-deploy/#_1)

在部署 RPM 版 Studio 之前，用户需要确认以下信息：

- NebulaGraph 服务已经部署并启动。详细信息，参考 [NebulaGraph 安装部署](https://docs.nebula-graph.com.cn/3.2.1/4.deployment-and-installation/1.resource-preparations/)

- 使用的 Linux 发行版为 CentOS ，已安装 lsof

- 确保以下端口未被占用，`7001`是Studio 提供 web 服务使用的



从官网下载[nebula-graph-studio-3.4.1.x86_64.rpm](https://oss-cdn.nebula-graph.com.cn/nebula-graph-studio/3.4.1/nebula-graph-studio-3.4.1.x86_64.rpm)，使用以下命令安装到指定路径

```sh
sudo rpm -i nebula-graph-studio-3.4.1.x86_64.rpm --prefix=<path> 
```

安装完，自动会启动，当屏幕返回以下信息时，表示 PRM 版 Studio 已经成功启动。

```sh
Start installing NebulaGraph Studio now...
NebulaGraph Studio has been installed.
NebulaGraph Studio started automatically.
```

启动成功后，在浏览器地址栏输入 `http://<ip address>:7001`

**注意**: host处填写为 `ip:port`格式

用户名和密码：根据 Nebula Graph 的身份验证设置填写登录账号和密码

- 如果未启用身份验证，可以填写默认用户名 `root` 和任意密码。
- 如果已启用身份验证，但是未创建账号信息，用户只能以 GOD 角色登录，必须填写 `root` 及对应的密码 `nebula`。
- 如果已启用身份验证，同时又创建了不同的用户并分配了角色，不同角色的用户使用自己的账号和密码登录。

如果在浏览器窗口中能看到以下登录界面，表示已经成功部署并启动 Studio

> 异常处理[¶](https://docs.nebula-graph.com.cn/3.2.1/nebula-studio/deploy-connect/st-ug-deploy/#_4)

如果在安装过程中自动启动失败或是需要手动启动或停止服务，请使用以下命令：

- 手动启动服务

  ```sh
  bash /usr/local/nebula-graph-studio/scripts/rpm/start.sh
  ```

- 手动停止服务

  ```sh
  bash /usr/local/nebula-graph-studio/scripts/rpm/stop.sh
  ```

如果启动服务时遇到报错报错 `ERROR: bind EADDRINUSE 0.0.0.0:7001`，用户可以通过以下命令查看端口 7001 是否被占用。

```sh
lsof -i:7001
```

如果端口被占用，且无法结束该端口上进程，用户可以通过以下命令修改 Studio 服务启动端口，并重新启动服务。

```sh
//修改 studio 服务启动端口
vi etc/studio-api.yam
//修改
Port: 7001 // 修改这个端口号，改成任意一个当前可用的即可
//重启服务
systemctl restart nebula-graph-studio.service
```



## 集群监控

[`NebulaGraph Dashboard`](https://docs.nebula-graph.com.cn/3.2.1/nebula-dashboard/1.what-is-dashboard/)(简称Dashboard)是一款用于监控NebulaGraph集群中机器和服务状态的可视化工具

## 数据导入

> [Nebula Importer 数据导入实践](https://baijiahao.baidu.com/s?id=1737412299821419025&wfr=spider&for=pc)

Nebula 目前作为较为成熟的产品，已经有着很丰富的生态。数据导入的维度而言就已经提供了多种选择。有大而全的Nebula Exchange，小而精简的Nebula Importer, 还有为 Spark / Flink 引擎提供的Nebula Spark Connector 和 Nebula Flink Connector。

使用场景介绍：

- Nebula Exchange

- - 需要将来自 Kafka、Pulsar 平台的流式数据, 导入 Nebula Graph 数据库
  - 需要从关系型数据库（如 MySQL）或者分布式文件系统（如 HDFS）中读取批式数据
  - 需要将大批量数据生成 Nebula Graph 能识别的 SST 文件

- Nebula Importer

- - Importer 适用于将本地 CSV 文件的内容导入至 Nebula Graph 中

- Nebula Spark Connector:

- - 在不同的 Nebula Graph 集群之间迁移数据
  - 在同一个 Nebula Graph 集群内不同图空间之间迁移数据
  - Nebula Graph 与其他数据源之间迁移数据
  - 结合 Nebula Algorithm 进行图计算

- Nebula Flink Connector

- - 在不同的 Nebula Graph 集群之间迁移数据
  - 在同一个 Nebula Graph 集群内不同图空间之间迁移数据
  - Nebula Graph 与其他数据源之间迁移数据

总体来说，Exchange 大而全，可以和大部分的存储引擎结合，导入到 Nebula 中，但是需要部署Spark 环境。

Importer 使用简单，所需依赖较少，但需要自己提前生成数据文件，配置好 schema 一劳永逸，但是不支持断点续传，适合数据量中等。

Spark / Flink Connector 需要和流数据结合。

不同的场景选择不同的工具，如果作为新人使用 Nebula 在导入数据时，建议使用 Nebula Importer 工具，简单快速上手。
