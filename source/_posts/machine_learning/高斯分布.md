---
title: 高斯分布相关算法
date: '2022/12/17 20:53:18'
top_img: 'https://pic.hycbook.com/i//hexo/post_imgs/蕾姆2.webp'
cover: 'https://pic.hycbook.com/i//hexo/post_cover/蕾姆2.webp'
categories:
  - machine_learning
tags:
  - 机器学习
  - 高斯分布
mathjax: true
description: 高斯分布相关算法
---



---



---



# 学派

对概率的诠释有两大学派，一种是频率派另一种是贝叶斯派，后面我们对观测集采用下面记号：
$$
X_{N\times p}=(x_{1},x_{2},\cdots,x_{N})^{T},x_{i}=(x_{i1},x_{i2},\cdots,x_{ip})^{T}
$$
$X$这个矩阵展开如下
$$
X=\left(\begin{array}{cccc}x_{11} & x_{12} & \cdots & x_{1 p} \\ x_{21} & x_{22} & \cdots & x_{2 p} \\ \vdots & & & \\ x_{N 1} & x_{N 22} & \cdots & x_{N p}\end{array}\right)_{N \times p}
$$
表示有$N$个样本，每个样本都是$p$维向量，其中假设每个观测都是由 $p(x|\theta)$生成的

## 频率派

频率派认为$p(x|\theta)$中的$\theta$是一个未知的常量，而数据是一个随机变量，关心的是数据，目标是估计未知的的常量$\theta$

常用的方法是`最大似然估计`
$$
\theta_{MLE}=\mathop{argmax}\limits _{\theta}\log p(X|\theta) \mathop{=} \mathop{argmax}\limits _{\theta}\prod\limits _{i=1}^{N}\log p(x_{i}|\theta) \mathop{=} \limits _{iid}\mathop{argmax}\limits _{\theta}\sum\limits _{i=1}^{N}\log p(x_{i}|\theta)
$$
其中每个样本$x_{i}$独立同分布于$P(x \mid \theta)$，因此上面的累乘可以改写成累加的形式

## 贝叶斯派

贝叶斯派认为$p(x|\theta)$中的$\theta$是一个随机变量，且$\theta$服从一定的概率分$\theta\sim p(\theta)$，通常将$p(\theta)$成为`先验`

根据贝叶斯定理，将参数的`先验`和`后验`用`似然函数`联系起来
$$
p(\theta|X) = \frac{p(X|\theta)\cdot p(\theta)}{p(X)} = \frac{p(X|\theta)\cdot p(\theta)}{\int _{\theta}p(X|\theta)\cdot p(\theta)d\theta} \propto p(X|\theta)\cdot p(\theta)
$$
其中先验是$ p(\theta)$，后验是$p(\theta|X)$，似然函数为$p(\theta){p(X)}$

> 最大后验估计

为了估计$\theta$的值，我们使用`最大后验估计MAP`进行求解，其目的是找到一个$\theta$使用估计出来的结果最大
$$
\theta_{MAP}=\mathop{argmax}\limits _{\theta}p(\theta|X)  = \mathop{argmax}\limits _{\theta}\frac{p(X|\theta)\cdot p(\theta)}{p(X)} \propto \mathop{argmax}\limits _{\theta}p(X|\theta)\cdot p(\theta)
$$
其中$\propto$是因为对于分类问题而言$p(X)$都是一样的，可以看成一个常数，严格意义上MAP并不是标准的贝叶斯方法

> 贝叶斯估计

真正的`贝叶斯估计`是实实在在的去求解后验概率
$$
p(\theta|X) = \frac{p(X|\theta)\cdot p(\theta)}{p(X)} = \frac{p(X|\theta)\cdot p(\theta)}{\int _{\theta}p(X|\theta)\cdot p(\theta)d\theta}
$$
其中$\int _{\theta}p(X|\theta)\cdot p(\theta)d\theta$通常求解难度较大，因此可以随机采样算法，如`蒙特卡洛`MCMC算法进行近似求解

求出来的后验概率可以在`贝叶斯预测`时使用，假设这时候来了个新样本$\tilde{x}$，预测问题就是求$p(\tilde{x}|X)$

通过$\theta$这个桥梁，即$X \rightarrow \theta \rightarrow \tilde{x}$，上式可以变换如下
$$
p(\tilde{x}|X) = \int _{\theta} p(\tilde{x}, \theta|X) d \theta = \int _{\theta} p(\tilde{x}| \theta) \cdot p(\theta | X) d \theta
$$
其中$p(\theta | X)$就是贝叶斯估计求出来的后验

## 小结

`频率派`和`贝叶斯派`分别给出了一系列的机器学习算法

频率派的观点导出了一系列的`统计机器学习`算法，而贝叶斯派导出了`概率图`理论

{% label success@频率派主要对应的问题是优化问题 %}

频率派主要对应的问题是优化问题，通常的步骤是

1. 定义模型
2. 定义loss function
3. 优化算法

贝叶斯派主要是求积分的问题

{% note info modern %}条件概率与似然函数{% endnote %}

在极大似然估计相关博文中，出现频率最高的就是这个公式:
$$
P(x \mid \theta)
$$
公式的输入分别为参数$\theta$以及结果$x$
根据$\theta$和$x$的已知或者末知的情况，该公式有两个不同的意义:

- 当$\theta$是已知的并且保持不变，$x$是变量时，该公式描述的是在参数确定的情况下，某一事件(结果)$x$出现的概率，是`概率函数`
- 当$\theta$是变量，$x$是已知的并且保持不变，该公式描述的是事件(结果)在不同$\theta$下出现的概率，是`似然函数`

在后面极大似然估计中，用到的是就是似然函数

在似然的意义下，$P(x \mid \theta)$还可以写成$p\left(x_{i} ; \theta\right)$，一般情况下为了特别的区分似然函数和概率函数，都会采用后面的写法




# 高斯分布

> 笔记整理自：Bilibili站上[shuhuai008](https://www.bilibili.com/video/BV1aE411o7qd/?p=3)强势手推讲解的[白板推导CRF系列课程](https://www.bilibili.com/video/av34444816/?p=1)，课程质量很高！
>
> [B站scyw读者整理的笔记](https://www.yuque.com/bystander-wg876/yc5f72/hu0291)

高斯分布在机器学习中占有举足轻重的作用，尤其在统计机器学习中，比如线性高斯模型

线性高斯模型是一个体系，比如`卡曼滤波`，隐变量服从线性高斯分布，即$$Z_t = A Z_{t-1}+B+\epsilon$$，其中$$\epsilon$$是一个高斯噪声


$$
\begin{array}{l}  X=\left(x_{1}, x_{2}, \cdots, x_{N}\right)^{\top}=\left(\begin{array}{c}x_{1}^{\top} \\ x_{1}^{\top} \\ \vdots \\ x_{N}^{\top}\end{array}\right)_{N \times p} \end{array}
$$
其中$$x_{i} \in \mathbb{R}^{p}$$，$$x_{i}$$独立同分布于$N(\mu, \Sigma)$

## 参数估计

高斯分布下的MLE可以表述为
$$
\theta_{MLE}= \mathop{argmax}\limits _{\theta}\log p(X|\theta)\mathop{=}\limits _{iid}\mathop{argmax}\limits _{\theta}\sum\limits _{i=1}^{N}\log p(x_{i}|\theta)
$$
这里先写出一维的高斯分布的概率密度函数
$$
p(x)=\frac{1}{\sqrt{2 \pi} \sigma} \exp \left(-\frac{(x-\mu)^{2}}{2 \sigma^{2}}\right)
$$
高维的高斯分布密度函数为
$$
p(x)=\frac{1}{(2 \pi)^{\frac{p}{2}}|\Sigma|^{\frac{1}{2}}} \exp \left(-\frac{1}{2}(x-\mu)^{T} \Sigma^{-1}(x-\mu)\right)
$$

### 一维情况

假设$p=1$，$\theta=(\mu, \sigma ^2)$，将一维高斯分布的概率密度函数带入到MLE中，可以得到
$$
\begin{aligned} \log P(x \mid \theta) & =\log \prod_{i=1}^{N} p\left(x_{i} \mid \theta\right) \mathop{=}\limits _{iid} \sum_{i=1}^{N} \log p\left(x_{i} \mid \theta\right) \\ & =\sum_{i=1}^{N} \log \frac{1}{\sqrt{2 \pi} \sigma} \exp \left(-\frac{\left(x_{i}-\mu\right)^{2}}{2 \sigma^{2}}\right) \\ & =\sum_{i=1}^{N}\left[\log \frac{1}{\sqrt{2 \pi}}+\log \frac{1}{\sigma}-\frac{\left(x_{i}-\mu\right)^{2}}{2 \sigma^{2}}\right]\end{aligned}
$$

> 求解$\mu$

我们的目标是求解参数$\theta$，这里先求解$\mu$，可以得到
$$
\begin{array}{l} \mu_{M L E}=\arg \max _{\mu} \log P(x \mid \theta) \\ =argmax \sum_{i=1}^{N}-\frac{\left(x_{i}-\mu\right)^{2}}{2 \sigma^{2}} \\ =argmin _{\mu} \sum_{i=1}^{N}\left(x_{i}-\mu\right)^{2} 
\end{array}
$$
对$\mu$求偏导可以得到
$$
\begin{array}{l}

\frac{\partial}{\partial \mu} \sum_{i=1}^{N} \left(x_{i}-\mu\right)^{2}=\sum_{i=1}^{N} 2 \cdot\left(x_{i}-\mu\right) \cdot(-1)=0 
\\ 

\sum_{i=1}^{N}\left(x_{i}-\mu\right) = \sum_{i=1}^{N} x_{i}-\sum_{i=1}^{N} \mu =
 \sum_{i=1}^{N} x_{i}- N \mu=0 
\\ \mu_{M L E}=\frac{1}{N} \sum_{i=1}^{N} x_{i}

\end{array}
$$
这里的$\mu$是`无偏估计`，因为如果对$\mu$求期望，可以得到
$$
\begin{aligned} & E\left[\mu_{M L E}\right] = \frac{1}{N} \sum_{i=1}^{N} E\left[x_{i}\right] = \frac{1}{N} \sum_{i=1}^{N} \mu =  \frac{1}{N} \cdot N \cdot \mu =  \mu\end{aligned}
$$


> 求解$\Sigma$

同理
$$
\begin{array}{l} \sigma_{MLE}^{2} = argmax _{\sigma} P(x | \sigma) 

\\ = argmax \sum _{i=1}^{N} \left(-\log \sigma-\frac{1}{2 \sigma^{2}}\left(x_{i}-\mu\right)^{2}\right) = argmax (\mathcal{L}(\sigma)) \end{array}
$$
对$\sigma$求偏导，可以得到
$$
\begin{array}{l} 

\frac{\partial \mathcal{L}}{\partial \sigma} = \sum_{i=1}^{N}\left[-\frac{1}{\sigma}+\frac{1}{2}\left(x_{i}-\mu\right)^{2} \cdot2 \sigma^{-3}\right]=0 \Rightarrow\sum_{i=1}^{N}\left[-\frac{1}{\sigma}+\left(x_{i}-\mu\right)^{2} \cdot \sigma^{-3}\right]=0 

\\ \Rightarrow \sum_{i=1}^{N}\left[-\sigma^{2}+\left(x_{i}-\mu\right)^{2}\right]=0 \\ -\sum_{i=1}^{N} \sigma^{2}+\sum_{i=1}^{N}\left(x_{i}-\mu\right)^{2}=0 \\ \sum_{i=1}^{N} \sigma^{2}=\sum_{i=1}^{N}\left(x_{i}-\mu\right)^{2} \\ \sigma_{M L E}^{2}=\frac{1}{N} \sum_{i=1}^{N}\left(x_{i}-\mu\right)^{2}\end{array}
$$
这里的$\mu$实际上应该时$\mu _{MLE}$，所以继续推导可以得到
$$
\begin{array}{l}  \sigma_{MLE}^{2}=\frac{1}{N} \sum_{i=1}^{N}\left(x_{i}-\mu_{M L E}\right)^{2}=\frac{1}{N} \sum_{i=1}^{N}\left(x_{i}^{2}-2 x_{i} \mu_{M L E}+\mu_{M L E}^{2}\right) \\ = \frac{1}{N} \sum_{i=1}^{N} x_{i}^{2}-\frac{1}{N} \sum_{i=1}^{N} 2 x_{i} \mu_{M L E}+\frac{1}{N} \sum_{i=1}^{N} \mu_{M L E}^{2} \\  =\frac{1}{N} \sum_{i=1}^{N} x_{i}^{2}-2 \cdot \mu_{M L E}^{2} + \mu_{M L E}^{2}  = \frac{1}{N} \sum_{i=1}^{N} x_{i}^{2}-\mu_{MLE}^{2}\end{array}
$$
上式中的$\mu$严格意义上应该是$$\mu_{MLE}$$，此时$$\sigma_{M L E}^{2}$$是`有偏估计`，我们对$\sigma_{M L E}^{2}$求期望可以得到
$$
\begin{array}{l} 
E[\sigma_{M L E}^{2}] = E[\frac{1}{N} \sum_{i=1}^{N} x_{i}^{2}-\mu_{MLE}^{2}] = E[(\frac{1}{N} \sum_{i=1}^{N} x_{i}^{2} - \mu ^2) - (\mu_{MLE}^{2} - \mu ^2)] 
\\ = E[(\frac{1}{N} \sum_{i=1}^{N} x_{i}^{2} - \mu ^2)] - E[(\mu_{MLE}^{2} - \mu ^2)] 
\\ = E[\frac{1}{N} \sum_{i=1}^{N} (x_{i}^{2} - \mu ^2)] - (E[\mu_{MLE}^{2}] - E[\mu ^2]) 
\\ = \frac{1}{N} \sum_{i=1}^{N} E[x_{i}^{2} - \mu ^2] - (E[\mu_{MLE}^{2}] - \mu ^2)
\\ = \frac{1}{N} \sum_{i=1}^{N} (E[x_{i}^{2}] - \mu ^2) - (E[\mu_{MLE}^{2}] - E^2[\mu _{MLE} ])
\\ = \frac{1}{N} \sum_{i=1}^{N} Var(x_i) - Var(\mu _{MLE})
\\ = \frac{1}{N} \sum_{i=1}^{N} \sigma ^2 - Var(\mu _{MLE})
\\ = \sigma ^2 - \frac{\sigma^{2}}{N}
 = \frac {N-1}{N} \sigma ^2 
\end{array}
$$
**注意以上公式推导中用到了以下的公式变换**

其中$E[x_i^2]-\mu^2 = Var(x_i)$的变换过程如下
$$
\begin{aligned}
Var(X) = E[(X-E[X])^2] = E[(X-\mu)^2] = E[X^2-2x \mu + \mu ^2] = E[X^2]-2E[x\mu]+E[\mu^2]
\\ = E[X^2]-2 \mu E[X]+\mu ^2 = E[X^2]-2\mu \cdot \mu + \mu^2 = E[X^2]+\mu^2
\end{aligned}
$$
$E[\mu _{MLE}^2] - E^2[\mu _{MLE}] = Var(\mu _{MLE})$的变换过程如下
$$
Var(X) = E[X^2]-2 \mu E[X]+\mu ^2 = E[X^2]-2E^2[X]+E^2[X] = E[X^2]-E^2[X]
$$
$Var(\mu _{MLE}) = \frac {\sigma ^2}{N}$的变换过程如下
$$
\begin{aligned}
Var(\mu) = Var(\frac {1}{N} \sum _{i=1}^{N}{x_i}) = \frac {1}{N^2}{Var(\sum _{i=1}^{N}{x_i})}
\\ = \frac {1}{N^2}{\sum _{i=1}^{N}Var({x_i})} = \frac {1}{N^2} \cdot N \cdot \sigma^2 = \frac {1}{\sigma^2}
\end{aligned}
$$
至此可以看到有偏体现在了$\frac {N-1}{N}$，其实方差被往小的方向估计了，其中
$$
\operatorname{Var}\left[\mu_{\text {MLE}}\right]=\operatorname{Var}\left[\frac{1}{N} \sum_{i=1}^{N} x_{i}\right]=\frac{1}{N^{2}} \sum_{i=1}^{N} \operatorname{Var}\left(x_{i}\right)=\frac{1}{N^{2}} \cdot \sum_{i=1}^{N} \sigma^{2}=\frac{1}{N^{2}} \cdot N \cdot \sigma^{2}=\frac{\sigma^{2}}{N}
$$
因此真实的`无偏估计`$\hat{\sigma}$应该是
$$
\hat{\sigma} = \frac {N}{N-1} \sigma_{M L E}^{2} = \frac {N}{N-1} \cdot \frac{1}{N} \sum_{i=1}^{N}\left(x_{i}-\mu_{M L E}\right)^{2}
\\ = \frac{1}{N-1}\sum\limits _{i=1}^{N}(x_{i}-\mu_{MLE})^{2}
$$

### 多维情况

> 高维的高斯分布

高维的高斯分布密度函数为
$$
p(x)=\frac{1}{(2 \pi)^{\frac{p}{2}}|\Sigma|^{\frac{1}{2}}} \exp \left(-\frac{1}{2}   \underbrace{(x-\mu)^{T} \Sigma^{-1}(x-\mu)}_{二次型}  \right)
$$

其中$x \in \mathbb{R}^p$，是一个$p$维随机变量，$\mu$也是$p$维变量，$\Sigma$是$p \times p$维的矩阵，为协方差矩阵
$$
x=\left(\begin{array}{c}x_{1} \\ x_{2} \\ \vdots \\ x_{p}\end{array}\right) \qquad \mu=\left(\begin{array}{c}\mu_{1} \\ \mu_{1} \\ \vdots \\ \mu_{p}\end{array}\right) \qquad \Sigma=\left(\begin{array}{cccc}\sigma_{11} & \sigma_{12} & \cdots \sigma_{1 p} \\ \sigma_{22} & \sigma_{22} & \cdots \sigma_{2 p} \\ \vdots & \vdots & \vdots \\ \sigma_{p 1} & \sigma_{p2} & \cdots \sigma_{p p}\end{array}\right)_{p \times p}
$$
一般情况下$\Sigma$是半正定的，并且是对称的，这里假设$\Sigma$是正定的，方便后面叙述

这里$(x-\mu)^{T} \Sigma^{-1}(x-\mu)$计算出来的结果是一个数，可以看作$x$和$\mu$的`马氏距离`

> 马氏距离例子

设
$$
Z_1=\left(\begin{array}{c} z_{11} \\ z_{12}\end{array}\right) \qquad 
Z_2=\left(\begin{array}{c} z_{21} \\ z_{22}\end{array}\right)
$$
此时马氏距离为
$$
\left(Z_{1}-Z_{2}\right)^{T} \Sigma^{-1}\left(Z_{1}-Z_{2}\right) = 
\left(\begin{array}{c} z_{11}-z_{21} & z_{21}-z_{22}\end{array}\right) \Sigma^{-1}  \left(\begin{array}{c} z_{11}-z_{21} \\ z_{21}-z_{22}\end{array}\right)
$$
当$\Sigma=I$为单位矩阵时，此时马氏距离就变成了`欧氏距离`，即
$$
\left(Z_{1}-Z_{2}\right)^{T} \Sigma^{-1}\left(Z_{1}-Z_{2}\right) = (z_{11}-z_{21})^2 + (z_{21}-z_{22})^2
$$

> 协方差矩阵计算

对于对称的协方差矩阵$\Sigma$可进行特征值分解
$$
\Sigma = U\Lambda U^{T} \qquad st. \quad UU^T=U^TU=I \qquad \Lambda=diag(\lambda_i)
$$
其中$U = (u_{1},u_{2},\cdots,u_{p})$，开始推导
$$
\begin{aligned}
\Sigma = U\Lambda U^{T} = \left(\begin{array}{llll}u_{1} & u_{2} & \cdots & u_{p}\end{array}\right)\left(\begin{array}{ccc}\lambda_{1} & \cdots & 0 \\ 0 & \lambda_{2} & \vdots \\ \vdots & & \lambda _p \end{array}\right)\left(\begin{array}{c}u_{1}^{T} \\ u_{2}^{T} \\ \vdots \\ u_{p}^{T}\end{array}\right) 
\\ = \left(\begin{array}{llll}u_{1} \lambda_{1} & u_{2} \lambda_{2} & \cdots & u_{p} \lambda_{\rho}\end{array}\right)\left(\begin{array}{c}u_{1}^{T} \\ u_{2}^{T} \\ \vdots \\ u_{p}^{T}\end{array}\right) = \sum_{i=1}^{p} u_{i} \lambda_{i} u_{i}^{T}
\end{aligned}
$$
同理可以推得
$$
\Sigma ^{-1} = (U\Lambda U^{T})^{-1} = U \Lambda ^{-1} U^T = \sum_{i=1}^{p} u_{i} \frac {1}{\lambda_{i}} u_{i}^{T}
$$

> 二次型推导

接下来对二次型$\Delta=(x-\mu)^{T}\Sigma^{-1}(x-\mu)$进行推导

$$
\Delta=(x-\mu)^{T}\Sigma^{-1}(x-\mu) = (x-\mu)^{T}  \cdot \sum\limits _{i=1}^{p}u_{i}\frac{1}{\lambda_{i}}u_{i}^{T} \cdot (x-\mu) = \sum\limits _{i=1}^{p}(x-\mu)^{T}  \cdot u_{i}\frac{1}{\lambda_{i}} \cdot u_{i}^{T}(x-\mu) 

= \sum\limits _{i=1}^{p}\frac{y_{i}^{2}}{\lambda_{i}}
$$
令
$$
y=\left(\begin{array}{c}y_{1} \\ y_{2} \\ \vdots \\ y_{p}\end{array}\right) \qquad
y_i = (x-\mu)^T u_i
$$
所以有
$$
\sum _{i=1}^{p}(x-\mu)^{T}  \cdot u_{i}\frac{1}{\lambda_{i}} \cdot u_{i}^{T}(x-\mu) 
= \sum _{i=1}^{p} y_i \frac {1}{\lambda_i}y_i^T
= \sum _{i=1}^{p}\frac{y_{i}^{2}}{\lambda_{i}}
$$
那么这里的马氏距离怎么理解呢，这里先令$p=2$进行分析，此时
$$
\Delta=\frac{y_{1}^{2}}{\lambda_{1}}+\frac{y_{2}^{2}}{\lambda_{2}}=1
$$
这里的1是我们给定的，此时表示的一个椭圆



...







