---
title: 机器学习_线性代数与矩阵论
date: '2022/10/11 20:38:22'
top_img: 'https://pic.hycbook.com/i/hexo/post_imgs/蕾姆1.webp'
cover: 'https://pic.hycbook.com/i/hexo/post_cover/蕾姆1.webp'
categories:
  - math
tags:
  - python
  - 机器学习数学
  - 线性代数
  - 矩阵论
mathjax: true
description: 机器学习的数学基础入门知识
swiper_index: 7
abbrlink: 47032
---

---



# 向量及其运算

## 基本概念

{% note success modern %}线性代数是多元函数微积分的基础{% endnote %}

> 定义

`向量(Vector)`是具有大小和方向的量，是由多个数构成的一维数组，每个数称为向量的分量，向量分量的数量称为向量的`维数`

> 向量的表示

物理中的力、速度以及加速度是典型的向量，$n$维向量$x$有$n$个分量，可以写成行向量的形式$(x_1 \cdots x_n)$

通常将向量写成**小写黑体斜体**字符，如果写成列的形式则称为列向量，这些分量在列方向排列
$$
\begin{bmatrix}
x_1 \\
\vdots \\
x_n
\end{bmatrix}
$$
这些向量的分量是实数，则成为实向量，如果是复数，则成为复向量，$n$维实向量的集合记为$\mathbb{R}^n$

与向量相对的是`标量(Scalar)`，标量只有大小而无方向，物理中的时间、质量以及电流是典型的标量

在数学中通常把向量表示成列向量，而计算机中通常按行存储

二维平面内的一个向量，其在$x$轴方向和$y$轴方向的分量分别为$3$和$1$，写成行向量形式为$\left[ \begin{matrix} 3 & 1 \end{matrix} \right]$

![二维平面内的向量](https://pic.hycbook.com/i//hexo/bk_resources/math/机器学习_线性代数与矩阵论/二维平面内的向量.webp)

图中的向量以虚线箭头表示，起点为原点，终点是以向量的分量为坐标的点，三维空间中的力是三维向量，写成向量形式为
$$
\left[ \begin{matrix} F_x & F_y & F_z \end{matrix} \right]
$$
力的加法遵守`平行四边形法则`

> 零向量

所有分量全为$0$的向量称为`零向量`，即为$0$，它的方向是不确定的

向量与空间的点是一一对应的，向量$x$是以原点为起点，以$x$点为终点

在机器学习中，样本数据通常用向量的形式表达，称为`特征向量(Feature Vectos)`，用于描述样本的特征

但是这里的特征向量和矩阵的特征向量是不同的概念，不要混淆



## 基本运算

`转置运算`(Transpose)将列向量变成行向量，将列向量转行向量，向量$x$的转置记为$x^T$
$$
\left[  
  \begin{matrix}
   1 & 0 & 0
  \end{matrix}
\right]^T = 
\left[  
  \begin{matrix}
   1 \\
   0 \\
   1
  \end{matrix} 
\right]
$$

> 加法

两个向量的加法定义为对应分量相加，要求参与运算的两个向量`维数相等`

向量$x$和$y$相加记为$x+y$，比如$\left[ \begin{matrix} 1 & 0 & 0 \end{matrix} \right] + \left[ \begin{matrix} 4 & 0 & 1 \end{matrix} \right] = \left[ \begin{matrix} 5 & 2 & 4 \end{matrix} \right]$

这与力的加法的平行四边形法则一致，是其在高维空间的推广

![向量的加法](https://pic.hycbook.com/i//hexo/bk_resources/math/机器学习_线性代数与矩阵论/向量的加法.webp)

向量满足`交换律`和`结合律`
$$
x+y=y+x \qquad x+y+z=x+(y+z)
$$

> 减法

两个向量的减法为它们对应分量相减，同样要求参与运算的两个向量`维数相等`

与向量加法的平行四边形法则相对应，向量减法符合三角形法则，$x-y$的结果是以$y$为起点，以$x$为终点的向量

> 乘积

向量$x$与标量$k$的乘积$kx$定义为标量与向量的每个分量相乘，比如
$$
5 \times \left[ \begin{matrix} 1 & 0 & 0 \end{matrix} \right] = \left[ \begin{matrix} 5 \times 2 & 5 \times 3 & 5 \times 1 \end{matrix} \right] = \left[ \begin{matrix} 10 & 15 & 5 \end{matrix} \right]
$$
乘积运算可以改变向量的大小和方向

加法和数乘满足分配律
$$
k(x+y) = kx + ky
$$

> 内积

两个向量$x$和$y$`内积(Inner Product)`定义为它们对应分量乘积之和
$$
x^{T}y = \sum _{i=1}^{n}{x_iy_i}
$$
内积可以记为$x \cdot y$
$$
\left[  
  \begin{matrix}
   1 \\
   2 \\
   3
  \end{matrix}
\right]^T
\left[  
  \begin{matrix}
   1 \\
   0 \\
   1
  \end{matrix}
\right] = 1 \times 1 + 2 \times 0 + 3 \times 1 = 4
$$
两个$n$维向量的内积运算需要执行$n$次乘法运算和$n-1$次加法运算

内积运算满足下面的规律
$$
x^Ty = y^Tx \qquad (kx)^Ty=kx^Ty \\ \qquad
(x+y)^Tz=x^T+y^Tz \qquad z^T(x+y)=z^Tx+z^Ty
$$
利用内积可以简化线性函数(一次函数)的表述

对于机器学习中广泛使用的线性模型的预测函数$\omega _1x_1 + \cdots \omega _nx_n + b$

定义系数(权重)向量$\omega = (\omega _1 \cdots \omega _n)^T$，输入向量$x = (x_1 \cdots x_n)^T$，$b$为偏置项，预测函数写成向量内积的话为
$$
\omega ^Tx + b
$$
向量与自身内积的结果为其所有分量的平方和，即$x^Tx=\sum _{i=1}^{n}{x_i^2}$

{% note primary modern %}两个向量的内积为$0$，则称它们`正交`{% endnote %}

正交是**几何垂直**这一概念在高维空间的推广
$$
\left[  
  \begin{matrix}
   1 \\
   0 \\
   0
  \end{matrix}
\right]^T
\left[  
  \begin{matrix}
   0 \\
   1 \\
   0
  \end{matrix}
\right] = 0
$$

> 阿达玛积

两个向量的`阿达玛(Hadamard)`积定义为它们对应分量相乘，结果为相同维数的向量，记为$x \odot y$

对于两个向量
$$
x = (x_1 \cdots x_n)^T \qquad y = (y_1 \cdots y_n)^T
$$
它们的阿达玛积为
$$
x \odot y = ({x_1}{y_1} \cdots {x_n}{y_n})^T
$$
阿达玛积可以简化问题的表述，在反向传播算法、各种梯度下降法中被使用

## 向量的范数

> 定义

向量的`范数(Norm)`是向量的模(长度)这一概念的推广，向量的$L-p$`范数是一个标量`，定义为
$$
||x||_p = (\sum _{i=1}^{n}{|x_i|^p})^{\frac {1}{p}}
$$
$p$为整数，常用的是$L1$和$L2$范数，$p$的取值分别为$1$和$2$



{% tabs 范数 %}
<!-- tab L1范数 -->

$L1$范数是所有分量的绝对值之和
$$
||x||_1 = \sum _{i=1}^{n}{|x|_i}
$$
对于向量$x = \left[ \begin{matrix} 1 & -1 & 2 \end{matrix} \right]$的$L1$范数为$||x||_1 = |1|+|-1|+|2| = 4$

<!-- endtab -->

<!-- tab L2范数 -->

$L2$范数也称为向量的模。即向量的长度，定义为
$$
||x||_p = \sqrt {\sum _{i=1}^{n}{|x_i|^2}}
$$
长度为$1$的向量称为单位向量，向量$x = \left[ \begin{matrix} 1 & -1 & 2 \end{matrix} \right]$的$L1$范数为$||x||_2 = \sqrt {1^2+(-1)^2+2^2} = \sqrt {6}$

$L1$范数和$L2$范数被用于构造机器学习的正则化项

向量范数默认指$L2$范数

<!-- endtab -->

<!-- tab 无穷范数 -->

当$p=\infty$时，称为$L - \infty$范数，其定义为
$$
||x||_{\infty} = max|x_i|
$$
即向量分量绝对值的最大值，向量$x = \left[ \begin{matrix} 1 & -1 & 2 \end{matrix} \right]$的$L1$范数为$||x||_1 = 2$

$L - \infty$范数是$L-p$范数的极限
$$
||x||_{\infty} = \lim _{p \rightarrow + \infty}{(\sum _{i=1}^{n}{|x_i|^p})^{\frac {1}{p}}}
$$
<!-- endtab -->

{% endtabs %}



> 性质

向量数乘之后的范数为$||kx|| = |k| \cdot ||x||$，显然有$x^Tx = ||x||_2^2$

对于非$0$向量，通过数乘向量模的倒数，可以将向量单位化(标准化)，使其长度为$1$

对于上面的$L2$范数，归一化之后为$\left[ \begin{matrix} \frac {1}{\sqrt 6} & \frac {-1}{\sqrt 6} & \frac {2}{\sqrt 6} \end{matrix} \right]$

> 向量内积和$L2$范数满足著名的`柯西-施瓦茨(Cauchy-Schwarz)`不等式

$$
x^Ty \leq ||x|| \cdot ||y||
$$

可以通过构造一元二次方程证明

由于$(x+ty)^T (x+ty) = y^Tyt^2 + 2x^Tyt + x^Tx \geq 0$

对于$t$的一元二次方程$y^Tyt^2+2x^Tyt+x^Tx = 0$，只有$x+ty=0$时才有实数解，根据二次方程的判别法则有
$$
\Delta = (2x^Ty - 4y^Tyx^Tx \leq 0)
$$
即$(x^Ty)^2 \leq ||x||^2 ||y||^2$，当且仅当$x+ty=0$即两个向量成比例时不等式取等号



> 向量内积、向量模与向量夹角之间的关系

可以表示为$x^Ty = ||x|| \cdot ||y|| \cdot cos \theta$

其中$\theta$为两个向量之间的夹角，其取值范围为$[0, \pi]$，变形后得到向量夹角计算公式
$$
cos \theta = \frac {x^Ty}{||x|| \cdot ||y||}
$$
当向量之间的夹角超过$\frac {\pi}{2}$时，它们的内积为负

对于两个长度确定的向量，当夹角为$0$时它们的内积最大，此时$cos \theta=1$；夹角为$\pi$时它们的内积最小，此时$cos \theta=-1$

这一结论常在`梯度下降法`和`最速下降法`的推导中被使用

对于向量$x = \left[ \begin{matrix} 1 & 1 & 0 \end{matrix} \right]$、$y=\left[ \begin{matrix} 0 & 1 & 1 \end{matrix} \right]$，它们夹角的余弦为
$$
cos \theta = \frac {x^Ty}{||x|| \cdot ||y||} = \frac {1 \times 0 + 1 \times 1 + 0 \times 1}{\sqrt {1^2+1^2+0^2} \times \sqrt {0^2+1^2+1^2} } = \frac {1}{2}
$$
因此它们的夹角为$\frac {\pi}{3}$

对于向量$x = \left[ \begin{matrix} 1 & 0 & 0 \end{matrix} \right]$、$y=\left[ \begin{matrix} 0 & 1 & 0 \end{matrix} \right]$，它们夹角的余弦为
$$
cos \theta = \frac {x^Ty}{||x|| \cdot ||y||} = \frac {1 \times 0 + 0 \times 1 + 0 \times 0}{\sqrt {1^2+0^2+0^2} \times \sqrt {0^2+1^2+0^2} } = 0
$$
因此它们的夹角为$\frac {\pi}{2}$，两个向量正交，正好是$x$轴和$y$轴

> 范数满足三角不等式，是平面几何中三角不等式的抽象

$$
||x+y|| \leq ||x|| + ||y||
$$

将三角不等式两边同时平方，有
$$
||x+y||^2 = (x+y)^T(x+y) = x^Tx + 2x^Ty + y^Ty
$$
以及
$$
(||x||+||y||)^2 = ||x||^2 + 2||x||||u|| + ||y||^2 = x^Tx + 2 ||x|| ||y|| + y^Ty
$$

> 欧氏距离

两个向量相减之后的$L2$范数是它们对应的点之间的距离，称为`欧氏距离`，即$||x-y||$

对于三维空间中的两个点$x_1 = \left[ \begin{matrix} 1 & 2 & 1 \end{matrix} \right]$与$x_2 = \left[ \begin{matrix} 1 & 2 & 3 \end{matrix} \right]$，它们之间的距离为
$$
d = ||x_1 - x_2|| = \sqrt {(1-1)^2+(2-2)^2+(1-3)^2} = 2
$$
除了欧氏距离还可以定义其他的距离

{% note primary modern %}一个将两个向量映射为实数的函数$d(x_1,x_2)$只要满足下面的性质，均可以作为距离函数{% endnote %}

* **非负性**: 距离必须是非负的，对于$ \forall x_1,x_2 \in \mathbb {R}^n$，均有$d(x_1,x_2) \geq 0$
* **对称性**: 距离是对称的，对于$ \forall x_1,x_2 \in \mathbb {R}^n$，均有$d(x_1,x_2) = d(x_2, x_1)$
* **三角不等式**: 对于$ \forall x_1,x_2,x_3 \in \mathbb {R}^n$，均有$d(x_1,x_2) + d(x_2,x_3) \geq d(x_1, x_2)$

这些性质是欧式几何中距离特性的抽象

## 解析几何

> 定义

介绍下线性代数在解析几何中的应用，结论可以从二维平面和三维空间

平面解析几何中直线方程为$ax+by+x=0$，空间解析几何中平面方程为$ax+by+cz+d = 0$

将其推广到$n$维空间，得到`超平面(Hyperplane)`方程$\omega ^Tx+b = 0$

> 法向量

超平面中的$\omega$称为法向量，它与超平面内任意两个不同点之间连成的直线垂直

![平面的法向量](https://pic.hycbook.com/i//hexo/bk_resources/math/机器学习_线性代数与矩阵论/平面的法向量.webp)

图中黑色虚线为平面的法向量，它与平面垂直，对于平面内任意两点$x_1$和$x_2$，它们的连线(平面上虚线)均与法向量垂直

事实上，如果这两个点在平面内，则它们满足平面方程，有$\omega ^Tx_1 + b = 0$和$\omega ^Tx_2 + b = 0$

两式相减可以得到$\omega ^T(x_1-x_2) = 0$，因此法向量$\omega$与平面内任意两点之间的连线$x_1x_2$正交

将线性方程式的两侧同时乘以一个非$0$的系数，表示的还是同一个超平面

> 点到超平面的距离

{% tabs 点到超平面的距离 %}
<!-- tab 平面解析 -->

在平面解析几何中，点$(x,y)$到直接的距离为
$$
d = \frac {|ax+by+c|}{\sqrt {a^2+b^2}}
$$
<!-- endtab -->
<!-- tab 空间解析 -->

在空间解析几何中，点到平面的距离为
$$
d = \frac {|ax+by+cz+d|}{\sqrt {a^2+b^2+c^2}}
$$
<!-- endtab -->
<!-- tab 超平面 -->

将其推广到$n$维空间，根据向量内积和范数可以计算出点到超平面的距离，对于上面定义的超平面，点$x$到它的距离为
$$
d = \frac {|\omega ^Tx+b|}{||\omega||^2}
$$
这与二维平面、三维空间中点到直线和平面的距离公式在形式上是统一的，在支持向量机的推导过程中会用到

<!-- endtab -->

{% endtabs %}

计算点$\left[ \begin{matrix} 1 & 1 & 1 & 1 \end{matrix} \right]$到超平面$x_1-2x_2+x_3-3x_4+1=0$的距离
$$
d = \frac {|1-2 \times 1 + 1 - 3 \times 1 +1|}{\sqrt {1^2+(-2)^2+1^2+(-3)^2}} = \frac {2}{\sqrt {15}}
$$


## 线性相关性

> 线性相关

根据数乘和加法运算定义`线性组合`的概念，有向量组$x_1, \cdots ,x_l$，如果存在一组实数$k_1, \cdots ,k_l$使得
$$
x = k_1x_1 + \cdots k_lx_l 
$$
则称向量$x$可由向量组$x_1, \cdots ,x_l$线性表达

右侧称为向量组$x_1, \cdots ,x_l$的`线性组合`，$k_1, \cdots ,k_l$为`组合系数`

对于向量组

$x_1 = \left[ \begin{matrix} 1 & 2 & 3 \end{matrix} \right] \qquad x_2 = \left[ \begin{matrix} 1 & 0 & 2 \end{matrix} \right] \qquad x_3 = \left[ \begin{matrix} 0 & 0 & 1 \end{matrix} \right] $

向量$x= x_1+2_x2+x_3 = \left[ \begin{matrix} 3 & 2 & 8 \end{matrix} \right] $，可由该向量组线性表达，组合系数为$\left[ \begin{matrix} 1 & 2 & 1 \end{matrix} \right]$

对于向量组$x_1, \cdots ,x_l$，如果存在一组不全为$0$的数$kx_1, \cdots ,k_l$，使得
$$
k_1x_1+k_2x_2+ \cdots +k_lx_l = 0
$$
则称这组向量`线性相关`，如果不存在一组不全为$0$的数使得上式成立，则称为这组向量`线性无关`，也称为`线性独立`

> 线性无关

线性相关意味着这组向量存在冗余，至少有一个向量可以由其他向量线性表达，如果$x_1 \neq 0$，则有
$$
x_i = -\frac {\alpha _1}{\alpha _i}{x_1} - \cdots \frac {\alpha _{i-1}}{\alpha _i}{x_{i-1}} - \frac {\alpha _{i+1}}{\alpha _i}{x_{i+1}} - \frac {\alpha _{l}}{\alpha _i}{x_{l}}
$$
比如下面的行向量线性无关

$x_1 = \left[ \begin{matrix} 1 & 0 & 0 \end{matrix} \right] \qquad x_2 = \left[ \begin{matrix} 0 & 1 & 0 \end{matrix} \right] \qquad x_3 = \left[ \begin{matrix} 0 & 0 & 1 \end{matrix} \right] $

给定组合系数$k_1$、$k_2$、$k_3$，有

$k_1 \left[ \begin{matrix} 1 & 0 & 0 \end{matrix} \right] + k_2 \left[ \begin{matrix} 0 & 1 & 0 \end{matrix} \right] + k_3 \left[ \begin{matrix} 0 & 0 & 1 \end{matrix} \right] = \left[ \begin{matrix} k_1 & k_2 & k_3 \end{matrix} \right]$

欲使该向量为$0$，则有$k_1=k_2=k_3=0$，因此这组向量线性无关

下面的行向量线性相关，因为存在系数$\left[ \begin{matrix} 1 & 1 & -1 \end{matrix} \right]$是的向量为$0$

$x_1 = \left[ \begin{matrix} 1 & 1 & 0 \end{matrix} \right] \qquad x_2 = \left[ \begin{matrix} 2 & 2 & 0 \end{matrix} \right] \qquad x_3 = \left[ \begin{matrix} 3 & 3 & 0 \end{matrix} \right] $

> 极大线性无关组

一个向量组数量最大的线性无关向量子集称为`极大线性无关组`

给定向量组$$x_1, \cdots, x_l$$，如果$$x_{i_1},x_{i_2}, \cdots, x_{i_m}$$线性无关，但任意加入一个向量$$x_{i_{m+1}}$$之后线性相关

则$$x_{i_1},x_{i_2}, \cdots, x_{i_m}$$是极大线性无关组，`极大线性无关组不唯一`

$n$维向量的极大线性无关组最多有$n$个向量，这意味着任意一个向量均可以由$n$个线性无关的$n$维向量线性表达

## 向量空间

> 定义

有$n$维向量的集合$X$，如果在其上定义了加法和数乘运算，且对两种计算封闭，即运算结果仍属于此集合，则称$X$为`向量空间(Vector Sapce)`，也称为线性空间，对于任意的向量$x,y \in X$都有$x+y \in X \qquad kx \in X$，则集合$X$为向量空间

根据线性组合的定义，向量空间中任意向量的线性组合仍属于此空间

设$S$是向量空间$X$的子集，如果$S$对加法和数乘运算都封闭，则称$S$为$X$的`子空间`

例如，由三维实向量构成的集合$\mathbb {R}^3$是一个线性空间，显然对于任意$x,y \in \mathbb {R}^3$以及$k \in \mathbb {R}^3$，都有
$$
x+y \in \mathbb {R}^3 \qquad kx \in \mathbb {R}^3
$$
集合$S = \{ x \in \mathbb {R}^3, x_i > 0 \}$，即分量全为正的三维向量的集合不是线性空间，因为它对数乘不封闭

$S$中的向量$x$数乘一个负数，结果向量的分量为负，不再属于该集合

> 基(维数)

向量空间的极大线性无关组称为空间的`基`，基所包含的向量数称为空间的维数

如果$u_1, \cdots ,u_n$是空间的一组基，空间中的任意向量$x$均可由这组基线性表达$x = k_1u_1 + cdots + k_nu_n$

则$k_1, \cdots k_n$称为向量$x$在这组基下的坐标

> 正交基

如果基向量$u_1, \cdots ,u_n$相互正交
$$
u_i^Tu_j = 0, i \neq j
$$
则称为`正交基`，如果基向量相互正交且长度均为$1$
$$
u_i^Tu_j = 0, i \neq j \qquad  u_i^Tu_i = 0
$$
则称为`标准正交基`

向量组$\left[ \begin{matrix} 1 & 0 & 0 \end{matrix} \right] \qquad \left[ \begin{matrix} 0 & 1 & 0 \end{matrix} \right] \qquad \left[ \begin{matrix} 0 & 0 & 1 \end{matrix} \right]$为$\mathbb {R}^3$的一组标准正交基，其方向对应三维空间的$3$个坐标轴方向

需要强调的是，空间的基和标准正交基不唯一

> 格拉姆-施密特(Gram-Schmidt)正交化

给定一组线性无关的向量，可以根据它们构造出标准正交基，用的是`格拉姆-施密特(Gram-Schmidt)正交化`

具体方法: 给定一组非$0$且线性无关的向量$x_1, \cdots ,x_l$，格拉姆-施密特正交化先构造出一组正交基$u_1, \cdots ,u_l$

然后将这组正交基进行标准化得到标准正交基$e_1, \cdots ,e_l$

首先选择向量$x_1$作为一个正交基方向，令$u_1=x_1$

然后加入$$x_2$$，构造$$u_1$$和$$x_2$$的线性组合，使得它与$u_1$正交，即$$u_2 = x_2 - \alpha_{21}u_1$$

由于$$u_2$$与$$u_1$$正交，因此有$$(x_2- \alpha _{21}u_1)^T u_1 = 0$$

解得$\alpha _{21} = \frac {x_2^Tu_1}{u_1^Tu_1}$

解释下这种做法的几何意义，由于$x_2^Tu_1 = ||x_2||||u_1|| cos \theta$

![通过向量投影构造垂直向量](https://pic.hycbook.com/i//hexo/bk_resources/math/机器学习_线性代数与矩阵论/通过向量投影构造垂直向量.webp)

因此$\frac {x_2^Tu_1}{||u_1||} = ||x_2|| cos \theta$就是$x_2$在$u_1$方向上投影向量的长度，是图中直角三角形$ABC$的直角边$AB$的长度，这里$x_2$是三角形的斜边$AC$

由于$\frac {u_1}{||u_1||}$是$u_1$方向的单位向量，$\frac {x_2^Tu_1}{||u_1||} \frac {u_1}{||u_1||} = \frac {x_2^Tu_1}{u_1^Tu_1} u_1$就是$x_2$在$u_1$方向上的投影向量，是图中的向量$AB$

根据向量减法的三角形法则，$x_2- \frac {x_2^Tu_1}{u_1^Tu_1} u_1$就是图中的向量$BC$，与$u_1$垂直

加下来加入$x_3$，构造出$u_3$，是$u_1$、$u_2$和$x_3$的线性组合，使得它与$u_1$及$u_2$均正交
$$
u_3 = x_3 - \alpha _{31} u_1 - \alpha _{31} u_2
$$
由于$u_3$与$u_1$正交，因此有
$$
(x_3- \alpha _{31} u_1 - \alpha _{32} u_2)^T_1 = 0
$$
而$$u_1$$与$$u_2$$正交，$$(\alpha _{32} u_2)^T u_1 = 0$$，因此可以解得$$\alpha _{31} = \frac {x_3^T u_1}{u_1^T u_1}$$

由于$u_3$与$u_2$正交，因此有
$$
(x_3 - \alpha _{31}u_1 - \alpha _{32}u_2 )^Tu_2 = 0
$$
而$$u_1$$与$$u_2$$正交，$$(\alpha _{31} u_1)^T u_2 = 0$$，因此可以解得$$\alpha _{32} = \frac {x_3^T u_2}{u_2^T u_2}$$

以此类推，在加入$x_k$时构造下面的线性组合
$$
u_k = x_k - \sum _{i=1}^{k-1}{\alpha _{ki}u_i}
$$
由于它与$$u_1, \cdots, u_{k-1}$$均正交，因此
$$
(x_k - \sum _{i=1}^{k-1}{\alpha _{ki} u_i})^Tu_j = 0, j=0, \cdots ,k-1
$$

而$u_j$与$u_i,i=1, \cdots, k-1 ,i \neq j$均正交，从而解得
$$
\alpha _{ki} = \frac {x_k^T u_i}{u_i^T u_i}
$$
反复执行上述步骤，可以得到一组正交基$u_1, \cdots , u_l$

将它们分别标准化，得到标准正交基$ \frac {u_1}{||u_1||} , \cdots , \frac {u_l}{||u_l||}$

> 格拉姆-施密特正交化的几何意义

首先考虑二维的情况

![二维平面的格拉姆-施密特正交化](https://pic.hycbook.com/i//hexo/bk_resources/math/机器学习_线性代数与矩阵论/二维平面的格拉姆-施密特正交化.webp)

图中向量$\frac {x_2^Tu_1}{u_1^Tu_1}u_1$与$u_1$同向，是向量$x_2$在$x_1$方向的投影，显然$x_2$减掉该投影之后的向量，即向量$u_2$，与$u_1$垂直

下面考虑三维的情况

首先构造出$u_2$，与二维平面的方法相同，保证$u_2$与$u_1$垂直，然后处理$x_3$，首先减掉其在$u_1$方向的投影，保证相减之后与$u_1$垂直，然后减掉在$u_2$方向的投影，保证与$u_2$垂直

![三维空间的格拉姆-施密特正交化](https://pic.hycbook.com/i//hexo/bk_resources/math/机器学习_线性代数与矩阵论/三维空间的格拉姆-施密特正交化.webp)

下面举例说明，有如下的向量组
$$
x_1 = \left[ \begin{matrix} 1 \\ 0 \\ 1 \end{matrix} \right] \qquad x_2 = \left[ \begin{matrix} 1 \\ 1 \\ 0 \end{matrix} \right] \qquad x_3 = \left[ \begin{matrix} 0 \\ 1 \\ 1 \end{matrix} \right]
$$
首先生成$u_1=x_2= \left[ \begin{matrix} 1 \\ 0 \\ 1 \end{matrix} \right]$

然后生成$u_2$，组合系数为
$$
\alpha _{21} = \frac {x_2^T u_1}{x_1^T u_1} = \frac {1 \times 1 + 0 \times 1 + 1 \times 0}{1^2+0^2+1^2} = \frac {1}{2}
$$
因此
$$
u_2 = x_2 - \alpha _{21}u_1 = \left[ \begin{matrix} 1 \\ 1 \\ 0 \end{matrix} \right] - \frac {1}{2} \left[ \begin{matrix} 1 \\ 0 \\ 1 \end{matrix} \right] = \frac {1}{2}\left[ \begin{matrix} 1 \\ 2 \\ -1 \end{matrix} \right]
$$
最后生成$u_3$，组合系数为
$$
\alpha _{31} = \frac {x_3^Tu_1}{u_1^Tu_1} = \frac {1 \times 0 + 0 \times 1 + 1 \times 1}{1^2+0^2+1^2} = \frac {1}{2}
$$
以及
$$
\alpha _{32} = \frac {x_3^Tu_2}{u_2^Tu_2} = \frac {\frac {1}{2} ( 1 \times 0 + 2 \times 1 + (-1) \times 1)}{ \frac {1}{4} (1^2+2^2+(-1)^2)} = \frac {1}{3}
$$
因此
$$
u_3= x_3- \alpha_{31} u_1 - \alpha _{32}u_2 = \left[ \begin{matrix} 0 \\ 1 \\ 1 \end{matrix} \right] - \frac {1}{2} \left[ \begin{matrix} 1 \\ 0 \\ 1 \end{matrix} \right] - \frac {1}{3} \times \frac {1}{2} \left[ \begin{matrix} 1 \\ 2 \\ -1 \end{matrix} \right] = \frac {2}{3} \left[ \begin{matrix} -1 \\ 1 \\ 1 \end{matrix} \right]
$$
最后对$u_1$、$u_2$和$u_3$进行单位化
$$
e_1 = \frac {u_1}{||u_1||}=\frac {1}{\sqrt {2}} \left[ \begin{matrix} 1 \\ 0 \\ 1 \end{matrix} \right] \qquad 
e_1 = \frac {u_2}{||u_2||}=\frac {1}{\sqrt {6}} \left[ \begin{matrix} 1 \\ 2 \\ -1 \end{matrix} \right] \qquad 
e_1 = \frac {u_3}{||u_3||}=\frac {1}{\sqrt {3}} \left[ \begin{matrix} -1 \\ 1 \\ 1 \end{matrix} \right]
$$
即为一组标准正交基


## 应用之线性回归

## 应用之线性分类器与支持向量机



# 矩阵及其运算

## 基本概念

> 定义

`矩阵`$A$是二维数组，一个$m \times n$的矩阵有$m$行和$n$列，每个位置$(i,j)$处的元素$a_{i,j}$是一个数，记为
$$
\left[ 
\begin{matrix} 
a_{11} & \cdots & a_{an} \\ 
\vdots & \ddots & \vdots \\ 
a_{m1} & \cdots & a_{mn} 
\end{matrix} 
\right]
$$
矩阵通常用大写的黑体、斜体字母表示

矩阵的元素可以是实数，称为`实矩阵`，元素为复数，称为`复矩阵`，全体$m \times n$实矩阵的集合记为$\mathbb R ^{m \times n}$

> 方阵

如果矩阵行数和列数相等，则称为`方阵`，$n \times n$的方阵称为$n$阶方阵

> 对称矩阵

如果一个方阵的元素满足$$a_{ij} = a_{ji}$$，则称为`对称矩阵`，比如
$$
\left[ 
\begin{matrix} 
1 & 2 & 3 \\ 
2 & 2 & 0 \\ 
3 & 0 & 4
\end{matrix} 
\right]
$$

> 对角矩阵

矩阵所有行号和列号相等的元素$a_{ii}$的全体称为`主对角线`，如果一个矩阵出主对角线之外所有的元素均为$0$，则称为`对角矩阵`
$$
\left[ 
\begin{matrix} 
1 & 0 & 0 \\ 
0 & 2 & 0 \\ 
0 & 0 & 4
\end{matrix} 
\right]
$$
该对角矩阵可以简记为$diag(1,2,3)$，通常将对角矩阵记为$A$

> 单位矩阵

如果矩阵的主对角线的元素为$1$，其他元素为$0$，则称为`单位矩阵`，记为$I$
$$
\left[ 
\begin{matrix} 
1 & 0 & 0 \\ 
0 & 1 & 0 \\ 
0 & 0 & 1
\end{matrix} 
\right]
$$
单位矩阵的作用类似于实数中的$1$，在矩阵乘法中会说明，$n$阶单位矩阵记为$I_n$

> 零矩阵

如果矩阵的所有元素都为$0$，则称为`零矩阵`，记为**$0$**，其作用类似于实数中的$0$

> 上三角矩阵

如果矩阵的主对角线下方的元素全为$0$，则称为`上三角矩阵`
$$
\left[ 
\begin{matrix} 
1 & 1 & 0 \\ 
0 & 2 & 1 \\ 
0 & 0 & 3
\end{matrix} 
\right]
$$

> 下三角矩阵

如果矩阵的主对角线上方的元素全为$0$，则称为`下三角矩阵`
$$
\left[ 
\begin{matrix} 
1 & 1 & 0 \\ 
4 & 2 & 0 \\ 
6 & 5 & 3
\end{matrix} 
\right]
$$

> 格拉姆(Gram)矩阵

一个向量组$$x_1, \cdots , x_n$$的`格拉姆(Gram)矩阵`是一个$n \times  n$的矩阵，其每一个矩阵元素$$G_{ij}$$为向量$$x_i$$与$$x_j$$的内积，即
$$
G = \left[ 
\begin{matrix} 
x_1^Tx_1 & x_1^Tx_2 & \cdots & x_1^Tx_n \\ 
x_2^Tx_1 & x_2^Tx_2 & \cdots & x_2^Tx_n \\ 
\vdots & \vdots & \ddots & \vdots \\ 
x_n^Tx_1 & x_n^Tx_3 & \cdots & x_n^Tx_n
\end{matrix} 
\right]
$$
由于$x_i^Tx_j = x_j^Tx_i$，因此格拉姆矩阵是一个对称矩阵

对于向量$$x_1 = \left[ \begin{matrix} 1 & 2 & 3 \end{matrix} \right] \qquad x_2 = \left[ \begin{matrix} 1 & 0 & 1 \end{matrix} \right]$$，其格拉姆矩阵为

$$
 G = \left[ \begin{matrix} x_1^Tx_1 & x_1^Tx_2 \\ x_2^Tx_1 & x_2^Tx_2 \end{matrix} \right] = \left[ \begin{matrix} 14 & 4 \\ 4 & 2 \end{matrix} \right]
$$
在机器学习中该矩阵常被使用，比如主成分分析、核主成分分析、线性判别分析、线性回归、logisitic回归以及支持向量机的推导和证明

## 基本运算

> 转置

矩阵的`转置(Transpose)`定义为行和列下标相互交换，一个$m \times n$的矩阵转置之后为$n \times m$的矩阵，矩阵$A$的转置记为$A^T$
$$
\left[ \begin{matrix} 1 & 2 & 3 \\ 4 & 5 & 6 \end{matrix} \right]^T = \left[ \begin{matrix} 1 & 4 \\ 2 & 5 \\ 3 & 6 \end{matrix} \right]
$$

> 加法

矩阵的加法为对应位置的元素相加，需要保证两个矩阵有相同的尺寸，矩阵$A$和$B$相加记为$A+B$
$$
\left[ \begin{matrix} 1 & 2 & 3 \\ 4 & 5 & 6 \end{matrix} \right] + 
\left[ \begin{matrix} 7 & 8 & 9 \\ 10 & 11 & 12 \end{matrix} \right] = 
\left[ \begin{matrix} 8 & 10 & 12 \\ 14 & 16 & 18 \end{matrix} \right]
$$
加法和转置满足$(A+B)^T = A^T+B^T$

加法满足交换律和结合律$A+B=B+A \qquad A+B+C = A+(B+C)$

> 数乘

矩阵和标量的乘法即`数乘`，定义为标量和矩阵每个元素相乘，矩阵$A$和$k$数乘记为$kA$
$$
5 \times \left[ \begin{matrix} 1 & 2 & 3 \\ 4 & 5 & 6 \end{matrix} \right] = \left[ \begin{matrix} 5 & 10 & 15 \\ 20 & 25 & 30 \end{matrix} \right]
$$
数乘和加法满足分配律$k(A+B) = kA + kB$

> 乘法

矩阵`乘法`定义为第一个矩阵的每个行向量和第二个矩阵的每个列向量做**内积**，形成结果矩阵的每个元素，矩阵相乘记为$AB$

要求第一个矩阵的列数要等于第二个矩阵的行数

结果矩阵第$i$行第$j$列位置处的元素为$A$的第$i$行与$B$的第$j$列的内积$$\sum _{k=1}^{p}{a_{ip}b_{pj}}$$
$$
\left[ \begin{matrix} 1 & 1 & 0 \\ 0 & 0 & 1 \end{matrix} \right] \times
\left[ \begin{matrix} 0 & 1 \\ 0 & 0 \\ 1 & 0 \end{matrix} \right] = 
\left[ \begin{matrix} 1 \times 0 + 1 \times 0 + 0 \times 1 & 1 \times 1 + 1 \times 0 + 0 \times 0 \\ 0 \times 0 + 0 \times 0 + 1 \times 1  & 0 \times 1 + 0 \times 0 + 1 \times 0 \end{matrix} \right] =
\left[ \begin{matrix} 0 & 1 \\ 1 & 0 \end{matrix} \right]
$$
结果矩阵的每个元素需要$p$次乘法运算、$p-1$次加法运算得到，结果矩阵有$m \times n$个元素

因此，矩阵乘法需要$m \times n \times p$次乘法和$m \times n \times (p-1)$次加法

使用矩阵乘法可以简化线性方程组的表述，对于如下的线性方程组
$$
\begin{cases} a_{11}x_1 + a_{12}x_2 +\cdots + a_{1n}x_n = b_1 \\
\vdots \\
a_{n11}x_1 + a_{n2}x_2 +\cdots + a_{nn}x_n = b_n
\end{cases} 
$$
定义系数矩阵为
$$
A = \left[ 
\begin{matrix} 
a_{11} & \cdots & x_{1n} \\ 
\vdots & \ddots & \vdots \\ 
x_{n1} & \cdots & x_{nn}
\end{matrix} 
\right]
$$
定义解向量和常数向量为
$$
x = \left[ 
\begin{matrix} 
x_{1} \\ 
\vdots \\ 
x_{n}
\end{matrix} 
\right]

\qquad

b = \left[ 
\begin{matrix} 
b_{1} \\ 
\vdots \\ 
b_{n}
\end{matrix} 
\right]
$$
既可以将方程组写成矩阵的形式$Ax=b$

这种表示可以与一元一次方程$ax=b$达成形式上的统一，系数矩阵和常数向量合并之后称为`增广矩阵`，比如以上的增广矩阵为
$$
A = \left[ 
\begin{matrix} 
a_{11} & \cdots & x_{1n} & b_1 \\ 
\vdots & \ddots & \vdots & \vdots \\ 
x_{n1} & \cdots & x_{nn} & b_n
\end{matrix} 
\right]
$$

> 阿达玛积

矩阵的阿达玛积定义为对应位置元素乘积形成的矩阵，记为$A \odot B$

> 矩阵分块表示

对于下面的矩阵
$$
A =
\left[ 
\begin{matrix} 
1 & 1 & 3 & 4 & 0 & 0 & 0 \\ 
5 & 6 & 7 & 8 & 0 & 0 & 0 \\ 
9 & 10 & 11 & 12 & 0 & 0 & 0 \\ 
0 & 0 & 0 & 0 & 1 & 0 & 0 \\ 
0 & 0 & 0 & 0 & 0 & 1 & 0 \\ 
0 & 0 & 0 & 0 & 0 & 0 & 1 \\ 
0 & 0 & 0 & 0 & 1 & 1 & 1
\end{matrix} 
\right]
$$
可以将其分块为
$$
A =
\left[ 
\begin{matrix} 
A_{11} & A_{12} \\ 
A_{21} & A_{22}
\end{matrix} 
\right]
$$
其中
$$
A_{11} =
\left[ 
\begin{matrix} 
1 & 1 & 3 & 4\\ 
5 & 6 & 7 & 8\\ 
9 & 10 & 11 & 12
\end{matrix} 
\right]

\qquad

A_{12} =
\left[ 
\begin{matrix} 
0 & 0 & 0 \\ 
0 & 0 & 0 \\ 
0 & 0 & 0 
\end{matrix} 
\right]
$$

$$
A_{11} =
\left[ 
\begin{matrix} 
0 & 0 & 0 & 0 \\ 
0 & 0 & 0 & 0 \\ 
0 & 0 & 0 & 0 \\ 
0 & 0 & 0 & 0
\end{matrix} 
\right]

\qquad

A_{12} =
\left[ 
\begin{matrix} 
1 & 0 & 0 \\ 
0 & 1 & 0 \\ 
0 & 0 & 1 \\ 
1 & 1 & 1
\end{matrix} 
\right]
$$

如果矩阵的`子矩阵`为$0$矩阵，或者单位矩阵等特殊类型的矩阵，这边表示会非常有效

如果矩阵$A,B$分块后各块的尺寸以及水平、垂直方向的块数量相容，那可以将块当做标量来计算乘积$AB$
$$
\left[ 
\begin{matrix} 
A_{11} & \cdots & A_{1s} \\ 
\vdots & \ddots & \vdots \\ 
A_{r1} & \cdots & A_{ns}
\end{matrix} 
\right]

\qquad

\left[ 
\begin{matrix} 
B_{11} & \cdots & B_{1t} \\ 
\vdots & \ddots & \vdots \\ 
B_{s1} & \cdots & B_{st}
\end{matrix} 
\right]
$$
如果各个位置处对应的两个字块尺寸相容，那么可以进行矩阵乘积运算
$$
AB = 
\left[ 
\begin{matrix} 
\sum_{i=1}^{s}{A_{1i}B_{i1}} & \cdots & \sum_{i=1}^{s}{A_{1i}B_{it}} \\ 
\vdots & \ddots & \vdots \\ 
\sum_{i=1}^{s}{A_{ri}B_{i1}} & \cdots & \sum_{i=1}^{s}{A_{ri}B_{it}}
\end{matrix} 
\right]
$$
🌰举个分块乘法的例子
$$
A =
\left[ 
\begin{matrix} 
1 & 0 & 0 & 0 & 0 \\ 
0 & 1 & 0 & 0 & 0 \\ 
-1 & 2 & 1 & 0 & 0 \\ 
1 & 1 & 0 & 1 & 0 \\ 
-2 & 0 & 0 & 0 & -1
\end{matrix} 
\right]

\qquad

B =
\left[ 
\begin{matrix} 
3 & 2 & 0 & 1 & 0 \\ 
1 & 3 & 0 & 0 & 1 \\ 
-1 & 0 & 0 & 0 & 0 \\ 
0 & -1 & 0 & 0 & 0 \\ 
0 & 0 & -1 & 0 & 0
\end{matrix} 
\right]
$$
将$A$分为4块
$$
A = 
\left[ 
\begin{matrix} 
I_{2} & 0_{2 \times 3} \\ 
A_{1} & I_{3}
\end{matrix} 
\right]

\qquad

A1 = 
\left[ 
\begin{matrix} 
-1 & 2 \\ 
1 & 1 \\ 
-2 & 0
\end{matrix} 
\right]
$$
将$B$分块为
$$
B = 
\left[ 
\begin{matrix} 
B_{1} & I_{2} \\ 
-I_{3} & 0_{3 \times 2}
\end{matrix} 
\right]

\qquad

B1 = 
\left[ 
\begin{matrix} 
3 & 2 & 0 \\ 
1 & 3 & 0
\end{matrix} 
\right]
$$
因此它们的乘积为
$$
AB =

\left[ 
\begin{matrix} 
I_{2} & 0_{2 \times 3} \\ 
A_{1} & I_{3}
\end{matrix} 
\right]

\left[ 
\begin{matrix} 
B_{1} & I_{2} \\ 
-I_{3} & 0_{3 \times 2}
\end{matrix} 
\right]
= 

\left[ 
\begin{matrix} 
B_{1} & I_{2} \\ 
A_{1}B_{1}-I_3 & A_{1}
\end{matrix} 
\right]
$$


其中
$$
A_{1}B_{1}-I_3 = 

\left[ 
\begin{matrix} 
-1 & 2 \\ 
1 & 1 \\ 
-2 & 0
\end{matrix} 
\right]

\left[ 
\begin{matrix} 
3 & 2 & 0 \\ 
1 & 3 & 0
\end{matrix} 
\right]

-

\left[ 
\begin{matrix} 
1 & 0 & 0 \\ 
0 & 1 & 0 \\
0 & 0 & 1
\end{matrix} 
\right]
$$
因此
$$
AB =

\left[ 
\begin{matrix} 
3 & 2 & 0 & 1 & 0 \\ 
1 & 3 & 0 & 0 & 1 \\ 
-2 & 4 & 0 & -1 & -2 \\ 
4 & 4 & 0 & 1 & 0 \\ 
-6 & -4 & -1 & -2 & 0
\end{matrix} 
\right]
$$
在多态正态分布中，将会对协方差矩阵进行分块

> 特性

1️⃣单位矩阵与任意矩阵的左乘和右乘都等于该矩阵本身，即
$$
IA = A = AI
$$
2️⃣矩阵$A$左乘对角矩阵$\Lambda = diag(k_1, \cdots, k_n)$相当于将$A$的第$i$行的所有元素都乘以$k_i$
$$
\left[ 
\begin{matrix} 
k_{1} & 0 & \cdots & 0 \\ 
0 & k_{2} & \cdots & 0 \\ 
\cdots & \cdots & \cdots & \cdots \\ 
0 & 0 & \cdots & k_{n}
\end{matrix} 
\right]

\left[ 
\begin{matrix} 
a_{11} & a_{12} & \cdots & a_{1n} \\ 
a_{21} & a_{22} & \cdots & a_{2n} \\ 
\cdots & \cdots & \cdots & \cdots \\ 
a_{n1} & a_{n2} & \cdots & a_{nn}
\end{matrix} 
\right]

=

\left[ 
\begin{matrix} 
k_1a_{11} & k_1a_{12} & \cdots & k_1a_{1n} \\ 
k_2a_{21} & k_2a_{22} & \cdots & k_2a_{2n} \\ 
\cdots & \cdots & \cdots & \cdots \\ 
k_na_{n1} & k_na_{n2} & \cdots & k_na_{nn}
\end{matrix} 
\right]
$$
3️⃣矩阵$A$右乘对角矩阵$\Lambda = diag(k_1, \cdots, k_n)$相当于将$A$的第$i$列的所有元素都乘以$k_i$
$$
\left[ 
\begin{matrix} 
a_{11} & a_{12} & \cdots & a_{1n} \\ 
a_{21} & a_{22} & \cdots & a_{2n} \\ 
\cdots & \cdots & \cdots & \cdots \\ 
a_{n1} & a_{n2} & \cdots & a_{nn}
\end{matrix} 
\right]

\left[ 
\begin{matrix} 
k_{1} & 0 & \cdots & 0 \\ 
0 & k_{2} & \cdots & 0 \\ 
\cdots & \cdots & \cdots & \cdots \\ 
0 & 0 & \cdots & k_{n}
\end{matrix} 
\right]

=

\left[ 
\begin{matrix} 
k_1a_{11} & k_2a_{12} & \cdots & k_na_{1n} \\ 
k_1a_{21} & k_2a_{22} & \cdots & k_na_{2n} \\ 
\cdots & \cdots & \cdots & \cdots \\ 
k_1a_{n1} & k_2a_{n2} & \cdots & k_na_{nn}
\end{matrix} 
\right]
$$
4️⃣向量组$$x_1, x_2, \cdots, x_n$$的格拉姆矩阵可以写成一个矩阵与其转置的乘积
$$
G = \left[ 
\begin{matrix} 
x_{1}^T \\ 
\vdots \\ 
x_{n}^T
\end{matrix} 
\right]

\left[ 
\begin{matrix} 
x_{1} & \cdots & x_{n}
\end{matrix} 
\right]

= X^TX
$$
其中$X=\left[ \begin{matrix} x_{1} & \cdots & x_{n} \end{matrix} \right]$是所有向量按列形成的矩阵

5️⃣矩阵的乘法满足结合律
$$
(AB)C = A(BC)
$$
这些由标量乘法的结合律可推得

6️⃣矩阵乘法和加法满足左分配律和右分配律
$$
A(B+C) = AB+AC \qquad (A+B)C=AC+BC
$$
注意矩阵的乘法不满足交换律，即一般情况下$AB \neq BA$

7️⃣矩阵乘法和转置满足`穿脱原则`
$$
(AB)^T = B^TA^T
$$

## 逆矩阵

> 定义

`逆矩阵`对应标量的倒数运算，对于$n$阶矩阵$A$，如果存在另一个$n$阶矩阵$B$，使得它们的乘积为单位矩阵
$$
AB=I \qquad BAI
$$
对于$AB=I$，$B$称为$A$的右逆矩阵，对于$BA=I$，$B$称为$A$的左逆矩阵

> 如果矩阵的左逆矩阵和右逆矩阵存在，则它们相等，统称为矩阵的逆，记为$A^{-1}$

假设$B_1$是$A$的左逆，$B_2$是$A$的右逆，则有
$$
B_1AB_2 = (B_1A)B_2=IB_2=B_2 \qquad B_1AB_2=B_1(AB_2)=B_1I=B_1
$$
因此$B_1=B_2$

> 非奇异矩阵和奇异矩阵

如果矩阵的逆矩阵存在，则称其`可逆(Invertable)`。可逆矩阵也称为`非奇异矩阵`，不可逆矩阵也称为`奇异矩阵`

> 如果矩阵可逆，则其逆矩阵唯一

假设$B$和$C$都是$A$的逆矩阵，则有$AB=BA=I$和$AC=CA-I$

从而有$CAB=(CA)B=IB=B$和$CAB=C(AB)=CI=C$，因此B=C

对于线性方程组，如果能得到系数矩阵的逆矩阵，方程两边同乘以该逆矩阵，可以得到方程的解
$$
A^{-1}Ax = A^{-1}b \Rightarrow x=A^{-1}b
$$
这与一元一次方程的求解形式上是统一的$ax=b \Rightarrow x=a^{-1b}$

> 如果对角矩阵$A$的主对角线非$0$，则其逆矩阵存在，且逆矩阵为对角矩阵，主对角线元素为矩阵$A$的主对角线元素的逆

$$
\left[ 
\begin{matrix} 
a_{11} & \cdots & 0 \\ 
\vdots & \ddots & \vdots \\ 
0 & \cdots & a_{nn}
\end{matrix} 
\right] ^{-1}

= 
\left[ 
\begin{matrix} 
a_{11}^{-1} & \cdots & 0 \\ 
\vdots & \ddots & \vdots \\ 
0 & \cdots & a_{nn}^{-1}
\end{matrix} 
\right]
$$

可以推出，上三角矩阵的逆矩阵仍然是上三角矩阵
$$
(AB)^{-1} = B^{-1}A^{-1} \qquad (A^{-1})^{-1} = A
$$

$$
(A^T)^{-1} = (A^{-1})^T  \qquad (\lambda A)^{-1} = \lambda ^{-1} A^{-1}
$$

第1个等式与矩阵乘法的转置类似
$$
(AB)(B^{-1}A^{-1}) = ABB^{-1}A^{-1} = A(BB^{-1})A^{-1} = AIA^{-1} = AA^{-1} = I
$$
因此第1个等式成立，这里利用了矩阵乘法的结合律

由于$AA^{-1}=I$根据逆矩阵的定义，第2个等式成立

由于$(A^{-1})^TA^T = (AA^{-1})^T = I^T = I$根据逆矩阵的定义，第3个等式成立

该等式可以证明对称矩阵的逆矩阵也是对称矩阵，用类似的方法可以证明第4个等式成立

> 矩阵的秩

矩阵的`秩`定义为矩阵线性无关的行向量或列向量的最大数量，记为$r(A)$
$$
\left[ 
\begin{matrix} 
1 & 2 & 0 & 0 \\ 
1 & 0 & 0 & 0 \\ 
0 & 0 & 0 & 0 \\ 
0 & 0 & 0 & 0
\end{matrix} 
\right]
$$
该矩阵秩为$2$，该矩阵的极大线性无关组为矩阵的前两个行向量或列向量

如果$n$阶方阵的秩为$n$，则称其`满秩`，矩阵可逆的充分必零条件是满秩

对于$m \times n$的矩阵$A$，其秩满足$r(A) \leq min(m,n)$，即矩阵的秩不超过其行数和列数的较小值

> 矩阵的秩相关结论

$$
r(A) = r(A^T) \qquad r(A+B) \leq r(A)+r(B) \qquad r(AB) \leq min(r(A),r(B))
$$

> 逆矩阵的计算

可以通过初等行变换计算逆矩阵，所谓矩阵的初等行变换是指以下3种变换

1. 用一个非零的数$k$乘矩阵的某一行
   $$
   \left[ 
   \begin{matrix} 
   1 & 2 & 3 \\ 
   4 & 5 & 6 \\ 
   7 & 8 & 9
   \end{matrix} 
   \right]
   
   \longrightarrow 
   $$
   

2. 把矩阵的某一行的$k$倍加到另一行，这里的$k$是任意实数

3. 互换矩阵的两行













## 矩阵的范数

## 应用——人工神经网络

## 线性变换



# 行列式

## 行列式的定义与性质

## 计算方法





# 线性方程组

## 高斯消元法

## 齐次方程组

## 非齐次方程组

# 特征值和特征向量

##  特征值与特征向量

## 相似变换

## 正交变换

## QR 算法

## 广义特征值

## 瑞利商

## 谱范数与特征值的关系

## 条件数

## 应用——谱归一化与谱正则化

# 二次型

## 基本概念

## 正定二次型与正定矩阵

## 标准型

# 矩阵分解

## 楚列斯基分解

## QR 分解

## 特征值分解

## 奇异值分解