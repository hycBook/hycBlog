---
title: 机器学习_线性代数与矩阵论
date: '2022/10/11 20:38:22'
top_img: 'https://pic.hycbook.com/i/hexo/post_imgs/蕾姆1.webp'
cover: 'https://pic.hycbook.com/i/hexo/post_cover/蕾姆1.webp'
categories:
  - math
tags:
  - python
  - 机器学习数学
  - 线性代数
  - 矩阵论
mathjax: true
description: 机器学习的数学基础入门知识
swiper_index: 7
abbrlink: 47032
---

---



# 向量及其运算

## 基本概念

{% note success modern %}线性代数是多元函数微积分的基础{% endnote %}

> 定义

`向量(Vector)`是具有大小和方向的量，是由多个数构成的一维数组，每个数称为向量的分量，向量分量的数量称为向量的`维数`

> 向量的表示

物理中的力、速度以及加速度是典型的向量，$n$维向量$x$有$n$个分量，可以写成行向量的形式$(x_1 \cdots x_n)$

通常将向量写成**小写黑体斜体**字符，如果写成列的形式则称为列向量，这些分量在列方向排列
$$
\begin{bmatrix}
x_1 \\
\vdots \\
x_n
\end{bmatrix}
$$
这些向量的分量是实数，则成为实向量，如果是复数，则成为复向量，$n$维实向量的集合记为$\mathbb{R}^n$

与向量相对的是`标量(Scalar)`，标量只有大小而无方向，物理中的时间、质量以及电流是典型的标量

在数学中通常把向量表示成列向量，而计算机中通常按行存储









## 基本运算

`转置运算`(Transpose)将列向量变成行向量，将列向量转行向量，向量$x$的转置记为$x^T$
$$
\left[  
  \begin{matrix}
   1 & 0 & 0
  \end{matrix}
\right]^T = 
\left[  
  \begin{matrix}
   1 \\
   0 \\
   1
  \end{matrix} 
\right]
$$

> 加法

两个向量的加法定义为对应分量相加，要求参与运算的两个向量`维数相等`

向量$x$和$y$相加记为$x+y$，比如$\left[ \begin{matrix} 1 & 0 & 0 \end{matrix} \right] + \left[ \begin{matrix} 4 & 0 & 1 \end{matrix} \right] = \left[ \begin{matrix} 5 & 2 & 4 \end{matrix} \right]$

这与力的加法的平行四边形法则一致，是其在高维空间的推广



向量满足`交换律`和`结合律`
$$
x+y=y+x \qquad x+y+z=x+(y+z)
$$

> 减法

两个向量的减法为它们对应分量相减，同样要求参与运算的两个向量`维数相等`

与向量加法的平行四边形法则相对应，向量减法符合三角形法则，$x-y$的结果是以$y$为起点，以$x$为终点的向量

> 乘积

向量$x$与标量$k$的乘积$kx$定义为标量与向量的每个分量相乘，比如$5 \times \left[ \begin{matrix} 1 & 0 & 0 \end{matrix} \right] = \left[ \begin{matrix} 5 \times 2 & 5 \times 3 & 5 \times 1 \end{matrix} \right] = \left[ \begin{matrix} 10 & 15 & 5 \end{matrix} \right]$

乘积运算可以改变向量的大小和方向

加法和数乘满足分配律
$$
k(x+y) = kx + ky
$$

> 内积

两个向量$x$和$y$`内积(Inner Product)`定义为它们对应分量乘积之和
$$
x^{T}y = \sum _{i=1}^{n}{x_iy_i}
$$
内积可以记为$x \cdot y$
$$
\left[  
  \begin{matrix}
   1 \\
   2 \\
   3
  \end{matrix}
\right]^T
\left[  
  \begin{matrix}
   1 \\
   0 \\
   1
  \end{matrix}
\right] = 1 \times 1 + 2 \times 0 + 3 \times 1 = 4
$$
两个$n$维向量的内积运算需要执行$n$次乘法运算和$n-1$次加法运算

内积运算满足下面的规律
$$
x^Ty = y^Tx \qquad (kx)^Ty=kx^Ty \\ \qquad
(x+y)^Tz=x^T+y^Tz \qquad z^T(x+y)=z^Tx+z^Ty
$$
利用内积可以简化线性函数(一次函数)的表述

对于机器学习中广泛使用的线性模型的预测函数$\omega _1x_1 + \cdots \omega _nx_n + b$

定义系数(权重)向量$\omega = (\omega _1 \cdots \omega _n)^T$，输入向量$x = (x_1 \cdots x_n)^T$，$b$为偏置项，预测函数写成向量内积的话为
$$
\omega ^Tx + b
$$
向量与自身内积的结果为其所有分量的平方和，即$x^Tx=\sum _{i=1}^{n}{x_i^2}$

{% note primary modern %}两个向量的内积为$0$，则称它们`正交`{% endnote %}

正交是**几何垂直**这一概念在高维空间的推广
$$
\left[  
  \begin{matrix}
   1 \\
   0 \\
   0
  \end{matrix}
\right]^T
\left[  
  \begin{matrix}
   0 \\
   1 \\
   0
  \end{matrix}
\right] = 0
$$

> 阿达玛积

两个向量的`阿达玛(Hadamard)`积定义为它们对应分量相乘，结果为相同维数的向量，记为$x \odot y$

对于两个向量
$$
x = (x_1 \cdots x_n)^T \qquad y = (y_1 \cdots y_n)^T
$$
它们的阿达玛积为
$$
x \odot y = ({x_1}{y_1} \cdots {x_n}{y_n})^T
$$
阿达玛积可以简化问题的表述，在反向传播算法、各种梯度下降法中被使用

## 向量的范数

## 解析几何

## 线性相关性

## 向量空间

## 应用之线性回归

## 应用之线性分类器与支持向量机



# 矩阵及其运算

## 基本概念

## 基本运算

## 逆矩阵

## 矩阵的范数

## 应用——人工神经网络

## 线性变换



# 行列式

## 行列式的定义与性质

## 计算方法





# 线性方程组

## 高斯消元法

## 齐次方程组

## 非齐次方程组

# 特征值和特征向量

##  特征值与特征向量

## 相似变换

## 正交变换

## QR 算法

## 广义特征值

## 瑞利商

## 谱范数与特征值的关系

## 条件数

## 应用——谱归一化与谱正则化

# 二次型

## 基本概念

## 正定二次型与正定矩阵

## 标准型

# 矩阵分解

## 楚列斯基分解

## QR 分解

## 特征值分解

## 奇异值分解