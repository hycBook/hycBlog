---
title: 机器学习_概率论
date: '2022/11/27 18:03:19'
top_img: 'https://pic.hycbook.com/i/hexo/post_imgs/蕾姆4.webp'
cover: 'https://pic.hycbook.com/i/hexo/post_cover/蕾姆4.webp'
categories:
  - math
tags:
  - 机器学习数学
  - 概率论
mathjax: true
description: 机器学习的数学基础入门知识
swiper_index: 8
abbrlink: 8271
---


---

> 写在前面，本系列主要是对下面这本书做的学习笔记

![](https://pic1.zhimg.com/80/v2-1a36e4e5fb49fe46e99ea3dfddaa9e54_720w.webp)


> [常用数学符号的 LaTeX 表示方法](http://www.mohu.org/info/symbols/symbols.htm)
>
> [Markdown 常用数学符号和公式](https://blog.csdn.net/qq_38253837/article/details/109923758)



# 随机事件与概率

概率论同样在机器学习和深度学习中有至关重要的作用。如果将机器学习算法的输入数据和输出数据看作随机变量，则可用概率论的方法对数据进行计算，以此对不确定性进行建模

使用概率模型，可以输出概率值而非确定性的值，这对某些应用是至关重要的

对于某些应用问题，需要对变量之间的概率依赖关系进行建模，也需要概率论的技术，`概率图`模型是典型代表

随机数生成算法，即采样算法，需要以概率论作为理论指导

某些随机算法，如蒙特卡洛算法、遗传算法，同样需要以概率论作为理论或实现依据。

![概率论的知识体系](https://pic.hycbook.com/i//hexo/bk_resources/math/机器学习_概率论/概率论的知识体系.webp)

随机事件和概率是概率论中基本的概念，也是理解随机变量的基础

## 随机事件概率

随机事件是可能发生也可能不发生的事件，这种事件每次出现的结果具有不确定性

例如，天气可能是晴天、雨天、阴天；考试分数可能为0与100之间的整数；掷骰子，1到6这几种点数都可能出现

> 以集合论作为工具，给出随机事件的定义

对于一个随机试验，其所有可能结果组成的集合称为`样本空间`记为$\Omega$

随机试验可能的结果称为`样本点`，记为$\omega$，它是样本空间$\Omega$中的元素

* 对于天气，样本空间为$$\Omega=\{\text { 晴天, 阴天, 雨天 }\}$$，每种天气均为一个样本点
* 对于考试成绩，样本空间为$$\Omega=\{0,1, \cdots, 100\}$$，每个分数均为一个样本点
* 对于掷骰子，样本空间为$$\Omega=\{1,2,3,4,5,6\}$$

> 离散事件和连续事件

样本空间可以是有限集，也可以是无限集

对于无限的样本空间，可以是可数集(离散的)，也可以是不可数集(连续的)

`有限样本空间`与`无限可数样本空间`中定义的随机事件称为`离散型`

* **无限可数样本空间**: 抛一枚硬币，如果令$n$为第一次出现正面时所试验的次数，则其取值为$[1,+\infty)$内的整数

  这种情况的样本空间是无限可数集，如果记事件$A_{n}$为直到扔到第$n$次才第一次出现正面朝上，则这样的事件有无穷多个

* **无限不可数样本空间**: 在区间$[0,1]$内随机扔一个点，这个点的取值为此区间内所有实数，是无限不可数集

  此时, 我们无法将样本空间中所有的样本点列出

样本空间$\Omega$ 元素构成的集合称为随机事件，通常用大写斜体字母表示，如记为$A$

显然$\Omega$也是随机本件，它一定会发生，称为必然事件；空集$\varnothing$则不可能发生，称为不可能事件

> 衡量随机事件的可能性

随机事件发生的可能性用概率进行度量，随机事件$A$发生的概率记为$p(A)$，表示此事件发生的可能性，其值满足
$$
0 \leqslant p(A) \leqslant 1
$$
概率值越大则事件越可能发生。一般情况下，假设样本空间中每个样本点发生的概率是相等的(称为`等概率假设`)

因此事件$A$发生的概率是该集合的基数与整个样本空间基数的比值:
$$
p(A)=\frac{|A|}{|\Omega|}
$$
根据定义，所有单个样本点构成的随机事件的概率之和为$$\sum_{i=1}^{n} p\left(A_{i}\right)=1$$

其中$$A_{1}, \cdots, A_{n}$$为样本空间中所有单个样本点构成的随机事件

对于有限样本空间中的随机事件，可直接根据集合的基数计算的概率值

* **抛硬币问题**: 记正面朝上为事件$A$，反面朝上为事件$B$，则有$p(A)=p(B)=\frac{1}{2}$

  表示正面朝上和反面朝上的概率是相等的

* **掷骰子问题**: 记事件$A$为出现的点数为 1 , 则有$p(A)=\frac{1}{6}$
  1至6点出现的概率相等，均为$\frac{1}{6}$ 

显然，不可能事件发生的概率为0；必然事件发生的概率为1

> 两个随机事件

对应集合的交运算与并运算，可以定义两个随机事件`同时发生`的概率，以及两个随机事件`至少有一个`发生的概率

🍁两个随机事件$A$和$B$`同时发生`即为它们的交集，记为$A \cap B$，其概率为
$$
p(A \cap B)=\frac{|A \cap B|}{|\Omega|}
$$
以掷骰子为例，记$A$为出现的点数为奇数，$B$为出现的点数不超过3，则有

$$
A \cap B=\{1,3\} \Rightarrow p(A \cap B)=\frac{2}{6}
$$
两个事件同时发生的概率也可以简记为$p(A, B)$ 
$$
|A \cap B| \leqslant|A| 和 |A \cap B| \leqslant|B| \Rightarrow p(A, B) \leqslant \min (p(A), p(B))
$$
可以将两个事件同时发生的概率推广到多个事件
$$
p\left(A_{1}, \cdots, A_{n}\right)=\frac{\left|A_{1} \cap \cdots \cap A_{n}\right|}{|\Omega|}
$$
如果两个随机事件满足$A \cap B=\varnothing$

则称为`互不相容事件`，即两个事件不可能同时发生，因此互不相容事件同时发生的概率为0

🍒两个随机事件$A$和$B$`至少有一个发生`即为它们的并集，记为$A \cup B$
$$
|A \cup B|=|A|+|B|-|A \cap B| \Rightarrow p(A \cup B)=p(A)+p(B)-p(A \cap B)
$$
称为加法公式，两个集合的并集元素数等于它们元素数之和减掉它们重复的部分

因为重复的部分被算了两次，所以有
$$
p(A \cap B) \geqslant 0 \Rightarrow  p(A \cup B) \leqslant p(A)+p(B)
$$
考虑拼骰子问题，定义事件$A$为点数大于或等于2，定义事件$B$为点数小于或等于4
$$
A=\{2,3,4,5,6\} 和 B=\{1,2,3,4\} \Rightarrow A \cup B=\{1,2,3,4,5,6\}
$$
因此有$p(A \cup B)=\frac{6}{6}=1$

如果用加法公式进行计算，则有
$$
p(A \cup B)=p(A)+p(B)-p(A \cap B)=\frac{5}{6}+\frac{4}{6}-\frac{3}{6}=1
$$
如果两个事件$A$和$B$是互不相容的，则加法公式变为
$$
p(A \cup B)=p(A)+p(B)
$$
这一结论可以推广到多个随机事件的情况

> 完备事件组

`完备事件组`是对样本空间的一个划分，显然，对于完备事件组，有
$$
p\left(A_{i}, A_{j}\right)=0, i \neq j \qquad \sum_{i=1}^{n} p\left(A_{i}\right)=1
$$
考虑集合的补运算，对应于对立事件，事件$A$的补集称为它的对立事件，记为$\bar{A}$，即$A$不发生，显然有
$$
p(A)+p(\bar{A})=1
$$
对于掷羖子问题，如果记$A$为出现的点数为偶数，则其对立事件为出现的点数为奇数，因为点数不是偶数就是奇数

考虑无限可数的样本空间: 做一个试验，每次成功的概率为$p$，假设各次试验之间无关，事件$A_{n}$定义为试验$n$次才取得第一次成功，即前面$n-1$次都失败，第$n$次成功，显然，整个样本空间为
$$
A_{1}, A_{2}, \cdots, A_{n}, \cdots
$$
可以得到概率值
$$
p\left(A_{n}\right)=(1-p)^{n-1} p
$$
其中$(1-p)^{n-1}$是前面$n-1$次都失败的概率，$p$是第$n$次时成功的概率，显然有
$$
\sum_{n=1}^{+\infty} p\left(A_{n}\right)=\sum_{n=1}^{+\infty}(1-p)^{n-1} p=p \frac{1}{1-(1-p)}=1
$$

满足所有基本事件的概率之和为1的要求，这里利用了下面的幂级数求和结果
$$
\sum_{n=0}^{+\infty} p^{n}=\frac{1}{1-p}, 0<p<1
$$

> 几何型概率

前面介绍的概率均针对有限的或无限可数的样本空间，下面介绍无限不可数样本空间概率的计算，称为`几何型概率`

几何型概率定义在无限不可数集上，根据积分值(也称为测度值，如长度、面积、体积)定义

事件$A$发生的概率为区域$A$的测度值与$\Omega$测度值的比值，即
$$
p(A)=\frac{s(A)}{s(\Omega)}
$$
其中$s(A)$为集合$A$的测度

这里同样假设落在区域内任意点处的可能性相等，同时保证整个样本空间的概率为1

> 一维几何

在$[0,1]$区间内随机扔一个点，计算该点落在$[0,0.7]$内的概率

假设点的坐标为$x$，落在区间$[0,0.7]$内，即$0 \leqslant x \leqslant 0.7$，由于落在区间中任意点处的可能性相等，因此概率值为
$$
\frac{\text { 区间 }[0,0.7] \text { 的长度 }}{\text { 区间 }[0,1] \text { 的长度 }}=\frac{0.7}{1}=0.7
$$
是短线段与长线段的长度之比

> 二维几何

推广到二维的情况，可用面积计算概率

在单位正方形$0 \leqslant x, y \leqslant 1$内部随机地扔一个点，计算该点落在区域$x \leqslant 0.2, y \leqslant 0.3$内的概率

由于落在任意点处的可能性相等，因此是两个矩形区域的面积之比
$$
p(x \leqslant 0.2, y \leqslant 0.3)=\frac{0.2 \times 0.3}{1 \times 1}=0.06
$$
考虑一个更复杂的例子，在圆周上随机选择两个点，计算这两个点与圆心的连线之间沿着逆时针方向的夹角是锐角的概率

假设点落在圆周上任意位置处的可能性相等

![圆周上两点与圆心的连线之间的夹角](https://pic.hycbook.com/i//hexo/bk_resources/math/机器学习_概率论/圆周上两点与圆心的连线之间的夹角.webp)

如图所示，假设圆周上的两个点 $A 、 B$ 与圆心的连线和$x$轴正半轴的夹角(按照逆时针方向计算)分别为$$\theta_{1}$$与$$\theta_{2}$$

显然有$$0 \leqslant \theta_{1}, \theta_{2} \leqslant 2 \pi$$，这里采用弧度作为计量单位

两个点与圆心的连线之间的夹角为$$\theta=\left|\theta_{1}-\theta_{2}\right|$$，夹角为锐角，即$$\left|\theta_{1}-\theta_{2}\right| \leqslant \frac{\pi}{2}$$，这等价于
$$
-\frac{\pi}{2} \leqslant \theta_{1}-\theta_{2} \leqslant \frac{\pi}{2}
$$
夹角为锐角的区域为直线$$\theta_{1}-\theta_{2}=-\frac{\pi}{2}$$之下、直线$$\theta_{1}-\theta_{2}=\frac{\pi}{2}$$之上的区域，因此是夹在这两条直线之间的区域，两点之间逆时针方向夹角为锐角的概率为
$$
\frac{\text { 带状区域的面积 }}{[0,2 \pi] \text { 正方形的面积 }}=\frac{[0,2 \pi] \text { 正方形的面积 }-\left[\frac{\pi}{2}, 2 \pi\right] \text { 正方形的面积 }}{[0,2 \pi] \text { 正方形的面积 }} = \frac{2 \pi \times 2 \pi-\frac{3}{2} \pi \times \frac{3}{2} \pi}{2 \pi \times 2 \pi}=\frac{7}{16}
$$

> 三维几何

对于三维的情况，可用体积值来计算概率值，对于更高维的情况，则借助多重积分的值进行计算

## 条件概率

条件概率主要描述的是多个随机件的概率关系

> 条件概率定义

对于随机事件$A$和$B$，在$A$发生的条件下$B$发生的概率称为(`条件概率`)，记为$p(B \mid A)$

如果事件$A$的概率大于0，则条件概率可按下式计算
$$
p(B \mid A)=\frac{p(A, B)}{p(A)}
$$
根据定义，条件概率是$A$和$B$同时发生的概率与$A$发生的概率的比值

> 条件概率计算的例子

下面用一个例子说明条件概率的计算

对于掷骰子问题，假设事件$A$为点数是奇数，事件$B$为点数小于或等于3，则二者的条件概率为
$$
p(A \mid B)=\frac{p(A, B)}{p(B)}=\frac{p(\{1,3,5\} \cap\{1,2,3\})}{p(\{1,2,3\})}=\frac{2 / 6}{3 / 6}=\frac{2}{3}
$$
类似地，有
$$
p(B \mid A)=\frac{p(A, B)}{p(A)}=\frac{p(\{1,3,5\} \cap\{1,2,3\})}{p(\{1,3,5\})}=\frac{2 / 6}{3 / 6}=\frac{2}{3}
$$
对条件概率公式进行变形，可以得到`乘法公式`
$$
p(A,B)=p(A)p(B|A)=p(B)p(A|B)
$$

> 两个以上的随机事件下的条件概率

将条件概率推广到两个以上的随机事件，对于两组随机事件$$A_{1}, \cdots, A_{m}$$与$$B_{1}, \cdots, B_{n}$$, 它们的条件概率定义为
$$
p\left(A_{1}, \cdots, A_{m} \mid B_{1}, \cdots, B_{n}\right)=\frac{p\left(A_{1}, \cdots, A_{m}, B_{1}, \cdots, B_{n}\right)}{p\left(B_{1}, \cdots, B_{n}\right)}
$$
将乘法公式推广到 3 个随机事件，可以得到
$$
p(A, B, C)=p(A, B) p(C \mid A, B)=p(A)(B \mid A) p(C \mid A, B)
$$
需要注意的是，这种分解的顺序不是唯一的，推广到$n$个随机事件，有
$$
p\left(A_{1}, \cdots, A_{n}\right)=p\left(A_{1}\right) p\left(A_{2} \mid A_{1}\right) p\left(A_{3} \mid A_{1}, A_{2}\right) \cdots p\left(A_{n} \mid A_{1}, \cdots, A_{n-1}\right)
$$

> 随机事件的独立性

如果$p(B \mid A)=p(B)$，或$p(A \mid B)=p(A)$，则称随机事件$A$和$B$独立

随机事件独立意味着一个事件是否发生并不影响另外一个事件

如果随机事件$A$和$B$独立，根据式 (5.4), 有
$$
p(A,B) = p(A) p(B)
$$
将上面的定义进行推广，如果$n$个随机事件$A_{i}, i=1, \cdots, n$ 相互独立，则对所有可能的组合$1 \leqslant i<j<k<\cdots \leqslant n$，都有
$$
\begin{aligned}
p\left(A_{i}, A_{j}\right) &=p\left(A_{i}\right) p\left(A_{j}\right) \\
p\left(A_{i}, A_{j}, A_{k}\right) &=p\left(A_{i}\right) p\left(A_{j}\right) p\left(A_{k}\right) \\
& \vdots \\
p\left(A_{1}, \cdots, A_{n}\right) &=\prod_{i=1}^{n} p\left(A_{i}\right)
\end{aligned}
$$

## 全概率公式

> 定义

如果随机事件$$A_{1}, \cdots, A_{n}$$是一个完备事件组，且$$p\left(A_{i}\right)>0, i=1, \cdots, n$$，$$B$$是任意随机事件，则有
$$
p(B)=\sum_{i=1}^{n} p\left(A_{i}\right) p\left(B \mid A_{i}\right)
$$
称为全概率公式，借助于条件概率，全概率公式将对复杂事件的概率计算问题转化为在不同情况下发生的简单事件的概率的求和问题

> 例子

| 箱子  | 红球 | 白球 |
| ----- | ---- | ---- |
| 箱子1 | 6    | 4    |
| 箱子2 | 5    | 5    |
| 箱子3 | 2    | 8    |

先随机抽取一个箱子，然后从中随机抽取一个球，计算抽中红球的概率

令$A_{i}$为抽中第$i$个箱子，$B$为抽中红球

根据全概率公式，有
$$
p(B)=p\left(A_{1}\right) p\left(B \mid A_{1}\right)+p\left(A_{2}\right) p\left(B \mid A_{2}\right)+p\left(A_{3}\right) p\left(B \mid A_{3}\right)=\frac{1}{3} \times \frac{6}{10}+\frac{1}{3} \times \frac{5}{10}+\frac{1}{3} \times \frac{2}{10}=\frac{13}{30}
$$

## 贝叶斯公式

贝叶斯公式由数学家贝叶斯(Bayes)提出，它阐明了随机事件之间的因果概率关系，根据条件概率的定义，有
$$
p(A,B)=p(A)p(B|A)=p(B)p(A|B)
$$
变形可得`贝叶斯公式`
$$
p(A, B)=p(A) p(B \mid A)=p(B) p(A \mid B)
$$

> 先后验和似然函数

它描述了先验概率和后验概率之间的关系，如果事件$A$是因，事件件$B$是果

* **先验概率**: 称$p(A)$为`先验概率`(Prior Probability)，意为事先已经知道其值
* **后验概率**: $p(A \mid B)$ 称为`后验概率`(Posterior Probability)，意为事后才知道其值
* **似然函数**: 条件概率$p(B \mid A)$则称为`似然函数`

先验概率是根据以往经验和分析得到的概率，在随机事件发生之前已经知道，是**原因**发生的概率

后验概率是根据**结果**信息所计算出的导致该结果的原因所出现的概率

后验概率用于在事情已经发生的条件下，分析使得这件事情发生的原因概率

根据贝叶斯公式可以实现这种因果推理，这在机器学习中是常用的

> 全概率公式与贝叶斯公式

如果事件$$A_{1}, \cdots, A_{n}$$构成一个完备事件组，且$$p\left(A_{i}\right)>0, p(B)>0$$，根据全概率公式与贝叶斯公式，可以得到
$$
p\left(A_{m} \mid B\right)=\frac{p\left(A_{m}\right) p\left(B \mid A_{m}\right)}{p(B)}=\frac{p\left(A_{m}\right) p\left(B \mid A_{m}\right)}{\sum_{i=1}^{n} p\left(A_{i}\right) p\left(B \mid A_{i}\right)}
$$

> 例子

| 箱子  | 红球 | 黑球 |
| ----- | ---- | ---- |
| 箱子1 | 5    | 5    |
| 箱子2 | 7    | 3    |
| 箱子3 | 9    | 1    |

首先随机地抽取一个箱子，然后从中随机抽取一个球，如果抽中的是红球，计算这个球来自每个箱子的概率

令$A_{i}, i=1,2,3$表示抽中第$i$个箱子，$B$表示抽中红球

则这个红球来自第1个箱子的概率为
$$
p\left(A_{1} \mid B\right)=\frac{\frac{1}{3} \times \frac{5}{10}}{\frac{1}{3} \times \frac{5}{10}+\frac{1}{3} \times \frac{7}{10}+\frac{1}{3} \times \frac{9}{10}}=\frac{5}{21}
$$
来自第2个箱子的概率为
$$
p\left(A_{2} \mid B\right)=\frac{\frac{1}{3} \times \frac{7}{10}}{\frac{1}{3} \times \frac{5}{10}+\frac{1}{3} \times \frac{7}{10}+\frac{1}{3} \times \frac{9}{10}}=\frac{7}{21}
$$
来自第3个箱子的概率为
$$
p\left(A_{3} \mid B\right)=\frac{\frac{1}{3} \times \frac{9}{10}}{\frac{1}{3} \times \frac{5}{10}+\frac{1}{3} \times \frac{7}{10}+\frac{1}{3} \times \frac{9}{10}}=\frac{9}{21}
$$

## 条件独立

> 定义

将随机事件的独立性与条件概率相结合，可以得到条件独立的概念

如果随机事件$A, B, C$满足$p(A \mid B, C)=p(A \mid C)$，则称$A$和$B$关于事件$C$条件独立

直观含义是在$C$发生的情况下，$B$是否发生并不影响$A$，它们之间相互独立，这意味着事件$C$的发生使得$A$和$B$相互独立
$$
p(A, B \mid C)=p(A \mid B, C) p(B \mid C)=p(A \mid C) p(B \mid C)
$$
$A$和$B$关于$C$条件独立可以记为$A \perp B \mid C$，条件独立的概念在概率图模型中被广泛使用

# 随机变量

普通的变量只允许取值可变，随机变量(Random Variable)是取值可变并且取每个值都有一 个概率的变量

从另外一个角度来看，随机变量是用于表示随机试验结果的变量

随机变量通常用大写斜体字母表示，如$X$， 随机变量的取值一般用小写斜体字母表示，如$x_{i}$，随机变量可分为`离散型`和`连续型`两种

* **离散型**: 取值集合为有限集或者无限可数集，对应离散型随机事件
* **连续型**: 取值集合为无限不可数集, 对应几何型随机事件

## 离散型随机变量

离散型随机变量的取值集合是离散集合，为有限集或无限可数集，可以将所有取值列举出来

例如，掷骰子出现的点数即为离散型随机变量，取值集合为1和6之间的整数

> 概率质量函数(Probability Mass Function, PMF)

描述离散型随机变量取值概率的是`概率质量函数`，函数由随机变量取每个值的概率
$$
p\left(X=x_{i}\right)=p\left(x_{i}\right)
$$
排列组成，可以将$$p\left(X=x_{i}\right)$$简记为 $p\left(x_{i}\right)$ 

这里的**质量**与物理学中的质量相对应，可看作是一些有质量的质点，概率质量函数值对应这些点的质量

概率质量函数必须满足以下约束条件
$$
p\left(x_{i}\right) \geqslant 0 \qquad \sum_{i} p\left(x_{i}\right)=1
$$
下表是一个离散型随机变量的概率质量函数，其取值集合为$\{1,2,3,4\}$

| 随机变量取值 | 概率质量函数值 |
| ------------ | -------------- |
| 1            | 0.1            |
| 2            | 0.5            |
| 3            | 0.2            |
| 4            | 0.2            |

离散型随机变量的取值可能为无限可数集，此时要保证下面的级数收敛，并且其值为1
$$
\sum_{i=1}^{+\infty} p\left(x_{i}\right)=1
$$

> 累积分布函数(Cumulative Distribution Function, CDF)

`累积分布函数`也称为分布函数，是概率质量函数的累加，定义为
$$
p\left(X \leqslant x_{j}\right)=\sum_{i=1}^{j} p\left(x_{i}\right)
$$
上表所示的随机变量，其累积分布函数如表所示

| 随机变量取值 | 累积分布函数值 |
| ------------ | -------------- |
| 1            | 0.1            |
| 2            | 0.6            |
| 3            | 0.8            |
| 4            | 1.0            |

## 连续型随机变量

几何型随机事件对应的是连续型随机变量，连续型随机变量的取值集合为无限不可数集

一般为实数轴上的一个或多个区间，或者是整个实数集$\mathbb{R}$

例如，我们要观测每个时间点的温度，是一个连续值，为连续型随机变量

考虑计算一维几何型随机事件概率的例子，随机点落在$[0, x]$区间内的概率就是$x$，这是$X \leqslant x$这一随机事件的概率
$$
p(X \leqslant x)=x
$$
其中$X$是一个连续型随机变量，是随机点的一维坐标，其允许的取值范围为$[0,1]$

> 累积分布函数

对上面的函数进行扩充，使得$X$的取值范围为整个实数集$\mathbb{R}$，可以得到如下的函数
$$
p(X \leqslant x)=\left\{\begin{array}{ll}
0, & x<0 \\
x, & 0 \leqslant x \leqslant 1 \\
1, & x>1
\end{array} \right.
$$
该函数称为累积分布函数

> 概率密度函数

对累积分布函数进行求导，即可得到概率密度函数， 表示连续型随机变量在每一个取值点处的**概率密度值**

除去0和1这两点，上面的累积分布函数是可导的，其导数为
$$
p^{\prime}(x)=\left\{\begin{array}{ll}
0, & x<0 \\
1, & 0 \leqslant x \leqslant 1 \\
0, & x>1
\end{array}\right.
$$
此函数在区间$[0,1]$内所有点处的取值相等，这意味着点$x$落在$[0,1]$内所有点处的可能性相等

> 概率密度函数与累积分布函数的定义

`概率密度函数`(Probability Density Function, PDF)定义了连续型随机变量的概率分布

其函数值表示随机变量取该值的可能性(注意，不是概率)

概率密度函数必须满足如下约束条件
$$
f(x) \geqslant 0 \qquad \int_{-\infty}^{+\infty} f(x) \mathrm{d} x=1
$$
这可以看作是离散型随机变量的推广，积分值为1对应取各个值的概率之和为1

连续型随机变量落在某一点处的概率值为0

落在某一区间内的概率值为概率密度函数在该区间内的定积分
$$
p\left(x_{1} \leqslant X \leqslant x_{2}\right)=\int_{x_{1}}^{x_{2}} f(x) \mathrm{d} x=F\left(x_{2}\right)-F\left(x_{1}\right)
$$
其中$F(x)$是$f(x)$的一个原函数，也称为`分布函数`，近似地有
$$
p(x \leqslant X \leqslant x+\Delta x) \approx f(\xi) \Delta x
$$
其中$\xi$是$[x, x+\Delta x]$内的一个点

概率密度函数中的**密度**可类比物理学中的密度，概率质量函数在每一点处的值为随机变量取该值的概率

而概率密度函数在每一点处的值并不是概率值，只有区间上的积分值才是随机变量落入此区间的概率

随机变量$X$服从概率分布$f(x)$，一般可以简记为
$$
X \sim f(x)
$$
对于连续型随机变量，分布函数是概率密度函数的变上限积分，定义为
$$
F(y)=p(X \leqslant y)=\int_{-\infty}^{y} f(x) \mathrm{d} x
$$
显然这是增函数，分布函数的意义是随机变量$X \leqslant y$的概率

根据分布函数的定义有
$$
\lim \limits_{x \rightarrow-\infty} F(x)=0 \qquad \lim \limits_{x \rightarrow+\infty} F(x)=1
$$
根据定义，分布函数单调递增

考虑指数分布，其概率密度函数为
$$
f(x)=\left\{\begin{array}{ll}
\lambda \mathrm{e}^{-\lambda x}, & x \geqslant 0 \\
0, & \text { 其他 }
\end{array}\right.
$$
其中$\lambda>0$ ，下面计算它的分布函数，如果$x<0$，则有
$$
F(x)=\int_{-\infty}^{x} 0 \mathrm{~d} u=0
$$
如果$x \geqslant 0$，则有
$$
F(x)=\int_{-\infty}^{x} f(u) \mathrm{d} u=\int_{0}^{x} \lambda \mathrm{e}^{-\lambda u} \mathrm{~d} u=\int_{0}^{x} \mathrm{e}^{-\lambda u} \mathrm{~d} \lambda u=-\left.\mathrm{e}^{-\lambda u}\right|_{0} ^{x}=1-\mathrm{e}^{-\lambda x}
$$
因此其分布函数为
$$
F(x)=\left\{\begin{array}{ll}
1-\mathrm{e}^{-\lambda x}, & x \geqslant 0 \\
0, & \text { 其他 }
\end{array}\right.
$$
下面以logistic回归为例说明概率密度函数与分布函数在机器学习中的应用，logistic回归的分布函数为logistic函数，定义为
$$
F(x)=\frac{1}{1+\mathrm{e}^{-x}}
$$
其形状如图所示，该函数单调递增，且有
$$
\lim \limits_{x \rightarrow-\infty} F(x)=0 \qquad \lim \limits_{x \rightarrow+\infty} F(x)=1
$$
满足分布函数的定义要求，其概率密度函数为$F(x)(1-F(x))$，概率密度函数的图像如图所示

![logistic函数的概率密度函数](https://pic.hycbook.com/i//hexo/bk_resources/math/机器学习_概率论/logistic函数的概率密度函数.svg)

## 数学期望

> 定义

`数学期望`(Mathematical Expectation)是平均值的推广，是加权平均值的抽象，对于随机变量，是其在概率意义下的均值

普通的均值没有考虑权重或概率，对于$n$个变量$$x_{1}, \cdots, x_{n}$$，它们的算术平均值为
$$
\frac{1}{n} \sum_{i=1}^{n} x_{i}
$$
这可看作变量取每个值的可能性相等，或者每个取值的权重相等

但对于很多应用，变量取每个值有不同的概率，因此这种简单的均值无法刻画出变量的性质

表为买彩票时各种奖的中多金额以及对应的概率值，中奖金额可看作离散型随机变量



如果要计算买一张彩票的平均中奖金额，直接用各种奖的中奖金额计算平均值显然是不合理的

正确的做法是考虑中各种奖的概率，以其作为权重来计算均值
$$
0 \times 0.9+10 \times 0.09+1000 \times 0.009+10000 \times 0.00099+1000000 \times 0.00001=29.8
$$
这种计算方式就是求数学期望

> 离散型随机变量的数学期望

对于离散型随机变量数学期望定义为
$$
E[X]=\sum_{i} x_{i} p(x_i)
$$
数学期望也可以写成$$E_{X \sim p(x)}[X]$$或$$E_{p(x)}[X]$$，表示用概率分布$$p(x)$$对随机变量$X$计算数学期望

如果式$E[X]$的级数收敛，则称数学期望存在

> 连续型随机变量的数学期望

对于连续型随机变量，数学期望通过定积分定义

假设连续型随机变量$X$的概率密度函数是$f(x)$，它的数学期望为
$$
E[X]=\int_{-\infty}^{+\infty} x f(x) \mathrm{d} x
$$
根据定积分的定义，连续型是离散型数学期望的极限情况

对于连续型随机变量，其数学期望是一个泛函

根据定义，常数的数学期望为其自身，即$E[c]=c$

根据数学期望的定义, 下面的公式成立
$$
E[k X]=k E[X]
$$
其中$k$为常数

如果$g(X)$是随机变量$X$的函数，则由它定义的随机变量的数学期望为
$$
E[g(X)]=\sum_{i} g\left(x_{i}\right) p\left(x_{i}\right)
$$
一般简记为$E_{X \sim p(x)}[g(X)]$

对于连续型椭机变量$X$，其函数$g(X)$的数学期望为
$$
E[g(X)]=\int_{-\infty}^{+\infty} g(x) f(x) \mathrm{d} x
$$
根据这种定义, 下面的公式成立
$$
E[g(X)+h(X)]=E[g(X)]+E[h(X)]
$$
以下表中的随机变量为例，$X^{2}$的数学期望为

| 随机变量取值 | 概率质量函数值 |
| ------------ | -------------- |
| 1            | 0.1            |
| 2            | 0.5            |
| 3            | 0.2            |
| 4            | 0.2            |

$$
E\left[X^{2}\right]=1^{2} \times 0.1+2^{2} \times 0.5+3^{2} \times 0.2+4^{2} \times 0.2=7.1
$$

下面以指数分布为例计算连续型概率分布的数学期望，如果$X$服从参数为$\lambda$的指数分布，则其数学期望为
$$
E[X]=\int_{0}^{+\infty} x \lambda \mathrm{e}^{-\lambda x} \mathrm{~d} x=-\int_{0}^{+\infty} x \mathrm{de}^{-\lambda x}=-\left.x \mathrm{e}^{-\lambda x}\right|_{0} ^{+\infty}+\int_{0}^{+\infty} \mathrm{e}^{-\lambda x} \mathrm{~d} x=-\left.\frac{1}{\lambda} \mathrm{e}^{-\lambda x}\right|_{0} ^{+\infty}=\frac{1}{\lambda}
$$

> 物理意义

如果将随机变量看作物体各点在空间中的坐标，概率密度函数是其在空间各点处的密度，则数学期望的物理意义是物体的质心

## 方差与标准差

> 定义

`方差`(Variance, var)反映随机变量取值的波动程度，是随机变量与其数学期望差值平差的数学期望
$$
var[X]=E[(X-E[X])^2]
$$
方差也可记为$D[X]$，如果不使用平方，则随机变量所有值与其数学期望的差值之和为0

> 离散型随机变量的方差

对于离散随机变量，方差定义为
$$
\operatorname{var}[X]=\sum_{i}\left(x_{i}-E[X]\right)^{2} p\left(x_{i}\right)
$$
对于下表中的随机变量，它的方差为

| 随机变量取值 | 概率质量函数值 |
| ------------ | -------------- |
| 1            | 0.1            |
| 2            | 0.5            |
| 3            | 0.2            |
| 4            | 0.2            |

$$
\operatorname{var}[X]=(1-2.5)^{2} \times 0.1+(2-2.5)^{2} \times 0.5+(3-2.5)^{2} \times 0.2+(4-2.5)^{2} \times 0.2=0.85
$$

> 连续型随机变量的方差

对于连续型随机变量，方差同样通过积分定义
$$
\operatorname{var}[X]=\int_{-\infty}^{+\infty}(x-E[X])^{2} f(x) \mathrm{d} x
$$
其中，$f(x)$为概率密度函数，根据定义，方差是非负的

计算指数分布的方差，其数学期望为$1 / \lambda$，其方差为
$$
\begin{aligned}
\operatorname{var}[X] &=\int_{0}^{+\infty}\left(x-\frac{1}{\lambda}\right)^{2} \lambda \mathrm{e}^{-\lambda x} \mathrm{~d} x=-\int_{0}^{+\infty}\left(x-\frac{1}{\lambda}\right)^{2} \mathrm{de}^{-\lambda x} \\
&=-\left.\left(x-\frac{1}{\lambda}\right)^{2} \mathrm{e}^{-\lambda x}\right|_{0} ^{+\infty}+\int_{0}^{+\infty} 2\left(x-\frac{1}{\lambda}\right) \mathrm{e}^{-\lambda x} \mathrm{~d} x=\frac{1}{\lambda^{2}}-\int_{0}^{+\infty} \frac{2}{\lambda}\left(x-\frac{1}{\lambda}\right) \mathrm{de}^{-\lambda x} \\
&=\frac{1}{\lambda^{2}}-\left.\frac{2}{\lambda}\left(x-\frac{1}{\lambda}\right) \mathrm{e}^{-\lambda x}\right|_{0} ^{+\infty}+\int_{0}^{+\infty} \frac{2}{\lambda} \mathrm{e}^{-\lambda x} \mathrm{~d} x=-\frac{1}{\lambda^{2}}-\left.\frac{2}{\lambda^{2}} \mathrm{e}^{-\lambda x}\right|_{0} ^{+\infty}=\frac{1}{\lambda^{2}}
\end{aligned}
$$
方差反映了随机变量偏离均值的程度，方差越小，随机变量的变化幅度越小，反之则越大

> 标准差(Standard Deviation)

标准差定义为方差的平方根
$$
\sigma=\sqrt{\operatorname{var}[X]}
$$
根据方差的定义，下面公式成立
$$
\begin{aligned}
\operatorname{var}[X] &=E\left[(X-E[X])^{2}\right]=E\left[X^{2}-2 X E[X]+E^{2}[X]\right]=E\left[X^{2}\right]-E[2 X E[X]]+E\left[E^{2}[X]\right] \\
&=E\left[X^{2}\right]-2 E[X] E[X]+E^{2}[X]=E\left[X^{2}\right]-E^{2}[X]
\end{aligned}
$$
实际计算方差时经常采用此式，在机器学习中被广泛使用

根据方差的定义，有$\operatorname{var}[k X]=k^{2} \operatorname{var}[X]$，这是因为
$$
\operatorname{var}[k X]=E\left[(k X-E[k X])^{2}\right]=E\left[(k X-k E[X])^{2}\right]=k^{2} E\left[(X-E[X])^{2}\right]=k^{2} \operatorname{var}[X]
$$
这意味着将随机变量的取值扩大$k$倍，其方差扩大$k^{2}$倍

> 物理意义

如果将随机变量看作物体各点在空间中的坐标，概率密度函数是其在空间各点处的密度，则方差的物理意义是物体的转动惯量

## Jensen不等式

介绍数学期望的一个重要不等式，Jensen不等式(Jensen's Inequality)，它在机器学习某些算法的推导中起着至关重要的作用

回顾凸函数的定义，如果$f(x)$是一个凸函数，$0 \leqslant \theta \leqslant 1$，则有
$$
f\left(\theta x_{1}+(1-\theta) x_{2}\right) \leqslant \theta f\left(x_{1}\right)+(1-\theta) f\left(x_{2}\right)
$$
将上式从两个点推广到$m$个点，如果
$$
a_{i} \geqslant 0, i=1,2, \cdots, m \qquad a_{1}+\cdots+a_{m}=1
$$
可以得到，对于$$\forall x_{1}, \cdots, x_{m}$$有
$$
f\left(a_{1} x_{1}+\cdots+a_{m} x_{m}\right) \leqslant a_{1} f\left(x_{1}\right)+\cdots+a_{m} f\left(x_{m}\right)
$$
如果将$x$看作是一个随机变量，$$p\left(x=x_{i}\right)=a_{i}$$是其概率分布，则有
$$
E[x]=a_{1} x_{1}+\cdots+a_{m} x_{m} \quad E[f(x)]=a_{1} f\left(x_{1}\right)+\cdots+a_{m} f\left(x_{m}\right)
$$
从而得到Jensen不等式
$$
E[f(x)] \geqslant f(E[x])
$$
对于凹函数，上面的不等式反号

> 可以根据定义式用归纳法证明式多点情况下的公式成立

首先考虑$m=2$的情况，$$a_{1} \geqslant 0, a_{2} \geqslant 0$$且$$a_{1}+a_{2}=1$$，即$$a_{2}=1-a_{1}$$，根据凸函数的定义，对于$$\forall x_{1}, x_{2}$$有
$$
f\left(a_{1} x_{1}+a_{2} x_{2}\right)=f\left(a_{1} x_{1}+\left(1-a_{1}\right) x_{2}\right) \leqslant a_{1} f\left(x_{1}\right)+\left(1-a_{1}\right) f\left(x_{2}\right)=a_{1} f\left(x_{1}\right)+a_{2} f\left(x_{2}\right)
$$
假设$m=n$时不等式成立，则当$m=n+1$时有
$$
\begin{aligned}
f\left(\sum_{i=1}^{n+1} a_{i} x_{i}\right) &=f\left(a_{1} x_{1}+\left(1-a_{1}\right) \sum_{i=2}^{n+1} \frac{a_{i}}{1-a_{1}} x_{i}\right) \leqslant a_{1} f\left(x_{1}\right)+\left(1-a_{1}\right) f\left(\sum_{i=2}^{n+1} \frac{a_{i}}{1-a_{1}} x_{i}\right) \\
& \leqslant a_{1} f\left(x_{1}\right)+\left(1-a_{1}\right) \sum_{i=2}^{n+1} \frac{a_{i}}{1-a_{1}} f\left(x_{i}\right)=a_{1} f\left(x_{1}\right)+\sum_{i=2}^{n+1} a_{i} f\left(x_{i}\right)=\sum_{i=1}^{n+1} a_{i} f\left(x_{i}\right)
\end{aligned}
$$


上面第2步利用了凸函数的定义，第3步成立是因为
$$
\sum_{i=2}^{n+1} \frac{a_{i}}{1-a_{1}}=\frac{\sum_{i=2}^{n+1} a_{i}}{1-a_{1}}=\frac{1-a_{1}}{1-a_{1}}=1
$$
根据归纳法的假设，$m=n$时不等式成立

如果$f(x)$是严格凸函数且$x$不是常数，则有
$$
E[f(x)]>f(E[x])
$$
这同样可以用归纳法证明，与前面的证明过程类似

如果$f(x)$是严格凸函数，当且仅当随机变量$x$是常数时，不等式取等号
$$
E[f(x)]=f(E[x])
$$
下面给出证明，如果随机变量$x$是常数，则有
$$
x_{1}=x_{2}=\cdots=x_{m}=c
$$
因此
$$
f\left(a_{1} x_{1}+\cdots+a_{m} x_{m}\right)=f\left(a_{1} c+\cdots+a_{m} c\right)=f\left(\left(a_{1}+\cdots+a_{m}\right) c\right)=f(c)
$$
以及
$$
[a_{1} f\left(x_{1}\right)+\cdots+a_{m} f\left(x_{m}\right)=a_{1} f(c)+\cdots+a_{m} f(c)=\left(a_{1}+\cdots+a_{m}\right) f(c)=f(c)
$$
因此有
$$
f\left(a_{1} x_{1}+\cdots+a_{m} x_{m}\right)=a_{1} f\left(x_{1}\right)+\cdots+a_{m} f\left(x_{m}\right)
$$
接下来证明如果不等式取等号，则有$$x_{1}=x_{2}=\cdots=x_{m}$$

可用反证法证明，如果$$x_{i} \neq x_{j}, i \neq j$$，由于$$f(x)$$是严格凸函数，根据前面的结论有
$$
f\left(a_{1} x_{1}+\cdots+a_{m} x_{m}\right)<a_{1} f\left(x_{1}\right)+\cdots+a_{m} f\left(x_{m}\right)
$$
Jensen不等式可以推广到随机向量的情况，在后面将利用此不等式推导出求解含有隐变量的最大似然估计问题的EM算法


# 常用概率分布

离散型概率分布包括均匀分布、伯努利分布、 二项分布、多项分布、几何分布

连续型概率分布包括均匀分布、正态分布, 以及$t$分布

## 均匀分布

> 定义


对于离散型随机变量$X$，如果服从`均匀分布`(Uniform Distribution)，则其取每个值的概率相等，即
$$
p\left(X=x_{i}\right)=\frac{1}{n}, i=1, \cdots, n
$$
对于连续型随机变量$X$，如果服从区间$[a, b]$上的均匀分布，则其概率密度函数为分段常数函数，定义为
$$
f(x)=\left\{\begin{array}{ll}
\frac{1}{b-a}, & a \leqslant x \leqslant b \\
0, & x<a, x>b
\end{array}\right.
$$
在允许取值的区间内，概率密度函数值相等，等于区间长度的倒数

下面计算它的分布函数，如果$x<a$，则有
$$
F(x)=\int_{-\infty}^{x} f(x) \mathrm{d} x=\int_{-\infty}^{x} 0 \mathrm{~d} x=0
$$
如果$a \leqslant x \leqslant b$，则有
$$
F(x)=\int_{-\infty}^{x} f(x) \mathrm{d} x=\int_{-\infty}^{a} 0 \mathrm{~d} x+\int_{a}^{x} \frac{1}{b-a} \mathrm{~d} x=\frac{x-a}{b-a}
$$
如果$x>b$，则有
$$
F(x)=\int_{-\infty}^{x} f(x) \mathrm{d} x=\int_{-\infty}^{a} 0 \mathrm{~d} x+\int_{a}^{b} \frac{1}{b-a} \mathrm{~d} x+\int_{b}^{x} 0 \mathrm{~d} x=1
$$
因此，其分布函数为
$$
F(x)=\left\{\begin{array}{ll}
0, & x<a \\
\frac{x-a}{b-a}, & a \leqslant x \leqslant b \\
1, & x>b
\end{array}\right.
$$
随机变量$X$服从区间$[a, b]$上的均匀分布，简记为$X \sim U(a, b)$

> 例子

下面计算服从区间$[a, b]$上均匀分布的随机变量的数学期望和方差，根据数学期望的定义
$$
E[X]=\int_{a}^{b} \frac{1}{b-a} x \mathrm{~d} x=\left.\frac{1}{2(b-a)} x^{2}\right|_{a} ^{b}=\frac{a+b}{2}
$$
因此，均匀分布的均值为区间的中点，根据方差的定义
$$
\operatorname{var}[X]=\int_{a}^{b}\left(x-\frac{a+b}{2}\right)^{2} \frac{1}{b-a} \mathrm{~d} x=\left.\frac{1}{3(b-a)}\left(x-\frac{a+b}{2}\right)^{3}\right|_{a} ^{b}=\frac{(b-a)^{2}}{12}
$$
均匀分布的方差与区间长度的平方成正比

均匀分布是最简单的概率分布, 在程序设计中各种概率分布的随机数一般通过均匀分布随机 数构造。算法生成的随机数不是真随机而是伪随机。通常情况下, 基本的随机数生成算法生成的 是某一区间 $\left[0, n_{\max }\right]$ 上均匀分布的随机整数。

对均匀分布随机整数进行变换，可以将其变为另外一个区间上的均匀分布随机数

例如，对$$\left[0, n_{\max }\right]$$上均匀分布随机整数$x$除以其最大值$$n_{\max }$$
$$
\frac{x}{n_{\max }}
$$
可以将其变换为区间$[0,1]$上的连续型均匀随机数

如果要生成某一区间上均匀分布的整数, 则可借助于取余运算实现

如果要生成$[0, k]$上均匀分布的整数，则可以将$x$除以$k+1$取余数
$$
x \bmod (k+1)
$$
这里假设$k<n_{\max }$，显然，该余数在$[0, k]$上均匀分布

## 伯努利分布

> 定义

服从`伯努利分布`(Bernoulli Distribution)的随机变量$X$的取值为0或1两种情况，该分布也称为$0-1$分布

取值为1的概率为$p$，取值为0的概率为$1-p$，其中$p$为$(0,1)$内的实数，是此概率分布的参数，即
$$
p(X=1)=p \qquad p(X=0)=1-p
$$
概率质量函数可统一写成
$$
p(X=x)=p^{x}(1-p)^{1-x}
$$
其中$x \in\{0,1\}$，如果$x=0$，则有
$$
p(X=0)=p^{0}(1-p)^{1-0}=1-p
$$
如果$x=1$，则有

$$
p(X=1)=p^{1}(1-p)^{1-1}=p
$$
如果$p=\frac{1}{2}$，那么此时的伯努利分布为离散型均匀分布，为方便起见，通常将$1-p$简记为$q$

如果将随机变量取值为1看作试验成功，取值为0看作试验失败，则伯努利分布是描述试验结果的一种概率分布

随机变量$X$服从参数为$p$的伯努利分布简记为
$$
X \sim B(p)
$$
机器学习中的二分类问题可以用伯努利分布描述，logistic回归拟合的是这种分布

根据定义，伯努利分布的数学期望为
$$
E[X]=0 \times(1-p)+1 \times p=p
$$
方差为
$$
\operatorname{var}[X]=(0-p)^{2} \times(1-p)+(1-p)^{2} p=p(1-p)
$$
可以看到，当$p=\frac{1}{2}$时，方差有极大值

根据服从均匀分布的随机数可以生成服从伯努利分布的随机数，对于伯努利分布
$$
p(X=1)=p
$$
将$[0,1]$区间划分成两个子区间，第1个子区间的长度为$p$，第二个子区间的长度为$1-p$

如果有一个$[0,1]$上均匀分布的随机数，则它落在第1个子区间内的概率即为$p$，落在第2个子区间内的概率即为$1-p$

因此，可以先生成$[0,1]$上均匀分布的随机数$\zeta$，然后判定其所在的子区间，如果它落在第1个子区间，即
$$
\zeta<p
$$
则输出1，否则输出0

算法输出的值即服从伯努利分布，深度学习中的Dropout机制、稀疏自动编码器都使用了伯努利分布

## 二项分布

> 定义

$n$个独立同分布的伯努利分布随机变量之和服从$n$重伯努利分布，也称为`二项分布`(Binomial Distribution)

此时，随机变量$X$表示$n$次试验中有$k$次成功的概率，其概率质量函数为
$$
p(X=k)=\mathrm{C}_{n}^{k} p^{k}(1-p)^{n-k}, k=0,1, \cdots, n
$$
其中$\mathrm{C}_{n}^{k}$为组合数，是$n$个数中有$k$个取值是1的所有可能情况数，$p$是每次试验时成功的概率，取值范围为$(0,1)$

二项分布的概率$p(X=k)$也是对$(p+(1-p))^{n}$进行二项式展开时第$k+1$个展开项，因此而得名

在$n$次试验中，$k$次成功的概率为$p^{k}$，$n-k$次失败的概率为$(1-p)^{n-k}$，而$k$次成功可以是$n$次试验中的任意$k$次，因此有组合系数$\mathrm{C}_{n}^{k}$ 

随机变量$X$服从参数为$n, p$的二项分布，可以简记为
$$
X \sim B(n, p)
$$
下图是一个二项分布的图像，其中$n=5, p=0.5$，横轴为随机变量的取值，纵轴为取各离散值的概率

![二项分布的图像示例](https://pic.hycbook.com/i/hexo/bk_resources/math/机器学习_概率论/二项分布的图像示例.svg)

此时二项分布的展开项是对称的，因此其概率质量函数的图像是对称的

从图可以看出，二项分布在中间位置的概率值更大，在两端处的概率值更小，随着$n$的值增大，它将以正态分布为极限分布

> 计算二项分布的数学期望

下面计算二项分布的数学期望，根据定义有
$$
\begin{aligned}
E[X] & =\sum_{k=0}^{n} k \times C_{n}^{k} p^{k}(1-p)^{n-k}=\sum_{k=1}^{n} k \times C_{n}^{k} p^{k}(1-p)^{n-k}=\sum_{k=1}^{n} k \times \frac{n !}{k !(n-k) !} p^{k}(1-p)^{n-k} \\
& =\sum_{k=1}^{n} \frac{n !}{(k-1) !(n-k) !} p^{k}(1-p)^{n-k}=n p \sum_{k=1}^{n} \frac{(n-1) !}{(k-1) !(n-k) !} p^{k-1}(1-p)^{n-k} \\
& =n p \sum_{a=0}^{b} \frac{b !}{a !(b-a) !} p^{a}(1-p)^{b-a}=n p \sum_{a=0}^{b} \mathrm{C}_{b}^{a} p^{a}(1-p)^{b-a}=n p
\end{aligned}
$$
上式第6步进行了换元，令$a=k-1, b=n-1, n-k=b-a$，方差为
$$
\operatorname{var}[X]=E\left[X^{2}\right]-E^{2}[X]
$$
而

$$
\begin{aligned} E\left[X^{2}\right]= & \sum_{k=0}^{n} k^{2} \times \mathrm{C}_{n}^{k} p^{k}(1-p)^{n-k} \\ = & \mathrm{C}_{n}^{1} p(1-p)^{n-1}+\sum_{k=2}^{n} n \mathrm{C}_{n-1}^{k-1} p^{k}(1-p)^{n-k}+\sum_{k=2}^{n} n \mathrm{C}_{n-2}^{k-1} p^{k}(1-p)^{n-k} \\ = & n p(1-p)^{n-1}+n p \sum_{k=1}^{n} \mathrm{C}_{n-1}^{k-1} p^{k-1}(1-p)^{n-k}-n p \mathrm{C}_{n-1}^{0}(1-p)^{n-1} \\ & +n(n-1) p^{2} \sum_{k=2}^{n} n \mathrm{C}_{n-2}^{k-1} p^{k-2}(1-p)^{n-k} \\ = & n p(1-p)^{n-1}+n p(p+1-p)^{n-1}-n p(1-p)^{n-1}+n(n-1) p^{2}(p+1-p)^{n-2} \\ = & n p(1-p)^{n-1}+n p-n p(1-p)^{n-1}+n(n-1) p^{2}=n p(1-p)+n^{2} p^{2}\end{aligned}
$$
上式第2步利用了$$\mathrm{C}_{n}^{k}=n \mathrm{C}_{n-1}^{k-1}+n \mathrm{C}_{n-2}^{k-1}$$，因此
$$
\operatorname{var}[X]=E\left[X^{2}\right]-E^{2}[X]=n p(1-p)+n^{2} p^{2}-(n p)^{2}=n p(1-p)
$$
由于二项分布是多个相互独立同分布的伯努利分布之和，因此其均值与方差和伯努利分布刚好为$n$倍的关系

## 多项分布

> 定义

`多项分布`(Multinomial Distribution)是伯努利分布的推广，随机变量$X$的取值有$k$种情况

假设取值为$\{1, \cdots, k\}$内的整数，则有
$$
p(X=i)=p_{i}, i=1, \cdots, k
$$
对$X$的取值进行One-Hot向量编码，由$k$个分量组成，如果$X$取值为$i$，则第$i$个分量为1，其余分量均为0

假设One-Hot编码结果为$$\left[b_{1} b_{2} \cdots b_{k}\right]$$，则概率质量函数可以统一写成
$$
\begin{array}{c}
p(X=i)=p_{1}^{b_{1}} p_{2}^{b_{2}} \cdots p_{k}^{b_{k}} \quad \Rightarrow \quad
p(X=i)=p_{1}^{0} \cdots p_{i}^{1} \cdots p_{k}^{0}=p_{i}
\end{array}
$$
如果$p_{i}=\frac{1}{k}, i=1, \cdots, k$，则多项分布为离散型均匀分布，多项分布对应多分类问题，softmax回归拟合的就是这种分布

根据服从均匀分布的随机数可以生成服从多项分布的随机数，其方法与生成伯努利分布随机数相同，对于多项分布
$$
p(X=i)=p_{i}, i=1, \cdots, k
$$
将$[0,1]$区间划分成$k$个子区间，第$i$个子区间的长度为$p_{i}$

如果有一个$[0,1]$上均匀分布的随机数，则它落在第$i$个子区间内的概率为$p_{i}$

因此，可以先生成$[0,1]$上均匀分布的随机数$\zeta$，然后判定其所在的子区间，如果它落在第$i$子区间，即
$$
\sum_{j=1}^{i-1} p_{j} \leqslant \zeta<\sum_{j=1}^{i} p_{j}
$$
则输出$i$，算法输出的数即服从多项分布

> 对多个样本按照权重抽样

借助于多项分布随机数可以实现对多个样本按照权重抽样，有$n$个样本$$x_{1}, \cdots, x_{n}$$，它们对应的归一化权重为$$w_{1}, \cdots, w_{n}$$

现在要对这些数进行抽样，保证抽中每个样本$$x_{i}$$的概率为$w_{i}$，借助于$$1 \sim n$$内多项分布的随机数，即可实现此功能

在粒子滤波器、遗传算法中均有这样的需求

## 几何分布

> 定义

前面已经说明了无限可数样本空间的概率值，几何分布是这种取值为无限种可能的离散型概率分布

做一个试验，每次成功的概率为$p$，假设各次试验之间相互独立，事件$A_{n}$定义为试验$n$次才取得第一次成功，与其对应，定义随机变量$X$表示第一次取得成功所需要的试验次数，其概率为
$$
p(X=n)=(1-p)^{n-1} p, n=1,2, \cdots
$$
如果令$q=1-p$，则上式可以写成
$$
p(X=n)=q^{n-1} p
$$
这就是`几何分布`(Geometric Distribution)，几何分布因其分布函数为几何级数而得名

随机变量$X$服从参数为$p$的几何分布，可以简记为$X \sim \mathrm{Geo}(p)$

> 分布函数

计算几何分布的分布函数，根据定义，有
$$
F(n)=p(X \leqslant n)=\sum_{i=1}^{n} q^{i-1} p=p \sum_{i=0}^{n-1} q^{i}=p \frac{1-q^{n}}{1-q}=1-(1-p)^{n}
$$
几何分布的数学期望为
$$
E[X]=\sum_{n=1}^{+\infty} n q^{n-1} p=p \sum_{n=1}^{+\infty}\left(q^{n}\right)^{\prime}=p\left(q \sum_{n=0}^{+\infty} q^{n}\right)^{\prime}=p\left(\frac{q}{1-q}\right)^{\prime}=p \frac{1}{(1-q)^{2}}=\frac{1}{p}
$$
上式利用了幂级数求导公式，当$0<q<1$时，幂级数$\sum_{n=0}^{+\infty} q^{n}$收敛于$\frac{1}{1-q}$

根据方差的定义，有
$$
\operatorname{var}[X]=E\left[X^{2}\right]-E^{2}[X]=E[X(X-1)]+E[X]-E^{2}[X]
$$
而
$$
\begin{array}{l}
E[X(X-1)]=\sum_{n=1}^{+\infty} n(n-1)(1-p)^{n-1} p \\ \\
=\sum_{n=2}^{+\infty} n(n-1)(1-p)^{n-1} p=p(1-p) \sum_{n=2}^{+\infty} n(n-1) q^{n-2} \\ \\
=p(1-p) \sum_{n=2}^{+\infty}\left(q^{n}\right)^{n}=p(1-p) \sum_{n=0}^{+\infty}\left(q^{n}\right)^{\prime \prime}=p(1-p)\left(\frac{1}{1-q}\right)^{\prime \prime} \\ \\
=2 p(1-p) \frac{1}{(1-q)^{3}}=\frac{2(1-p)}{p^{2}}
\end{array}
$$
同样，这里利用了幂级数求导公式，因此几何分布的方差为
$$
\operatorname{var}[X]=\frac{2(1-p)}{p^{2}}+\frac{1}{p}-\frac{1}{p^{2}}=\frac{1-p}{p^{2}}
$$
几何分布的随机数也可以借助于均匀分布的随机数生成，方法与伯努利分布类似

算法循环进行尝试，每次生成一个伯努利分布随机数，如果遇到1，则结束循环，返回尝试的次数

## 正态分布

> 定义

`正态分布`(Normal Distribution)也称为`高斯分布`(Gaussian Distribution)，它的概率密度函数为
$$
f(x)=\frac{1}{\sqrt{2 \pi} \sigma} \mathrm{e}^{-\frac{(x-\mu)^{2}}{2 \sigma^{2}}}
$$
其中$\mu$和$\sigma^{2}$分别为均值和方差

该函数在$(-\infty,+\infty)$上的积分为1，令$t=\frac{x-\mu}{\sqrt{2} \sigma}$，则有
$$
\int_{-\infty}^{+\infty} \frac{1}{\sqrt{2 \pi} \sigma} \mathrm{e}^{-\frac{(x-\mu)^{2}}{2 \sigma^{2}}} \mathrm{~d} x=\int_{-\infty}^{+\infty} \frac{1}{\sqrt{2 \pi} \sigma} \mathrm{e}^{-t^{2}} \mathrm{~d}(\sqrt{2} \sigma t+\mu)=\frac{1}{\sqrt{\pi}} \int_{-\infty}^{+\infty} \mathrm{e}^{-t^{2}} \mathrm{~d} t=1
$$
显然，概率密度函数关于数学期望$x=\mu$对称，且在该点处有极大值

在远离数学期望时，概率密度函数的值单调递减

具体地，在$(-\infty, \mu)$内单调递增，在$(\mu,+\infty)$内单调递减，该函数的极限为
$$
\lim \limits_{x \rightarrow+\infty} f(x)=0 \qquad \lim \limits_{x \rightarrow-\infty} f(x)=0
$$
现实世界中的很多数据，例如人的身高、体重、寿命等，近似服从正态分布

随机变量$X$服从均值为$\mu$ 、方差为$\sigma^{2}$的正态分布，简记为$X \sim N\left(\mu, \sigma^{2}\right)$

> 标准正态分布

如果正态分布的均值为0，方差为1，则称为`标准正态分布`，此时的概率密度函数为
$$
f(x)=\frac{1}{\sqrt{2 \pi}} \mathrm{e}^{-\frac{x^{2}}{2}}
$$
该函数是一个偶函数，下图红色为标准正态分布的概率密度函数图像，其形状像钟，因此也称为`钟形分布`

![各种均值和方差的正态分布的概率密度函数](https://pic.hycbook.com/i/hexo/bk_resources/math/机器学习_概率论/各种均值和方差的正态分布的概率密度函数.svg)

正态分布的均值决定了其概率密度函数峰值出现的位置，方差则决定了曲线的宽和窄，方差越大，曲线越宽，反之则越窄

正态分布$N\left(\mu, \sigma^{2}\right)$的分布函数为
$$
F(x)=\int_{-\infty}^{x} \frac{1}{\sqrt{2 \pi} \sigma} \mathrm{e}^{-\frac{(u-\mu)^{2}}{2 \sigma^{2}}} \mathrm{~d} u
$$
由于$\mathrm{e}^{-x^{2}}$的不定积分不是初等函数，因此该函数无解析表达式

假设随机变量$Z$服从标准正态分布$N(0,1)$，则随机变量$X=\sigma Z+\mu$服从正态分布$N\left(\mu, \sigma^{2}\right)$

相反，如果随机变量$X$服从正态分布$N\left(\mu, \sigma^{2}\right)$，则随机变量$Z=\frac{X-\mu}{\sigma}$服从标准正态分布$N(0,1)$

> 置信区间

正态分布的$k-\sigma$置信区间定义为$[\mu-k \sigma, \mu+k \sigma]$，其中$k$为一个正整数

随机变量落入该区间的概率为
$$
p(\mu-k \sigma<X<\mu+k \sigma)=F(\mu+k \sigma)-F(\mu-k \sigma)
$$
随机变量在$\sigma, 2 \sigma, 3 \sigma$区间内的概率分别为
$$
\begin{array}{r}
p(\mu-\sigma<X<\mu+\sigma)=0.6827 \\
p(\mu-2 \sigma<X<\mu+2 \sigma)=0.9545 \\
p(\mu-3 \sigma<X<\mu+3 \sigma)=0.9973
\end{array}
$$

> 数学期望

下面计算正态分布的数学期望，使用换元法，令
$$
z=\frac{x-\mu}{\sigma}
$$
则有
$$
x=\mu+\sigma z
$$
根据数学期望的定义，有
$$
\begin{aligned}
E[X] & =\int_{-\infty}^{+\infty} x \frac{1}{\sqrt{2 \pi} \sigma} \mathrm{e}^{-\frac{(\sigma-\mu)^{2}}{2 \sigma^{2}}} \mathrm{~d} x=\int_{-\infty}^{+\infty}(\sigma z+\mu) \frac{1}{\sqrt{2 \pi} \sigma} \mathrm{e}^{-\frac{x^{2}}{2}} \mathrm{~d}(\sigma z+\mu) \\
& =\frac{1}{\sqrt{2 \pi}} \int_{-\infty}^{+\infty}(\sigma z+\mu) \mathrm{e}^{-\frac{\alpha^{2}}{2}} \mathrm{~d} z=\frac{\sigma}{\sqrt{2 \pi}} \int_{-\infty}^{+\infty} z \mathrm{e}^{-\frac{\alpha^{2}}{2}} \mathrm{~d} z+\frac{\mu}{\sqrt{2 \pi}} \int_{-\infty}^{+\infty} \mathrm{e}^{-\frac{\alpha^{2}}{2}} \mathrm{~d} z=0+\frac{\mu}{\sqrt{2 \pi}} \sqrt{2 \pi} \\
& =\mu
\end{aligned}
$$
上式第5步成立是因为$z \mathrm{e}^{-\frac{x^{2}}{2}}$是奇函数，它在$(-\infty,+\infty)$上的积分为0

第6步利用了下面的结论
$$
\int_{-\infty}^{+\infty} \mathrm{e}^{-\frac{\mu^{2}}{2}} \mathrm{~d} z=\sqrt{2 \pi}
$$
这可以用式换元法证明

> 方差

下面计算方差，同样令$z=\frac{x-\mu}{\sigma}$，则有
$$
\begin{aligned}
\operatorname{var}[X] & =\int_{-\infty}^{+\infty}(x-\mu)^{2} \frac{1}{\sqrt{2 \pi} \sigma} \mathrm{e}^{-\frac{(x-\mu)^{2}}{2 \sigma^{2}}} \mathrm{~d} x=\int_{-\infty}^{+\infty} \sigma^{2} z^{2} \frac{1}{\sqrt{2 \pi} \sigma} \mathrm{e}^{-\frac{\alpha^{2}}{2}} \mathrm{~d}(\sigma z+\mu) \\
& =\frac{\sigma^{2}}{\sqrt{2 \pi}} \int_{-\infty}^{+\infty} z^{2} \mathrm{e}^{-\frac{\alpha^{2}}{2}} \mathrm{~d} z=-\frac{\sigma^{2}}{\sqrt{2 \pi}} \int_{-\infty}^{+\infty} z \mathrm{de}^{-\frac{\mu^{2}}{2}}=-\frac{\sigma^{2}}{\sqrt{2 \pi}}\left(\left.z \mathrm{e}^{-\frac{\alpha^{2}}{2}}\right|_{-\infty} ^{+\infty}-\int_{-\infty}^{+\infty} \mathrm{e}^{-\frac{L^{2}}{2}} \mathrm{~d} z\right) \\
& =\frac{\sigma^{2}}{\sqrt{2 \pi}} \int_{-\infty}^{+\infty} \mathrm{e}^{-\frac{\mu^{2}}{2}} \mathrm{~d} z=\sigma^{2}
\end{aligned}
$$
上式第5步利用了分部积分法，第6步成立是因为
$$
\lim \limits_{x \rightarrow+\infty} z \mathrm{e}^{-\frac{x^{2}}{2}}=0 \qquad \lim \limits_{x \rightarrow-\infty} z \mathrm{e}^{-\frac{x^{2}}{2}}=0
$$
正态分布的概率密度函数由均值和方差决定，这是非常好的一个性质，通过控制这两个参数，即可接制均值和方差

`中心极限定理`指出正态分布是某些概率分布的极限分布

正态分布具有$(-\infty,+\infty)$的支撑区间，且在所有定义于此区间内的连续型概率分布中，正态分布的熵最大，这些优良的性质使得正态分布在机器学习中得到了大量的使用

多个正态分布的加权组合可形成高斯混合模型，是混合模型的一种，它可以逼近任意连续型

## t分布

> 定义

$t$分布其概率密度函数为
$$
f(x)=\frac{\Gamma\left(\frac{\nu+1}{2}\right)}{\sqrt{\nu \pi} \Gamma\left(\frac{\nu}{2}\right)}\left(1+\frac{x^{2}}{\nu}\right)^{-\frac{\nu+1}{2}}
$$
其中$\Gamma$为伽马函数，$\nu$为自由度，是一个正整数

显然，当$x=0$时，概率密度函数有极大值且函数是偶函数，伽马函数是阶乘的推广，将其从正整数推广到正实数，通过积分定义
$$
\Gamma(x)=\int_{0}^{+\infty} t^{x-1} \mathrm{e}^{-t} \mathrm{~d} t
$$
此函数的定义域为$(0,+\infty)$且在该定义域内连续

根据定义，有
$$
\Gamma(1)=\int_{0}^{+\infty} t^{0} \mathrm{e}^{-t} \mathrm{~d} t=-\left.\mathrm{e}^{-t}\right|_{0} ^{+\infty}=1
$$
伽马函数满足与阶乘相同的递推关系
$$
\Gamma(x+1)=x \Gamma(x)
$$
这可以通过分部积分验证
$$
\Gamma(x+1)=\int_{0}^{+\infty} t^{x} \mathrm{e}^{-t} \mathrm{~d} t=-\left.t^{x} \mathrm{e}^{-t}\right|_{0} ^{+\infty}+x \int_{0}^{+\infty} t^{x-1} \mathrm{e}^{-t} \mathrm{~d} t=x \Gamma(x)
$$
根据这两个结果，对于$n \in \mathbb{N}$，有
$$
\Gamma(n)=(n-1) !
$$
$t$分布概率密度函数的形状与正态分布类似，如下图所示

啧啧啧

其分布函数为
$$
F(x) = \frac{1}{2} + x \Gamma\left(\frac{\nu+1}{2}\right) \frac{_{2} F_{1}(\frac{1}{2}, \frac{\nu+1}{2} ; \frac{3}{2} ; -\frac{x^{2}}{\nu})}{\sqrt{\pi \nu \Gamma}\left(\frac{\nu}{2}\right)}
$$
其中$$_{2} F_{1}$$为`超几何函数`(Hypergeometric Function)，定义为

$$
{ }_{2} F_{1}(a, b ; c ; z)=\sum_{n=0}^{+\infty} \frac{(a)_{n}(b)_{n}}{(c)_{n}} \frac{z^{n}}{n !}
$$

上图显示了各种自由度取值时的$t$分布概率密度函数曲线以及标准正态分布的概率密度函数曲线

$t$分布具有长尾的特点，在远离中心点的位置依然有较大的概率密度函数值，且自由度越小，长尾性越强

随着自由度的增加，它以标准正态分布为极限分布

考虑$\nu \rightarrow+\infty$时的极限情况，有
$$
\lim \limits_{\nu \rightarrow+\infty}\left(1+\frac{x^{2}}{\nu}\right)^{-\frac{v+1}{2}}=\lim \limits_{\nu \rightarrow+\infty} e^{-\frac{\nu+1}{2} \ln \left(1+\frac{x^{2}}{\nu}\right)}=\lim \limits_{t \rightarrow 0} \mathrm{e}^{-\frac{1+1}{2 t} \ln \left(1+x^{2} t\right)}=\lim \limits_{t \rightarrow 0} \mathrm{e}^{-\frac{1+1}{2} \frac{\ln \left(1+x^{2} t\right)}{1}}=\mathrm{e}^{-\frac{1}{2} x^{2}}
$$

上面第2步进行了换元，令$t=\frac{1}{\nu}$，第4步利用了等价无穷小，同时
$$
\lim \limits_{\nu \rightarrow+\infty} \frac{\Gamma\left(\frac{\nu+1}{2}\right)}{\sqrt{\nu \pi} \Gamma\left(\frac{\nu}{2}\right)}=\frac{1}{\sqrt{2 \pi}}
$$
因此$\nu \rightarrow+\infty$时$t$分布的极限分布是标准正态分布

$t$分布具有长尾的特点，远离概率密度函数中心点的位置仍然有较大的概率密度函数值，因此更易于产生远离均值的样本，它在机器学习中的典型应用是$t$-SNE降维算法

# 分布变换

已知从某一概率分布的随机变量$X$，可以对它进行变换，得到服从其他概率分布的随机变量$Y$，即根据一个概率分布的样本得到另一个概率分存的样本

## 随机变量函数

> 定义

随机变量函数是以随机变量为自变量的函数，它将一个随机变量映射成另外一个随机变量，二者一般有不同的分布

假设随机变量$X$的概率密度函数为$$f_{X}(x)$$，分布函数为$$F_{X}(x)$$，对于$X$的函数
$$
Y=g(X)
$$
假设该函数严格单调，反函数存在且$g^{-1}(x)=h(x)$，现在计算$Y$所服从的概率分布

首先计算$Y$的分布函数，由于$X$的分布函数是已知的，因此需要借助于它的分布函数，如果$g(x)$单调增，根据分布函数的定义，$Y$的分布函数为
$$
F_{Y}(y)=p(Y \leqslant y)=p(g(X) \leqslant y)=p\left(X \leqslant g^{-1}(y)\right)=F_{X}(h(y))=\int_{-\infty}^{h(y)} f_{X}(x) \mathrm{d} x
$$
即有
$$
F_{Y}(y)=F_{X}(h(y))
$$
对该函数进行求导即可得到$Y$的概率密度函数，根据变上限积分与复合函数求导公式，有
$$
f_{Y}(y)=\left(\int_{-\infty}^{h(y)} f_{X}(x) \mathrm{d} x\right)^{\prime}=f_{X}(h(y)) h^{\prime}(y)
$$
如果$g(X)$单调递减，则有
$$
F_{Y}(y)=p(g(X) \leqslant y)=p\left(X \geqslant g^{-1}(y)\right)=1-F_{X}(h(y))=1-\int_{-\infty}^{h(y)} f_{X}(x) \mathrm{d} x
$$
概率密度函数为
$$
f_{Y}(y)=\left(1-\int_{-\infty}^{h(y)} f_{X}(x) \mathrm{d} x\right)^{\prime}=-f_{X}(h(y)) h^{\prime}(y)
$$
此时$h^{\prime}(x)<0$，综合这两种情况，有
$$
f_{Y}(y)=f_{X}(h(y))\left|h^{\prime}(y)\right|
$$
本质上是定积分的换元法

> 例子

假设随机变量$X$服从均匀分布$U(0,1)$，计算$Y=\exp (X)$的概率分布，$X$的概率密度函数为
$$
f_{X}(x)=1,0 \leqslant x \leqslant 1
$$
根据式$f_{Y}(y)$定义，当$1 \leqslant y \leqslant \mathrm{e}$时，有
$$
f_{Y}(y)=f_{X}(h(y))\left|h^{\prime}(y)\right|=1 \times(\ln (y))^{\prime}=1 / y
$$
根据式$f_{Y}(y)$可以得到逆变换采样算法，实现各种概率分布之间的转换，对服从简单分布的随机数进行变换，得到想要的概率分布的随机数

> 证明: 假设随机变量$X$服从标准正态分布$N(0,1)$，则随机变量$X=\sigma Z+\mu$服从正态分布$N\left(\mu, \sigma ^{2}\right)$

从$Z$到$X$的变換函数为$X=\sigma Z+\mu$，其反函数为
$$
Z=\frac{X-\mu}{\sigma}
$$
反函数的导数为
$$
\frac{\mathrm{d} Z}{\mathrm{~d} X}=\frac{1}{\sigma}
$$
利用式$f_{Y}(y)$，$X$ 的概率密度函数为
$$
f_{X}(x)=f_{Z}(h(x))\left|h^{\prime}(x)\right|=\frac{1}{\sqrt{2 \pi}} e^{-\frac{\left(\frac{x-\mu}{\sigma}\right)^{2}}{2}} \cdot \frac{1}{\sigma}=\frac{1}{\sqrt{2 \pi} \sigma} e^{-\frac{(x-\mu)^{2}}{2 \sigma^{2}}}
$$
因此，随机变量$X$服从正态分布$N\left(\mu, \sigma^{2}\right)$

## 逆变换采样算法

> 采样

在机器学习中，通常需要生成某种概率分布的随机数，称为`采样`，采样可以通过概率分布变换而实现

在计算机中，能够直接得到的随机数通常是均匀分布的随机数(事实上是`伪随机数`)，对它进行变换，可以得到我们想要的概率分布的随机数，`逆变换算法`(Inverse Transform Sampling)是一种典型的采样算法

假设随机变量$X$的分布函数为$$F_{X}(x)$$，随机变量$Y$的分布函数为$$F_{Y}(y)$$，它们均已知

$Y$通过单调递增的随机变量函数$Y=g(X)$对$X$进行变换而得到，现在要确定该函数，已知
$$
F_{X}\left(g^{-1}(y)\right)=F_{Y}(y)
$$
由于分布函数是单调递增的，可以解得
$$
g^{-1}(y)=F_{X}^{-1}\left(F_{Y}(y)\right)
$$
根据$X$和$Y$的分布函数可以确定此变换

> 重要结论

下面假设$X$或$Y$为均匀分布的随机数，根据上式来计算此变换函数，得到如下的两个结论

**结论1**: 假设随机变量$X$的分布函数为$$F_{X}(x)$$，该函数是严格单调增函数，则随机变量$Y=F_{X}(X)$服从均匀分布$U(0,1)$

下面给出证明，根据分布函数的定义，有
$$
F_{Y}(y)=p(Y \leqslant y)=p\left(F_{X}(X) \leqslant y\right)=p\left(X \leqslant F_{X}^{-1}(y)\right)=F_{X}\left(F_{X}^{-1}(y)\right)=y
$$
这就是均匀分布的分布函数，上式第1步和第4步使用了分布函数的定义，第5步使用了反函数的恒等式
$$
f\left(f^{-1}(x)\right)=x
$$
这一结论给出了根据一个已知概率分布的随机数构造均匀分布随机数的方法

也可以根据式$g^{-1}(y)$求解出$g(X)$，由于$Y$服从均匀分布$U(0,1)$，因此$F_{Y}(y)=y$，从而有
$$
g^{-1}(y)=F_{X}^{-1}\left(F_{Y}(y)\right)=F_{X}^{-1}(y)
$$
即$g(X)=F_{X}(X)$

**结论2**: 假设随机变量$X$服从均匀分布$U(0,1)$，随机变量$Y$的分布函数为$$F_{Y}(y)$$，则随机变量$$Y=F_{Y}^{-1}(X)$$服从概率分布$$F_{Y}(y)$$ 

下面给出证明，根据分布函数的定义，有
$$
F_{Y}(y)=p(Y \leqslant y)=p\left(F_{Y}^{-1}(X) \leqslant y\right)=p\left(X \leqslant F_{Y}(y)\right)=F_{Y}(y)
$$
上式第1步利用了分布函数的定义，第4步成立是因为$X$服从均匀分布，其分布函数为
$$
p(X \leqslant x)=x
$$
此结论给出了根据均匀分布随机数构造出某一分布函数已知的概率分布随机数的方法，只需要将均匀分布随机数$X$用目标概率分布$$F_{Y}(y)$$的反函数$$F_{Y}^{-1}(X)$$进行映射

同样，根据式$g^{-1}(y)$可以直接解出$g(X)$，由于$X$服从均匀分布，因此
$$
F_{X}^{-1}(x)=x
$$
从而有
$$
g^{-1}(y)=F_{X}^{-1}\left(F_{Y}(y)\right)=F_{Y}(y)
$$
即
$$
g(y)=F_{Y}^{-1}(y)
$$

> 例子

下面以指数分布为例说明逆变换采样算法，其分布函数为
$$
F(x)=\left\{\begin{array}{ll}
1-\mathrm{e}^{-\lambda x}, & x>0 \\
0, & \text { 其他 }
\end{array}\right.
$$
其反函数为
$$
F^{-1}(x)=-\frac{1}{\lambda} \ln (1-x)
$$
首先产生均匀分布$U(0,1)$的随机数$u$，然后计算
$$
x=-\frac{1}{\lambda} \ln (1-u)
$$
则$x$就是我们想要的指数分布的随机数

逆变换采样算法可以根据均匀分布的随机数生成任意概率分布的随机数，但实现时可能存在困难

对于某些概率分布，我们无法得到分布函数反函数$F_{Y}^{-1}(x)$的解析表达式，如正态分布

# 随机向量

向量是标量的推广，将随机变量推广到多维即为随机向量，每个分量都是随机变量，因此随机机向量是带有概率值的向量

描述随机向量的是多维概率分布

## 离散型随机向量

推广到多个变量可以得到随机向量，随机向量$\boldsymbol{x}$是一个向量，它的每个分量都是随机变量，各分量之间可能存在相关性

例如，描述一个人的基本信息的向量$(性别 \quad 年龄 \quad 学历 \quad 收人)$，是一个随机向量，各个分量之间存在依赖关系，收入与学历、年龄有关

性别为男和女的概率各为$0.5$，年龄为$0$和$120$之间的整数，服从各年龄段的人口统计分布规律

随机向量也分为`离散型`和`连续型`两种情况，描述离散型随机向量分布的是`联合概率质量函数`，是`概率质量函数`的推广定义了随机向量取每个值的概率
$$
p\left(\boldsymbol{x}=\boldsymbol{x}_{i}\right)=p\left(\boldsymbol{x}_{i}\right)
$$

对于二维离散型随机向量，联合概率质量函数是一个二维表(矩阵)，每个位置处的元素为随机向量$x$取该位置对应值的概率
$$
p\left(X=x_{i}, Y=y_{j}\right)=p_{i j}
$$
联合概率质量函数必须满足如下约束
$$
p\left(\boldsymbol{x}_{i}\right) \geqslant 0 \qquad \sum_{i} p\left(\boldsymbol{x}_{i}\right)=1
$$
下表是一个二维随机向量的联合概率质量函数

| X \ Y | 1    | 2    | 3    | 4    |
| ----- | ---- | ---- | ---- | ---- |
| 1     | 0.1  | 0.1  | 0.1  | 0.0  |
| 2     | 0.25 | 0.0  | 0.15 | 0.05 |
| 3     | 0.1  | 0.15 | 0.0  | 0.0  |

表中第3行第4列的元素表示$X$取值为$2$、$Y$取值为3的概率为$0.15$

> 边缘分布

对联合概率质量函数中某些变量的所有取值情况求和，可以得到`边缘概率质量函数`(Marginal Probability Mass Function)，也称为`边缘分布`

对于二维随机向量，对$X$和$Y$分别求和可以得到另外一个变量的边缘分布
$$
p_{X}(x)=\sum_{y} p(x, y) \qquad p_{Y}(y)=\sum_{x} p(x, y)
$$
有时候会将$p_{X}(x)$简写为$p(x)$

对于上表所示的联合概率质量函数，其边缘分布函数下表所示

| X    | 1    | 2    | 3    |
| ---- | ---- | ---- | ---- |
| 1    | 0.3  | 0.45 | 0.25 |

| Y    | 1    | 2    | 3    | 4    |
| ---- | ---- | ---- | ---- | ---- |
| 1    | 0.45 | 0.25 | 0.25 | 0.05 |

对$X$的边缘分布是对联合概率质量函数按行求和的结果，边缘分布可看作是将联合概率质量函数投影到某一个坐标轴后的结果

对$Y$的边缘分布函数是对联合概率质量函数按列求和的结果

> $n$维下的边缘分布

下面将边缘分布推广到随机向量的多个分量，有$n$维随机向量$\boldsymbol{x}$，将它拆分成子向量$$\boldsymbol{x}_{A}=$$ $$\left(x_{1} \cdots x_{r}\right)^{\mathrm{T}}$$和$$x_{B}=\left(x_{r+1} \cdots x_{n}\right)^{\mathrm{T}}$$

对$$x_{A}$$的边缘分布是对$$x_{B}$$的所有分量取各个值时的联合概率质量函数求和的结果
$$
p_{\boldsymbol{x}_{A}}\left(\boldsymbol{x}_{A}\right)=\sum_{x_{r+1}} \cdots \sum_{x_{n}} p(\boldsymbol{x})
$$
类似地有
$$
p_{\boldsymbol{x}_{B}}\left(\boldsymbol{x}_{B}\right)=\sum_{x_{1}} \cdots \sum_{x_{r}} p(\boldsymbol{x})
$$
类似于条件概率,条件分布定义为
$$
p_{X|Y}{(x|y)}=\frac {p(x,y)}{p_Y(y)} \qquad p_{Y|X}{(y|y)}=\frac {p(x,y)}{p_X(x)}
$$
有时候会将$p_{X \mid Y}(x \mid y)$简写为$p(x \mid y)$ ，将条件分布推广到随机向量的多个分量，按照本节前面的随机向量拆分方案，有
$$
p_{\boldsymbol{x}_{A} \mid \boldsymbol{x}_{B}}\left(\boldsymbol{x}_{A} \mid \boldsymbol{x}_{B}\right)=\frac{p(\boldsymbol{x})}{p_{\boldsymbol{x}_{B}}\left(\boldsymbol{x}_{B}\right)}
$$
对于二维随机向量, 如果对$\forall x, y$满足
$$
p_{X \mid Y}(x \mid y)=p_{X}(x) \quad p_{Y \mid X}(y \mid x)=p_{Y}(y)
$$
或者写成
$$
p(x, y)=p_{X}(x) p_{Y}(y)
$$
则称随机变量$X$和$Y$相互独立，这与随机事件独立性的定义一致

推广到$n$维随机向量，如果$$\forall x_{1}, x_{2}, \cdots, x_{n}$$满足
$$
p(\boldsymbol{x})=p_{X_{1}}\left(x_{1}\right) p_{X_{2}}\left(x_{2}\right) \cdots p_{X_{n}}\left(x_{n}\right)
$$
则称这些随机变量相互独立，对于离散型随机变量，贝叶斯公式同样适用

考虑下表的联合概率质量函数

| X \ Y | 1              | 2               | 3              |
| ----- | -------------- | --------------- | -------------- |
| 1     | $\frac{1}{12}$ | $\frac{4}{12}$  | $\frac{1}{12}$ |
| 2     | $\frac{1}{12}$ | $\frac {4}{12}$ | $\frac{1}{12}$ |

对$X$的边缘分布如表所示

| X    | 1             | 2             |
| ---- | ------------- | ------------- |
| 1    | $\frac{1}{2}$ | $\frac{1}{2}$ |

对$Y$的边缘分布如表所示

| Y    | 1             | 2             | 3             |
| ---- | ------------- | ------------- | ------------- |
| 1    | $\frac{1}{6}$ | $\frac{2}{3}$ | $\frac{1}{6}$ |

可以验证对所有$X$和$Y$取值$$x_{i}$$和$$y_{j}$$，有
$$
p\left(X=x_{i}, Y=y_{j}\right)=p_{X}\left(X=x_{i}\right) p_{Y}\left(Y=y_{j}\right)
$$
例如
$$
p(X=2, Y=2)=p_{X}(X=2) p_{Y}(Y=2)=\frac{1}{2} \times \frac{2}{3}
$$
因此$X$和$Y$相互独立

## 连续型随机向量

描述连续型随机向量的是`联合概率密度函数`，是概率密度函数的推广，联合概率密度函数必须满足如下约束条件
$$
f(x) \geqslant 0 \qquad \int_{R^{n}} f(x) \mathrm{d} x=1
$$
第2个等式为$n$重积分，对于二维随机向量，其联合概率密度函数满足的约束条件为
$$
f(x, y) \geqslant 0 \qquad \int_{-\infty}^{+\infty} \int_{-\infty}^{+\infty} f(x, y) \mathrm{d} x \mathrm{~d} y=1
$$
连续型随机向量在某一点处的概率为0

`分布函数`为联合概率密度函数对所有变量的变上限积分，对于二维随机向量，分布函数为
$$
F(x, y)=p(X \leqslant x, Y \leqslant y)=\int_{-\infty}^{x} \int_{-\infty}^{y} f(u, v) \mathrm{d} u \mathrm{~d} v
$$

> 边缘概率密度函数

`边缘概率密度函数`(Marginal Probability Density Function)将离散型随机向量边缘概率质量函数计算公式中的求和换成积分

对每个随机变量的边缘密度为对其他变量积分后的结果，对于二维随机向量为
$$
f_{X}(x)=\int_{-\infty}^{+\infty} f(x, y) \mathrm{d} y \qquad f_{Y}(y)=\int_{-\infty}^{+\infty} f(x, y) \mathrm{d} x
$$
下面将边缘概率密度函数推广到随机向量的多个分量，有$n$维随机向量$x$，将它拆分成子向量$$x_{A}=\left(x_{1} \cdots x_{r}\right)^{\mathrm{T}}$$和$$x_{B}=\left(x_{r+1} \cdots x_{n}\right)^{\mathrm{T}}$$

对$$x_{A}$$的边缘概率密度函数是联合概率密度函数对$$x_{B}$$的所有分量求积分的结果
$$
f_{\boldsymbol{x}_{A}}\left(\boldsymbol{x}_{A}\right)=\int_{-\infty}^{+\infty} \cdots \int_{-\infty}^{+\infty} f(\boldsymbol{x}) \mathrm{d} x_{r+1} \cdots \mathrm{d} x_{n}
$$
类似地有
$$
f_{\boldsymbol{x}_{B}}\left(\boldsymbol{x}_{B}\right)=\int_{-\infty}^{+\infty} \cdots \int_{-\infty}^{+\infty} f(\boldsymbol{x}) \mathrm{d} x_{1} \cdots \mathrm{d} x_{\Gamma}
$$
有时会将$f_{X}(x)$简写为$f(x)$

> 边缘累积分布函数

`边缘累积分布函数`(Marginal Cumulative Distribution Function )则为边缘密度的积分，类似于单随机变量的情况

对于二维随机向量，边缘累积分布函数为
$$
F_{X}(x)=\int_{-\infty}^{x} f_{X}(u) \mathrm{d} u \qquad F_{Y}(y)=\int_{-\infty}^{y} f_{Y}(v) \mathrm{d} v
$$
对于二维随机变量条件概率密度函数定义为
$$
f_{X \mid Y}(x \mid y)=\frac{f(x, y)}{f_{Y}(y)}
$$
通常情况下，在使用条件密度函数$f_{X \mid Y}(x \mid y)$时，$y$的值是已知的

有时会将$f_{X \mid Y}(x \mid y)$简写为$f(x \mid y)$

将条件概率密度函数推广到随机向量的多个分量，按照本节前面的随机向量拆分方案，有
$$
f_{x_{A} \mid \boldsymbol{x}_{B}}\left(\boldsymbol{x}_{A} \mid \boldsymbol{x}_{B}\right)=\frac{f(x)}{f_{\boldsymbol{x}_{B}}\left(\boldsymbol{x}_{B}\right)}
$$
随机向量的联合概率符合链式法则
$$
\begin{aligned}
f\left(x_{1}, \cdots, x_{n}\right) & =f\left(x_{n} \mid x_{1}, \cdots, x_{n-1}\right) f\left(x_{1}, \cdots, x_{n-1}\right) \\
& =f\left(x_{n} \mid x_{1}, \cdots, x_{n-1}\right) f\left(x_{n-1} \mid x_{1}, \cdots, x_{n-2}\right) f\left(x_{1}, \cdots, x_{n-2}\right) \\
& =\cdots \\
& =f\left(x_{1}\right) \prod_{i=2}^{n} f\left(x_{i} \mid x_{1}, \cdots, x_{i-1}\right)
\end{aligned}
$$
条件分布函数是对条件密度函数的积分，对于二维随机向量，定义为
$$
F_{X \mid Y}(x \mid y)=\int_{-\infty}^{x} f_{X \mid Y}(u \mid y) \mathrm{d} u=\int_{-\infty}^{x} \frac{f(u, y)}{f_{Y}(y)} \mathrm{d} u
$$
对于两个随机变量$(X, Y)$，如果下式几乎处处成立(不成立点为有限集或无限可数集）
$$
f(x, y)=f_{X}(x) f_{Y}(y)
$$
则称它们相互独立

对于$n$维随机向量$$\boldsymbol{x}=\left(X_{1}, \cdots, X_{n}\right)$$，如果下式几乎处处成立
$$
f(\boldsymbol{x})=f_{X_{1}}\left(x_{1}\right) f_{X_{2}}\left(x_{2}\right) \cdots f_{X_{n}}\left(x_{n}\right)
$$
则称它们相互独立

> 独立同分布

如果一组随机变量相互之间独立，且服从同一种概率分布，则称它们`独立同分布`(Independent And Identically Distributed, IID )

在机器学习中，一般假设各个样本之间独立 同分布，如果样本集$x_{i}, i=1, \cdots, l$独立同分布，均服从概率分布$p(\boldsymbol{x})$，则它们的联合概率为
$$
p\left(\boldsymbol{x}_{1}, \boldsymbol{x}_{2}, \cdots, \boldsymbol{x}_{l}\right)=\prod_{i=1}^{l} p\left(\boldsymbol{x}_{i}\right)
$$
在参数估计如最大似然估计，以及各种机器学习、深度学习算法中，经常使用此假设，以简化联合概率的计算

贝叶斯公式对于连续型随机变量同样适用，如果$X, Y$均为连续型随机变量，它们的联合概率密度函数为$f(x, y)$，则有
$$
f_{Y \mid X}(y \mid x)=\frac{f_{X \mid Y}(x \mid y) f_{Y}(y)}{f_{X}(x)}=\frac{f_{X \mid Y}(x \mid y) f_{Y}(y)}{\int_{-\infty}^{+\infty} f(x, y) \mathrm{d} y}
$$
在贝叶斯分类器等推断算法中这个公式经常被使用

## 数学期望

随机向量的数学期望$\mu$是一个向量，它的分量是对单个随机变量的数学期望
$$
\mu_{i}=E\left[x_{i}\right]
$$
其具体的计算方式与随机变量相同，对于离散型随机向量，分量$x_{i}$的数学期望为
$$
E\left[x_{i}\right]=\sum_{x_{1}} \cdots \sum_{x_{n}} x_{i} p(\boldsymbol{x})
$$
对于连续型随机向量，分量的数学期望为$n$重积分
$$
E\left[x_{i}\right]=\int_{\mathbb{R}^{n}} x_{i} f(\boldsymbol{x}) \mathrm{d} \boldsymbol{x}
$$
对于下表中的随机向量

| X \ Y | 1    | 2    | 3    | 4    |
| ----- | ---- | ---- | ---- | ---- |
| 1     | 0.1  | 0.1  | 0.1  | 0.0  |
| 2     | 0.25 | 0.0  | 0.15 | 0.05 |
| 3     | 0.1  | 0.15 | 0.0  | 0.0  |

其对$X$的数学期望为
$$
\begin{aligned}
E(X)= & 1 \times 0.1+1 \times 0.1+1 \times 0.1+1 \times 0.0+2 \times 0.25+2 \times 0.0+2 \times 0.15+2 \times 0.05 \\
& +3 \times 0.1+3 \times 0.15+3 \times 0.0+3 \times 0.0=1.95
\end{aligned}
$$
类似地可以定义随机向量函数的数学期望，对于离散型随机向量$\boldsymbol{x}$，定义为
$$
E[g(\boldsymbol{x})]=\sum_{x_{1}} \cdots \sum_{x_{n}} g(\boldsymbol{x}) p(\boldsymbol{x})
$$
对于连续型随机向量$\boldsymbol{x}, g(\boldsymbol{x})$的数学期望为
$$
E[g(\boldsymbol{x})]=\int_{\mathbb{R}^{n}} g(\boldsymbol{x}) f(\boldsymbol{x}) \mathrm{d} \boldsymbol{x}
$$
根据数学期望的定义可以证明
$$
E\left[\sum_{i=1}^{n} a_{i} x_{i}+b\right]=\sum_{i=1}^{n} a_{i} E\left[x_{i}\right]+b
$$
如果两个随机变量$X$和$Y$相互独立，则
$$
E[X Y]=E[X] E[Y]
$$
下面对连续型概率分布进行证明，假设$X$和$Y$的联合概率密度函数为$f(x, y)$，由于相互独立，因此
$$
f(x, y)=f_{X}(x) f_{Y}(y)
$$
根据数学期望的定义，有
$$
\begin{aligned}
E[X Y] & =\int_{-\infty}^{+\infty} \int_{-\infty}^{+\infty} x y f(x, y) \mathrm{d} x \mathrm{~d} y=\int_{-\infty}^{+\infty} \int_{-\infty}^{+\infty} x y f_{X}(x) f_{Y}(y) \mathrm{d} x \mathrm{~d} y \\
& =\int_{-\infty}^{+\infty} x f_{X}(x) \mathrm{d} x \int_{-\infty}^{+\infty} y f_{Y}(y) \mathrm{d} y=E[X] E[Y]
\end{aligned}
$$
对于随机向量函数，有
$$
E[g(\boldsymbol{x})+h(\boldsymbol{x})]=E[g(\boldsymbol{x})]+E[h(\boldsymbol{x})]
$$

## 协方差

> 定义

`协方差`(Covariance, cov)是方差对两个随机变量的推广，它反映了两个随机变量$X$与$Y$联合变动的程度

协方差定义为
$$
\operatorname{cov}(X, Y)=E[(X-E[X])(Y-E[Y])]
$$
是两个随机变量各自的偏差之积的数学期望，对于离散型随机变量，协方差的计算公式为
$$
\operatorname{cov}(X, Y)=\sum_{i=1}^{m} \sum_{j=1}^{n}\left(x_{i}-E[X]\right)\left(y_{j}-E[Y]\right) p\left(x_{i}, y_{j}\right)
$$
例如对于如下的概率分布

| X \ Y | 1              | 2              | 3              |
| ----- | -------------- | -------------- | -------------- |
| 1     | $\frac {1}{4}$ | $\frac {1}{4}$ | 0              |
| 2     | 0              | $\frac {1}{4}$ | $\frac {1}{4}$ |

可以得到$X$的边缘概率分布为

| X    | 1              | 2              |
| ---- | -------------- | -------------- |
| 1    | $\frac {1}{2}$ | $\frac {1}{2}$ |

$X$的数学期望为
$$
E[X]=1 \times \frac{1}{2}+2 \times \frac{1}{2}=\frac{3}{2}
$$
可以得到$Y$的边缘概率分布为

| Y    | 1              | 2              | 3              |
| ---- | -------------- | -------------- | -------------- |
| 1    | $\frac {1}{4}$ | $\frac {1}{2}$ | $\frac {1}{4}$ |

$Y$的数学期望为
$$
E[Y]=1 \times \frac{1}{4}+2 \times \frac{1}{2}+3 \times \frac{1}{4}=2
$$
根据定义，协方差为
$$
\begin{aligned} \operatorname{cov}(X, Y)= & \frac{1}{4} \times\left(1-\frac{3}{2}\right) \times(1-2)+\frac{1}{4} \times\left(1-\frac{3}{2}\right) \times(2-2)+0 \times\left(1-\frac{3}{2}\right) \times(3-2) \\ & +0 \times\left(2-\frac{3}{2}\right) \times(1-2)+\frac{1}{4} \times\left(2-\frac{3}{2}\right) \times(2-2)+\frac{1}{4} \times\left(2-\frac{3}{2}\right) \times(3-2)=\frac{1}{4}\end{aligned}
$$
对于连续型隨机变量，协方差的计算公式为
$$
\operatorname{cov}(X, Y)=\int_{-\infty}^{+\infty} \int_{-\infty}^{+\infty}(x-E[X])(y-E[Y]) f(x, y) \mathrm{d} x \mathrm{~d} y
$$
需要注意的是，协方差不能保证是非负的

根据定义，协方差具有对称性
$$
\operatorname{cov}(X, Y)=\operatorname{cov}(Y, X)
$$
可以证明下式成立
$$
\operatorname{cov}(X, Y)=E[X Y]-E[X] E[Y]
$$
根据定义，有
$$
\begin{aligned}
\operatorname{cov}(X, Y) & =E[(X-E[X])(Y-E[Y])]=E[X Y-X E[Y]-E[X] Y+E[X] E[Y]] \\ \\
& =E[X Y]-E[X] E[Y]-E[X] E[Y]+E[X] E[Y]=E[X Y]-E[X] E[Y]
\end{aligned}
$$
通常用式$\operatorname{cov}(X, Y)=E[X Y]-E[X] E[Y]$计算协方差，根据定义，一个随机变量与其自身的协方差就是该随机变量的方差
$$
\operatorname{cov}(X, X)=\operatorname{var}(X)
$$
根据协方差的定义，可以证明下面的等式成立
$$
\begin{array}{l}
\operatorname{cov}(X, a)=0 \qquad \operatorname{cov}(a X, b Y)=a b \operatorname{cov}(X, Y) \qquad \operatorname{cov}(X+a, Y+b)=\operatorname{cov}(X, Y) \\ \\
\operatorname{cov}(a X+b Y, c W+d V)=a c \operatorname{cov}(X, W)+a d \operatorname{cov}(X, V)+b c \operatorname{cov}(Y, W)+b d \operatorname{cov}(Y, V)
\end{array}
$$

> 例子

下面计算两个离散型随机变量在取值为有限种可能，且取每种值的概率相等时的协方差

有随机向量$(X, Y)$，其取值为
$$
\left(x_{i}, y_{i}\right), i=1, \cdots, n
$$
取每一对值的概率相等，即$p_{i}=\frac{1}{n}$，则这两个随机变量的协方差为
$$
\operatorname{cov}(X, Y)=\sum_{i=1}^{n} p_{i}\left(x_{i}-E[X]\right)\left(y_{i}-E[Y]\right)=\frac{1}{n} \sum_{i=1}^{n}\left(x_{i}-E[X]\right)\left(y_{i}-E[Y]\right)
$$
可进一步简化为
$$
\begin{array}{l}
\operatorname{cov}(X, Y)=\frac{1}{n} \sum_{i=1}^{n}\left(x_{i}-E[X]\right)\left(y_{i}-E[Y]\right) \\
=\frac{1}{n} \sum_{i=1}^{n}\left(x_{i}-\frac{1}{n} \sum_{j=1}^{n} x_{j}\right)\left(y_{i}-\frac{1}{n} \sum_{j=1}^{n} y_{j}\right)=\frac{1}{n^{3}} \sum_{i=1}^{n}\left(\sum_{j=1}^{n}\left(x_{i}-x_{j}\right)\right)\left(\sum_{j=1}^{n}\left(y_{i}-y_{j}\right)\right) \\
=\frac{1}{n^{2}} \sum_{i=1}^{n} \sum_{j=1}^{n} \frac{1}{2}\left(x_{i}-x_{j}\right)\left(y_{i}-y_{j}\right)=\frac{1}{n^{2}} \sum_{i=1}^{n} \sum_{j=i+1}^{n}\left(x_{i}-x_{j}\right)\left(y_{i}-y_{j}\right)
\end{array}
$$
如果两个随机变量的协方差为0，则称它们不相关(Uncorrelated)，如果两个随机变量相互独立，则它们的协方差为0
$$
\operatorname{cov}(X, Y)=0
$$
因此，相互独立的随机变量一定不相关

两个随机变量的协方差为0，不能推导出这两个随机变量相互独立

例如$X$服从均匀分布$U(-1,1)$，令$Y=X^{2}$，则有
$$
\operatorname{cov}(X, Y)=E\left[X \cdot X^{2}\right]-E[X] E\left[X^{2}\right]=E\left[X^{3}\right]-E[X] E\left[X^{2}\right]=0-0 \cdot E\left[X^{2}\right]=0
$$
但$X$和$Y$不相互独立，它们之间存在确定的非线性关系，协方差衡量的是线性相关性，协方差为0只能说明两个随机变量线性独立

如果两个随机变量服从正态分布，则不相关与独立等价，对于两个随机变量$X$和$Y$，有
$$
\operatorname{var}[X+Y]=\operatorname{var}[X]+\operatorname{var}[Y]+2 \operatorname{cov}(X, Y)
$$
根据方差的定义，有
$$
\begin{aligned}
\operatorname{var}[X+Y] & =E\left[(X+Y-E[X+Y])^{2}\right]=E\left[(X+Y-E[X]-E[Y])^{2}\right] \\
& =E\left[(X-E[X])^{2}+(Y-E[Y])^{2}+2(X-E[X])(Y-E[Y])\right] \\
& =\operatorname{var}[X]+\operatorname{var}[Y]+2 \operatorname{cov}(X, Y)
\end{aligned}
$$
推广到多个随机变量，对于随机变量$$X_{1}, \cdots, X_{n}$$，有
$$
\operatorname{var}\left[\sum_{i=1}^{n} X_{i}\right]=\sum_{i=1}^{n} \operatorname{var}\left[X_{i}\right]+2 \sum_{i=1}^{n} \sum_{j=i+1}^{n} \operatorname{cov}\left(X_{i}, X_{j}\right)
$$
如果两个随机变量$X$和$Y$相互独立，它们之和的方差等于各自的方差之和
$$
\operatorname{var}[X+Y]=\operatorname{var}[X]+\operatorname{var}[Y]
$$
推广到多个随机变量，如果$$X_{1}, \cdots, X_{n}$$相互独立，则有
$$
\operatorname{var}\left[\sum_{i=1}^{n} X_{i}\right]=\sum_{i=1}^{n} \operatorname{var}\left[X_{i}\right]
$$

> 协方差矩阵

对于$n$维随机向量$\boldsymbol{x}$，其任意两个分量$$x_{i}$$和$$x_{j}$$之间的协方差$$\operatorname{cov}\left(x_{i}, x_{j}\right)$$组成的矩阵称为`协方差矩阵`
$$
\boldsymbol{\Sigma}=\left(\begin{array}{ccc}
\operatorname{cov}\left(x_{1}, x_{1}\right) & \cdots & \operatorname{cov}\left(x_{1}, x_{n}\right) \\
\vdots & & \vdots \\
\operatorname{cov}\left(x_{n}, x_{1}\right) & \cdots & \operatorname{cov}\left(x_{n}, x_{n}\right)
\end{array}\right)
$$
